//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36037853
// Cuda compilation tools, release 12.9, V12.9.86
// Based on NVVM 7.0.1
//

.version 8.8
.target sm_86
.address_size 64

	// .globl	__direct_callable__readModifiedNormalFromNormalMap
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.global .align 1 .u8 _ZZ27__closesthit__setupGBuffersE8firstHit;
.const .align 1 .b8 __nv_static_46__12649d28_24_optix_shocker_gbuffer_cu_8117c865__ZN55_INTERNAL_12649d28_24_optix_shocker_gbuffer_cu_8117c8656shared16PermutationTableE[76] = {0, 10, 2, 7, 3, 5, 6, 4, 8, 1, 9, 5, 11, 6, 8, 1, 10, 12, 9, 3, 7, 0, 4, 2, 13, 10, 11, 5, 6, 9, 4, 3, 8, 7, 14, 2, 0, 1, 15, 12, 1, 13, 5, 14, 12, 3, 6, 16, 0, 8, 9, 2, 11, 4, 15, 7, 10, 10, 6, 5, 8, 15, 0, 17, 7, 14, 18, 13, 16, 2, 9, 12, 1, 11, 4, 3};
.extern .const .align 8 .b8 plp[16];
.global .align 1 .b8 $str[83] = {83, 104, 111, 99, 107, 101, 114, 71, 66, 117, 102, 102, 101, 114, 32, 82, 71, 58, 32, 116, 114, 97, 118, 72, 97, 110, 100, 108, 101, 61, 37, 108, 108, 117, 44, 32, 111, 114, 105, 103, 105, 110, 61, 40, 37, 46, 50, 102, 44, 37, 46, 50, 102, 44, 37, 46, 50, 102, 41, 44, 32, 100, 105, 114, 61, 40, 37, 46, 50, 102, 44, 37, 46, 50, 102, 44, 37, 46, 50, 102, 41, 10};
.global .align 1 .b8 $str$1[76] = {83, 104, 111, 99, 107, 101, 114, 71, 66, 117, 102, 102, 101, 114, 32, 82, 71, 58, 32, 82, 97, 121, 32, 116, 121, 112, 101, 115, 32, 45, 32, 83, 66, 84, 32, 111, 102, 102, 115, 101, 116, 61, 37, 117, 44, 32, 83, 66, 84, 32, 115, 116, 114, 105, 100, 101, 61, 37, 117, 44, 32, 109, 105, 115, 115, 32, 105, 110, 100, 101, 120, 61, 37, 117, 10};
.global .align 1 .b8 $str$2[84] = {83, 104, 111, 99, 107, 101, 114, 71, 66, 117, 102, 102, 101, 114, 32, 82, 71, 58, 32, 72, 73, 84, 32, 97, 116, 32, 91, 37, 117, 44, 37, 117, 93, 33, 32, 105, 110, 115, 116, 83, 108, 111, 116, 61, 37, 117, 44, 32, 112, 114, 105, 109, 73, 110, 100, 101, 120, 61, 37, 117, 44, 32, 112, 111, 115, 61, 40, 37, 46, 50, 102, 44, 37, 46, 50, 102, 44, 37, 46, 50, 102, 41, 10};
.global .align 1 .b8 $str$3[74] = {83, 104, 111, 99, 107, 101, 114, 71, 66, 117, 102, 102, 101, 114, 32, 67, 72, 58, 32, 72, 73, 84, 32, 68, 69, 84, 69, 67, 84, 69, 68, 33, 32, 112, 105, 120, 101, 108, 32, 40, 37, 117, 44, 32, 37, 117, 41, 44, 32, 105, 110, 115, 116, 97, 110, 99, 101, 32, 37, 117, 44, 32, 114, 97, 121, 32, 116, 61, 37, 46, 51, 102, 10};
.global .align 1 .b8 $str$4[77] = {83, 104, 111, 99, 107, 101, 114, 71, 66, 117, 102, 102, 101, 114, 32, 67, 72, 32, 68, 101, 116, 97, 105, 108, 58, 32, 112, 114, 105, 109, 73, 110, 100, 101, 120, 61, 37, 117, 44, 32, 103, 101, 111, 109, 73, 110, 115, 116, 83, 108, 111, 116, 61, 37, 117, 44, 32, 98, 99, 66, 61, 37, 46, 51, 102, 44, 32, 98, 99, 67, 61, 37, 46, 51, 102, 10};
.global .align 1 .b8 $str$5[76] = {83, 104, 111, 99, 107, 101, 114, 71, 66, 117, 102, 102, 101, 114, 32, 67, 72, 58, 32, 87, 111, 114, 108, 100, 32, 112, 111, 115, 61, 40, 37, 46, 50, 102, 44, 32, 37, 46, 50, 102, 44, 32, 37, 46, 50, 102, 41, 44, 32, 110, 111, 114, 109, 97, 108, 61, 40, 37, 46, 50, 102, 44, 32, 37, 46, 50, 102, 44, 32, 37, 46, 50, 102, 41, 10};
.global .align 1 .b8 $str$6[71] = {83, 104, 111, 99, 107, 101, 114, 71, 66, 117, 102, 102, 101, 114, 32, 67, 72, 58, 32, 65, 108, 98, 101, 100, 111, 32, 115, 101, 116, 32, 116, 111, 32, 40, 37, 46, 50, 102, 44, 32, 37, 46, 50, 102, 44, 32, 37, 46, 50, 102, 41, 32, 97, 116, 32, 112, 105, 120, 101, 108, 32, 40, 37, 117, 44, 32, 37, 117, 41, 10};
.global .align 1 .b8 $str$7[58] = {83, 104, 111, 99, 107, 101, 114, 71, 66, 117, 102, 102, 101, 114, 32, 77, 83, 58, 32, 83, 101, 116, 116, 105, 110, 103, 32, 109, 105, 115, 115, 32, 118, 97, 108, 117, 101, 115, 32, 102, 111, 114, 32, 112, 105, 120, 101, 108, 32, 40, 37, 117, 44, 37, 117, 41, 10};
.weak .global .align 4 .b8 _ZNK11LambertBRDF20getSurfaceParametersEP5RGB_TIfES2_Pf$566[12];
.weak .global .align 4 .b8 _ZNK22DiffuseAndSpecularBRDF16sampleThroughputERK10Vector3D_TIfLb0EEffPS1_Pf$574[12] = {0, 0, 128, 63, 0, 0, 128, 63, 0, 0, 128, 63};
.weak .global .align 4 .b8 _ZNK22DiffuseAndSpecularBRDF16sampleThroughputERK10Vector3D_TIfLb0EEffPS1_Pf$575[12];
.weak .global .align 4 .b8 _ZNK22DiffuseAndSpecularBRDF8evaluateERK10Vector3D_TIfLb0EES3_$645[12] = {0, 0, 128, 63, 0, 0, 128, 63, 0, 0, 128, 63};
.weak .global .align 4 .b8 _ZNK22DiffuseAndSpecularBRDF8evaluateERK10Vector3D_TIfLb0EES3_$646[12];
.weak .global .align 4 .b8 _ZNK22DiffuseAndSpecularBRDF29evaluateDHReflectanceEstimateERK10Vector3D_TIfLb0EE$707[12] = {0, 0, 128, 63, 0, 0, 128, 63, 0, 0, 128, 63};
.weak .global .align 4 .b8 _ZNK22DiffuseAndSpecularBRDF29evaluateDHReflectanceEstimateERK10Vector3D_TIfLb0EE$708[12] = {0, 0, 128, 63, 0, 0, 128, 63, 0, 0, 128, 63};
.weak .global .align 4 .b8 _ZNK22DiffuseAndSpecularBRDF25GGXMicrofacetDistribution6sampleERK10Vector3D_TIfLb0EEffPS1_IfLb1EEPf$745[12] = {0, 0, 128, 63};

.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__readModifiedNormalFromNormalMap(
	.param .b64 __direct_callable__readModifiedNormalFromNormalMap_param_0,
	.param .align 4 .b8 __direct_callable__readModifiedNormalFromNormalMap_param_1[4],
	.param .align 4 .b8 __direct_callable__readModifiedNormalFromNormalMap_param_2[8],
	.param .b32 __direct_callable__readModifiedNormalFromNormalMap_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [__direct_callable__readModifiedNormalFromNormalMap_param_0];
	ld.param.f32 	%f1, [__direct_callable__readModifiedNormalFromNormalMap_param_3];
	ld.param.u8 	%rs1, [__direct_callable__readModifiedNormalFromNormalMap_param_1+3];
	ld.param.f32 	%f2, [__direct_callable__readModifiedNormalFromNormalMap_param_2+4];
	ld.param.f32 	%f3, [__direct_callable__readModifiedNormalFromNormalMap_param_2];
	tex.level.2d.v4.f32.f32 	{%f4, %f5, %f6, %f7}, [%rd1, {%f3, %f2}], %f1;
	fma.rn.ftz.f32 	%f8, %f5, 0f40000000, 0fBF800000;
	and.b16  	%rs2, %rs1, 64;
	setp.eq.s16 	%p1, %rs2, 0;
	neg.ftz.f32 	%f9, %f8;
	selp.f32 	%f10, %f8, %f9, %p1;
	fma.rn.ftz.f32 	%f11, %f6, 0f40000000, 0fBF800000;
	fma.rn.ftz.f32 	%f12, %f4, 0f40000000, 0fBF800000;
	st.param.f32 	[func_retval0+0], %f12;
	st.param.f32 	[func_retval0+4], %f10;
	st.param.f32 	[func_retval0+8], %f11;
	ret;

}
	// .globl	__direct_callable__readModifiedNormalFromNormalMap2ch
.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__readModifiedNormalFromNormalMap2ch(
	.param .b64 __direct_callable__readModifiedNormalFromNormalMap2ch_param_0,
	.param .align 4 .b8 __direct_callable__readModifiedNormalFromNormalMap2ch_param_1[4],
	.param .align 4 .b8 __direct_callable__readModifiedNormalFromNormalMap2ch_param_2[8],
	.param .b32 __direct_callable__readModifiedNormalFromNormalMap2ch_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [__direct_callable__readModifiedNormalFromNormalMap2ch_param_0];
	ld.param.f32 	%f1, [__direct_callable__readModifiedNormalFromNormalMap2ch_param_3];
	ld.param.u8 	%rs1, [__direct_callable__readModifiedNormalFromNormalMap2ch_param_1+3];
	ld.param.f32 	%f2, [__direct_callable__readModifiedNormalFromNormalMap2ch_param_2+4];
	ld.param.f32 	%f3, [__direct_callable__readModifiedNormalFromNormalMap2ch_param_2];
	tex.level.2d.v4.f32.f32 	{%f4, %f5, %f6, %f7}, [%rd1, {%f3, %f2}], %f1;
	fma.rn.ftz.f32 	%f8, %f5, 0f40000000, 0fBF800000;
	fma.rn.ftz.f32 	%f9, %f4, 0f40000000, 0fBF800000;
	mul.ftz.f32 	%f10, %f9, %f9;
	mov.f32 	%f11, 0f3F800000;
	sub.ftz.f32 	%f12, %f11, %f10;
	mul.ftz.f32 	%f13, %f8, %f8;
	sub.ftz.f32 	%f14, %f12, %f13;
	and.b16  	%rs2, %rs1, 64;
	setp.eq.s16 	%p1, %rs2, 0;
	neg.ftz.f32 	%f15, %f8;
	sqrt.approx.ftz.f32 	%f16, %f14;
	selp.f32 	%f17, %f8, %f15, %p1;
	st.param.f32 	[func_retval0+0], %f9;
	st.param.f32 	[func_retval0+4], %f17;
	st.param.f32 	[func_retval0+8], %f16;
	ret;

}
	// .globl	__direct_callable__readModifiedNormalFromHeightMap
.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__readModifiedNormalFromHeightMap(
	.param .b64 __direct_callable__readModifiedNormalFromHeightMap_param_0,
	.param .align 4 .b8 __direct_callable__readModifiedNormalFromHeightMap_param_1[4],
	.param .align 4 .b8 __direct_callable__readModifiedNormalFromHeightMap_param_2[8]
)
{
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<22>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [__direct_callable__readModifiedNormalFromHeightMap_param_0];
	ld.param.u8 	%r1, [__direct_callable__readModifiedNormalFromHeightMap_param_1+3];
	ld.param.f32 	%f1, [__direct_callable__readModifiedNormalFromHeightMap_param_2+4];
	ld.param.f32 	%f2, [__direct_callable__readModifiedNormalFromHeightMap_param_2];
	tld4.r.2d.v4.f32.f32 	{%f3, %f4, %f5, %f6}, [%rd1, {%f2, %f1}];
	shl.b32 	%r2, %r1, 24;
	ld.param.u8 	%r3, [__direct_callable__readModifiedNormalFromHeightMap_param_1+2];
	shl.b32 	%r4, %r3, 16;
	or.b32  	%r5, %r2, %r4;
	ld.param.u8 	%rs1, [__direct_callable__readModifiedNormalFromHeightMap_param_1+1];
	mul.wide.u16 	%r6, %rs1, 256;
	or.b32  	%r7, %r5, %r6;
	ld.param.u8 	%r8, [__direct_callable__readModifiedNormalFromHeightMap_param_1];
	and.b32  	%r9, %r6, 16128;
	or.b32  	%r10, %r9, %r8;
	shr.u32 	%r11, %r7, 14;
	and.b32  	%r12, %r11, 16383;
	cvt.rn.f32.u32 	%f7, %r10;
	mul.ftz.f32 	%f8, %f7, 0f3BA00000;
	sub.ftz.f32 	%f9, %f4, %f3;
	mul.ftz.f32 	%f10, %f8, %f9;
	cvt.rn.f32.u32 	%f11, %r12;
	mul.ftz.f32 	%f12, %f11, 0f3BA00000;
	sub.ftz.f32 	%f13, %f3, %f6;
	mul.ftz.f32 	%f14, %f12, %f13;
	mul.ftz.f32 	%f15, %f14, %f14;
	fma.rn.ftz.f32 	%f16, %f10, %f10, %f15;
	add.ftz.f32 	%f17, %f16, 0f3F800000;
	rsqrt.approx.ftz.f32 	%f18, %f17;
	mul.ftz.f32 	%f19, %f18, %f10;
	mul.ftz.f32 	%f20, %f18, %f14;
	neg.ftz.f32 	%f21, %f19;
	st.param.f32 	[func_retval0+0], %f21;
	st.param.f32 	[func_retval0+4], %f20;
	st.param.f32 	[func_retval0+8], %f18;
	ret;

}
	// .globl	__direct_callable__LambertBRDF_getSurfaceParameters
.visible .func __direct_callable__LambertBRDF_getSurfaceParameters(
	.param .b64 __direct_callable__LambertBRDF_getSurfaceParameters_param_0,
	.param .b64 __direct_callable__LambertBRDF_getSurfaceParameters_param_1,
	.param .b64 __direct_callable__LambertBRDF_getSurfaceParameters_param_2,
	.param .b64 __direct_callable__LambertBRDF_getSurfaceParameters_param_3
)
{
	.reg .f32 	%f<7>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [__direct_callable__LambertBRDF_getSurfaceParameters_param_0];
	ld.param.u64 	%rd2, [__direct_callable__LambertBRDF_getSurfaceParameters_param_1];
	ld.param.u64 	%rd3, [__direct_callable__LambertBRDF_getSurfaceParameters_param_2];
	ld.param.u64 	%rd4, [__direct_callable__LambertBRDF_getSurfaceParameters_param_3];
	ld.f32 	%f1, [%rd1];
	ld.f32 	%f2, [%rd1+4];
	ld.f32 	%f3, [%rd1+8];
	st.f32 	[%rd2], %f1;
	st.f32 	[%rd2+4], %f2;
	st.f32 	[%rd2+8], %f3;
	ld.global.f32 	%f4, [_ZNK11LambertBRDF20getSurfaceParametersEP5RGB_TIfES2_Pf$566];
	ld.global.f32 	%f5, [_ZNK11LambertBRDF20getSurfaceParametersEP5RGB_TIfES2_Pf$566+4];
	ld.global.f32 	%f6, [_ZNK11LambertBRDF20getSurfaceParametersEP5RGB_TIfES2_Pf$566+8];
	st.f32 	[%rd3], %f4;
	st.f32 	[%rd3+4], %f5;
	st.f32 	[%rd3+8], %f6;
	mov.u32 	%r1, 1065353216;
	st.u32 	[%rd4], %r1;
	ret;

}
	// .globl	__direct_callable__LambertBRDF_sampleThroughput
.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__LambertBRDF_sampleThroughput(
	.param .b64 __direct_callable__LambertBRDF_sampleThroughput_param_0,
	.param .b64 __direct_callable__LambertBRDF_sampleThroughput_param_1,
	.param .b32 __direct_callable__LambertBRDF_sampleThroughput_param_2,
	.param .b32 __direct_callable__LambertBRDF_sampleThroughput_param_3,
	.param .b64 __direct_callable__LambertBRDF_sampleThroughput_param_4,
	.param .b64 __direct_callable__LambertBRDF_sampleThroughput_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<46>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd2, [__direct_callable__LambertBRDF_sampleThroughput_param_0];
	ld.param.u64 	%rd3, [__direct_callable__LambertBRDF_sampleThroughput_param_1];
	ld.param.f32 	%f17, [__direct_callable__LambertBRDF_sampleThroughput_param_2];
	ld.param.f32 	%f18, [__direct_callable__LambertBRDF_sampleThroughput_param_3];
	ld.param.u64 	%rd4, [__direct_callable__LambertBRDF_sampleThroughput_param_4];
	ld.param.u64 	%rd5, [__direct_callable__LambertBRDF_sampleThroughput_param_5];
	fma.rn.ftz.f32 	%f1, %f17, 0f40000000, 0fBF800000;
	fma.rn.ftz.f32 	%f42, %f18, 0f40000000, 0fBF800000;
	setp.eq.ftz.f32 	%p1, %f1, 0f00000000;
	mov.f32 	%f16, 0f00000000;
	setp.eq.ftz.f32 	%p2, %f42, 0f00000000;
	and.pred  	%p3, %p1, %p2;
	mov.f32 	%f44, %f16;
	mov.f32 	%f45, %f16;
	@%p3 bra 	$L__BB4_9;

	neg.ftz.f32 	%f3, %f42;
	setp.ltu.ftz.f32 	%p4, %f1, %f3;
	@%p4 bra 	$L__BB4_5;
	bra.uni 	$L__BB4_2;

$L__BB4_5:
	setp.gt.ftz.f32 	%p6, %f1, %f42;
	@%p6 bra 	$L__BB4_7;
	bra.uni 	$L__BB4_6;

$L__BB4_7:
	div.approx.ftz.f32 	%f22, %f1, %f42;
	add.ftz.f32 	%f43, %f22, 0f40C00000;
	mov.f32 	%f42, %f3;
	bra.uni 	$L__BB4_8;

$L__BB4_2:
	setp.gt.ftz.f32 	%p5, %f1, %f42;
	@%p5 bra 	$L__BB4_4;
	bra.uni 	$L__BB4_3;

$L__BB4_4:
	div.approx.ftz.f32 	%f43, %f42, %f1;
	mov.f32 	%f42, %f1;
	bra.uni 	$L__BB4_8;

$L__BB4_6:
	neg.ftz.f32 	%f6, %f1;
	div.approx.ftz.f32 	%f21, %f42, %f1;
	add.ftz.f32 	%f43, %f21, 0f40800000;
	mov.f32 	%f42, %f6;
	bra.uni 	$L__BB4_8;

$L__BB4_3:
	div.approx.ftz.f32 	%f19, %f1, %f42;
	mov.f32 	%f20, 0f40000000;
	sub.ftz.f32 	%f43, %f20, %f19;

$L__BB4_8:
	mul.ftz.f32 	%f23, %f43, 0f3F490FDB;
	cos.approx.ftz.f32 	%f24, %f23;
	mul.ftz.f32 	%f44, %f42, %f24;
	sin.approx.ftz.f32 	%f25, %f23;
	mul.ftz.f32 	%f45, %f42, %f25;

$L__BB4_9:
	mul.ftz.f32 	%f26, %f44, %f44;
	mov.f32 	%f27, 0f3F800000;
	sub.ftz.f32 	%f28, %f27, %f26;
	mul.ftz.f32 	%f29, %f45, %f45;
	sub.ftz.f32 	%f30, %f28, %f29;
	max.ftz.f32 	%f32, %f16, %f30;
	sqrt.approx.ftz.f32 	%f33, %f32;
	st.f32 	[%rd4], %f44;
	st.f32 	[%rd4+4], %f45;
	st.f32 	[%rd4+8], %f33;
	mov.f32 	%f34, 0f40490FDB;
	div.approx.ftz.f32 	%f35, %f33, %f34;
	st.f32 	[%rd5], %f35;
	ld.f32 	%f36, [%rd3+8];
	setp.gtu.ftz.f32 	%p7, %f36, 0f00000000;
	@%p7 bra 	$L__BB4_11;

	ld.f32 	%f37, [%rd4+8];
	neg.ftz.f32 	%f38, %f37;
	st.f32 	[%rd4+8], %f38;

$L__BB4_11:
	ld.f32 	%f39, [%rd2+8];
	ld.f32 	%f40, [%rd2+4];
	ld.f32 	%f41, [%rd2];
	st.param.f32 	[func_retval0+0], %f41;
	st.param.f32 	[func_retval0+4], %f40;
	st.param.f32 	[func_retval0+8], %f39;
	ret;

}
	// .globl	__direct_callable__LambertBRDF_evaluate
.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__LambertBRDF_evaluate(
	.param .b64 __direct_callable__LambertBRDF_evaluate_param_0,
	.param .b64 __direct_callable__LambertBRDF_evaluate_param_1,
	.param .b64 __direct_callable__LambertBRDF_evaluate_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<19>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [__direct_callable__LambertBRDF_evaluate_param_0];
	ld.param.u64 	%rd2, [__direct_callable__LambertBRDF_evaluate_param_1];
	ld.param.u64 	%rd3, [__direct_callable__LambertBRDF_evaluate_param_2];
	ld.f32 	%f10, [%rd3+8];
	ld.f32 	%f11, [%rd2+8];
	mul.ftz.f32 	%f12, %f11, %f10;
	setp.leu.ftz.f32 	%p1, %f12, 0f00000000;
	mov.f32 	%f16, 0f00000000;
	mov.f32 	%f17, %f16;
	mov.f32 	%f18, %f16;
	@%p1 bra 	$L__BB5_2;

	ld.f32 	%f13, [%rd1];
	ld.f32 	%f14, [%rd1+4];
	ld.f32 	%f15, [%rd1+8];
	mul.ftz.f32 	%f18, %f15, 0f3EA2F983;
	mul.ftz.f32 	%f17, %f14, 0f3EA2F983;
	mul.ftz.f32 	%f16, %f13, 0f3EA2F983;

$L__BB5_2:
	st.param.f32 	[func_retval0+0], %f16;
	st.param.f32 	[func_retval0+4], %f17;
	st.param.f32 	[func_retval0+8], %f18;
	ret;

}
	// .globl	__direct_callable__LambertBRDF_evaluatePDF
.visible .func  (.param .b32 func_retval0) __direct_callable__LambertBRDF_evaluatePDF(
	.param .b64 __direct_callable__LambertBRDF_evaluatePDF_param_0,
	.param .b64 __direct_callable__LambertBRDF_evaluatePDF_param_1,
	.param .b64 __direct_callable__LambertBRDF_evaluatePDF_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<10>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [__direct_callable__LambertBRDF_evaluatePDF_param_1];
	ld.param.u64 	%rd2, [__direct_callable__LambertBRDF_evaluatePDF_param_2];
	ld.f32 	%f1, [%rd2+8];
	ld.f32 	%f5, [%rd1+8];
	mul.ftz.f32 	%f6, %f5, %f1;
	setp.leu.ftz.f32 	%p1, %f6, 0f00000000;
	mov.f32 	%f9, 0f00000000;
	@%p1 bra 	$L__BB6_2;

	abs.ftz.f32 	%f7, %f1;
	mov.f32 	%f8, 0f40490FDB;
	div.approx.ftz.f32 	%f9, %f7, %f8;

$L__BB6_2:
	st.param.f32 	[func_retval0+0], %f9;
	ret;

}
	// .globl	__direct_callable__LambertBRDF_evaluateDHReflectanceEstimate
.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__LambertBRDF_evaluateDHReflectanceEstimate(
	.param .b64 __direct_callable__LambertBRDF_evaluateDHReflectanceEstimate_param_0,
	.param .b64 __direct_callable__LambertBRDF_evaluateDHReflectanceEstimate_param_1
)
{
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [__direct_callable__LambertBRDF_evaluateDHReflectanceEstimate_param_0];
	ld.f32 	%f1, [%rd1+8];
	ld.f32 	%f2, [%rd1+4];
	ld.f32 	%f3, [%rd1];
	st.param.f32 	[func_retval0+0], %f3;
	st.param.f32 	[func_retval0+4], %f2;
	st.param.f32 	[func_retval0+8], %f1;
	ret;

}
	// .globl	__direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters
.visible .func __direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters(
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters_param_0,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters_param_1,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters_param_2,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters_param_3
)
{
	.reg .f32 	%f<8>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [__direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters_param_0];
	ld.param.u64 	%rd2, [__direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters_param_1];
	ld.param.u64 	%rd3, [__direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters_param_2];
	ld.param.u64 	%rd4, [__direct_callable__DiffuseAndSpecularBRDF_getSurfaceParameters_param_3];
	ld.f32 	%f1, [%rd1];
	ld.f32 	%f2, [%rd1+4];
	ld.f32 	%f3, [%rd1+8];
	st.f32 	[%rd2], %f1;
	st.f32 	[%rd2+4], %f2;
	st.f32 	[%rd2+8], %f3;
	ld.f32 	%f4, [%rd1+12];
	ld.f32 	%f5, [%rd1+16];
	ld.f32 	%f6, [%rd1+20];
	st.f32 	[%rd3], %f4;
	st.f32 	[%rd3+4], %f5;
	st.f32 	[%rd3+8], %f6;
	ld.f32 	%f7, [%rd1+24];
	st.f32 	[%rd4], %f7;
	ret;

}
	// .globl	__direct_callable__DiffuseAndSpecularBRDF_sampleThroughput
.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__DiffuseAndSpecularBRDF_sampleThroughput(
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_0,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_1,
	.param .b32 __direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_2,
	.param .b32 __direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_3,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_4,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_5
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<400>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [__direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_0];
	ld.param.f32 	%f121, [__direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_2];
	ld.param.f32 	%f122, [__direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_3];
	ld.param.u64 	%rd3, [__direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_4];
	ld.param.u64 	%rd4, [__direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_5];
	ld.f32 	%f1, [%rd1+24];
	mul.ftz.f32 	%f2, %f1, %f1;
	ld.param.u64 	%rd5, [__direct_callable__DiffuseAndSpecularBRDF_sampleThroughput_param_1];
	add.s64 	%rd2, %rd5, 8;
	ld.f32 	%f3, [%rd5+8];
	setp.ge.ftz.f32 	%p1, %f3, 0f00000000;
	ld.f32 	%f368, [%rd5];
	@%p1 bra 	$L__BB9_2;
	bra.uni 	$L__BB9_1;

$L__BB9_2:
	ld.f32 	%f369, [%rd2+-4];
	mov.f32 	%f370, %f3;
	bra.uni 	$L__BB9_3;

$L__BB9_1:
	neg.ftz.f32 	%f368, %f368;
	ld.f32 	%f123, [%rd2+-4];
	neg.ftz.f32 	%f369, %f123;
	neg.ftz.f32 	%f370, %f3;

$L__BB9_3:
	mov.f32 	%f124, 0f3F800000;
	sub.ftz.f32 	%f125, %f124, %f370;
	mul.ftz.f32 	%f126, %f125, %f125;
	mul.ftz.f32 	%f127, %f126, %f126;
	mul.ftz.f32 	%f12, %f125, %f127;
	add.ftz.f32 	%f13, %f1, %f1;
	mul.ftz.f32 	%f128, %f13, %f3;
	mul.ftz.f32 	%f14, %f1, 0f3F000000;
	fma.rn.ftz.f32 	%f129, %f3, %f128, %f14;
	sub.ftz.f32 	%f15, %f124, %f12;
	fma.rn.ftz.f32 	%f130, %f12, %f129, %f15;
	ld.f32 	%f16, [%rd1];
	ld.f32 	%f17, [%rd1+4];
	mul.ftz.f32 	%f131, %f17, 0f3F371437;
	fma.rn.ftz.f32 	%f132, %f16, 0f3E59C6ED, %f131;
	ld.f32 	%f18, [%rd1+8];
	fma.rn.ftz.f32 	%f133, %f18, 0f3D93D07D, %f132;
	mul.ftz.f32 	%f134, %f130, %f130;
	mul.ftz.f32 	%f135, %f133, %f134;
	sub.ftz.f32 	%f136, %f124, %f1;
	fma.rn.ftz.f32 	%f19, %f1, 0f3F298953, %f136;
	mul.ftz.f32 	%f20, %f19, %f135;
	ld.f32 	%f21, [%rd1+12];
	ld.f32 	%f22, [%rd1+16];
	mul.ftz.f32 	%f137, %f22, 0f3F371437;
	fma.rn.ftz.f32 	%f138, %f21, 0f3E59C6ED, %f137;
	ld.f32 	%f23, [%rd1+20];
	fma.rn.ftz.f32 	%f139, %f23, 0f3D93D07D, %f138;
	fma.rn.ftz.f32 	%f24, %f15, %f139, %f12;
	add.ftz.f32 	%f25, %f20, %f24;
	setp.eq.ftz.f32 	%p2, %f25, 0f00000000;
	mov.f32 	%f397, 0f00000000;
	mov.f32 	%f398, 0f00000000;
	mov.f32 	%f399, 0f00000000;
	@%p2 bra 	$L__BB9_37;
	bra.uni 	$L__BB9_4;

$L__BB9_37:
	mov.u32 	%r2, 0;
	st.u32 	[%rd4], %r2;
	bra.uni 	$L__BB9_38;

$L__BB9_4:
	mul.ftz.f32 	%f26, %f25, %f122;
	setp.lt.ftz.f32 	%p3, %f26, %f20;
	@%p3 bra 	$L__BB9_18;
	bra.uni 	$L__BB9_5;

$L__BB9_18:
	div.approx.ftz.f32 	%f231, %f26, %f20;
	fma.rn.ftz.f32 	%f62, %f121, 0f40000000, 0fBF800000;
	fma.rn.ftz.f32 	%f377, %f231, 0f40000000, 0fBF800000;
	setp.eq.ftz.f32 	%p10, %f62, 0f00000000;
	mov.f32 	%f381, 0f00000000;
	setp.eq.ftz.f32 	%p11, %f377, 0f00000000;
	and.pred  	%p12, %p10, %p11;
	mov.f32 	%f387, %f381;
	mov.f32 	%f388, %f381;
	@%p12 bra 	$L__BB9_27;

	neg.ftz.f32 	%f64, %f377;
	setp.ltu.ftz.f32 	%p13, %f62, %f64;
	@%p13 bra 	$L__BB9_23;
	bra.uni 	$L__BB9_20;

$L__BB9_23:
	setp.gt.ftz.f32 	%p15, %f62, %f377;
	@%p15 bra 	$L__BB9_25;
	bra.uni 	$L__BB9_24;

$L__BB9_25:
	div.approx.ftz.f32 	%f235, %f62, %f377;
	add.ftz.f32 	%f378, %f235, 0f40C00000;
	mov.f32 	%f377, %f64;
	bra.uni 	$L__BB9_26;

$L__BB9_5:
	sub.ftz.f32 	%f140, %f26, %f20;
	div.approx.ftz.f32 	%f27, %f140, %f24;
	mul.ftz.f32 	%f141, %f2, %f368;
	mul.ftz.f32 	%f142, %f2, %f369;
	mul.ftz.f32 	%f143, %f142, %f142;
	fma.rn.ftz.f32 	%f144, %f141, %f141, %f143;
	mul.ftz.f32 	%f28, %f370, %f370;
	add.ftz.f32 	%f145, %f28, %f144;
	rsqrt.approx.ftz.f32 	%f146, %f145;
	mul.ftz.f32 	%f29, %f141, %f146;
	mul.ftz.f32 	%f30, %f142, %f146;
	mul.ftz.f32 	%f31, %f370, %f146;
	mul.ftz.f32 	%f147, %f30, %f30;
	fma.rn.ftz.f32 	%f148, %f29, %f29, %f147;
	sqrt.approx.ftz.f32 	%f32, %f148;
	setp.lt.ftz.f32 	%p4, %f31, 0f3F7FF972;
	@%p4 bra 	$L__BB9_7;
	bra.uni 	$L__BB9_6;

$L__BB9_7:
	rcp.approx.ftz.f32 	%f150, %f32;
	mul.ftz.f32 	%f371, %f30, %f150;
	mul.ftz.f32 	%f151, %f29, %f150;
	neg.ftz.f32 	%f372, %f151;
	mov.f32 	%f373, 0f00000000;
	bra.uni 	$L__BB9_8;

$L__BB9_6:
	ld.global.f32 	%f371, [_ZNK22DiffuseAndSpecularBRDF25GGXMicrofacetDistribution6sampleERK10Vector3D_TIfLb0EEffPS1_IfLb1EEPf$745];
	ld.global.f32 	%f372, [_ZNK22DiffuseAndSpecularBRDF25GGXMicrofacetDistribution6sampleERK10Vector3D_TIfLb0EEffPS1_IfLb1EEPf$745+4];
	ld.global.f32 	%f373, [_ZNK22DiffuseAndSpecularBRDF25GGXMicrofacetDistribution6sampleERK10Vector3D_TIfLb0EEffPS1_IfLb1EEPf$745+8];

$L__BB9_8:
	add.ftz.f32 	%f152, %f31, 0f3F800000;
	rcp.approx.ftz.f32 	%f41, %f152;
	setp.gt.ftz.f32 	%p5, %f41, %f27;
	@%p5 bra 	$L__BB9_10;
	bra.uni 	$L__BB9_9;

$L__BB9_10:
	div.approx.ftz.f32 	%f374, %f27, %f41;
	bra.uni 	$L__BB9_11;

$L__BB9_9:
	sub.ftz.f32 	%f153, %f27, %f41;
	mov.f32 	%f154, 0f3F800000;
	sub.ftz.f32 	%f155, %f154, %f41;
	div.approx.ftz.f32 	%f156, %f153, %f155;
	add.ftz.f32 	%f374, %f156, 0f3F800000;

$L__BB9_11:
	sqrt.approx.ftz.f32 	%f158, %f121;
	mul.ftz.f32 	%f159, %f374, 0f40490FDB;
	sin.approx.ftz.f32 	%f160, %f159;
	cos.approx.ftz.f32 	%f161, %f159;
	mul.ftz.f32 	%f162, %f158, %f161;
	mul.ftz.f32 	%f163, %f158, %f160;
	selp.f32 	%f164, 0f3F800000, %f31, %p5;
	mov.f32 	%f165, 0f3F800000;
	mul.ftz.f32 	%f166, %f164, %f163;
	mul.ftz.f32 	%f167, %f372, %f162;
	mul.ftz.f32 	%f168, %f31, %f372;
	mul.ftz.f32 	%f169, %f168, %f166;
	mul.ftz.f32 	%f170, %f31, %f371;
	mul.ftz.f32 	%f171, %f170, %f166;
	mul.ftz.f32 	%f172, %f32, %f166;
	fma.rn.ftz.f32 	%f173, %f371, %f162, %f169;
	sub.ftz.f32 	%f174, %f167, %f171;
	fma.rn.ftz.f32 	%f175, %f373, %f162, %f172;
	mul.ftz.f32 	%f176, %f162, %f162;
	sub.ftz.f32 	%f177, %f165, %f176;
	mul.ftz.f32 	%f178, %f166, %f166;
	sub.ftz.f32 	%f179, %f177, %f178;
	sqrt.approx.ftz.f32 	%f180, %f179;
	fma.rn.ftz.f32 	%f181, %f29, %f180, %f173;
	fma.rn.ftz.f32 	%f182, %f30, %f180, %f174;
	fma.rn.ftz.f32 	%f183, %f31, %f180, %f175;
	mul.ftz.f32 	%f184, %f2, %f181;
	mul.ftz.f32 	%f185, %f2, %f182;
	mul.ftz.f32 	%f186, %f185, %f185;
	fma.rn.ftz.f32 	%f187, %f184, %f184, %f186;
	fma.rn.ftz.f32 	%f188, %f183, %f183, %f187;
	rsqrt.approx.ftz.f32 	%f189, %f188;
	mul.ftz.f32 	%f45, %f189, %f184;
	mul.ftz.f32 	%f46, %f189, %f185;
	mul.ftz.f32 	%f47, %f189, %f183;
	setp.le.ftz.f32 	%p7, %f47, 0f00000000;
	mov.f32 	%f376, 0f00000000;
	mov.f32 	%f393, %f376;
	@%p7 bra 	$L__BB9_13;

	mul.ftz.f32 	%f190, %f46, %f46;
	fma.rn.ftz.f32 	%f191, %f45, %f45, %f190;
	mul.ftz.f32 	%f192, %f2, %f47;
	fma.rn.ftz.f32 	%f193, %f192, %f192, %f191;
	mul.ftz.f32 	%f194, %f193, %f193;
	mul.ftz.f32 	%f195, %f194, 0f40490FDB;
	mul.ftz.f32 	%f196, %f2, %f2;
	div.approx.ftz.f32 	%f393, %f196, %f195;

$L__BB9_13:
	mul.ftz.f32 	%f198, %f369, %f46;
	fma.rn.ftz.f32 	%f199, %f368, %f45, %f198;
	fma.rn.ftz.f32 	%f383, %f370, %f47, %f199;
	mul.ftz.f32 	%f200, %f370, %f383;
	setp.le.ftz.f32 	%p8, %f200, 0f00000000;
	@%p8 bra 	$L__BB9_15;

	mul.ftz.f32 	%f201, %f2, %f2;
	mul.ftz.f32 	%f202, %f369, %f369;
	fma.rn.ftz.f32 	%f203, %f368, %f368, %f202;
	mul.ftz.f32 	%f204, %f201, %f203;
	div.approx.ftz.f32 	%f205, %f204, %f28;
	add.ftz.f32 	%f206, %f205, 0f3F800000;
	sqrt.approx.ftz.f32 	%f207, %f206;
	add.ftz.f32 	%f208, %f207, 0f3F800000;
	mov.f32 	%f209, 0f40000000;
	div.approx.ftz.f32 	%f376, %f209, %f208;

$L__BB9_15:
	mov.f32 	%f210, 0f3F800000;
	min.ftz.f32 	%f392, %f383, %f210;
	add.ftz.f32 	%f211, %f392, %f392;
	mul.ftz.f32 	%f212, %f45, %f211;
	mul.ftz.f32 	%f213, %f46, %f211;
	mul.ftz.f32 	%f214, %f47, %f211;
	sub.ftz.f32 	%f387, %f212, %f368;
	sub.ftz.f32 	%f388, %f213, %f369;
	sub.ftz.f32 	%f389, %f214, %f370;
	mul.ftz.f32 	%f215, %f370, %f389;
	setp.gtu.ftz.f32 	%p9, %f215, 0f00000000;
	@%p9 bra 	$L__BB9_17;
	bra.uni 	$L__BB9_16;

$L__BB9_17:
	mul.ftz.f32 	%f219, %f392, 0f40800000;
	rcp.approx.ftz.f32 	%f220, %f219;
	abs.ftz.f32 	%f221, %f370;
	abs.ftz.f32 	%f222, %f383;
	mul.ftz.f32 	%f223, %f376, %f222;
	mul.ftz.f32 	%f224, %f393, %f223;
	div.approx.ftz.f32 	%f225, %f224, %f221;
	mul.ftz.f32 	%f391, %f225, %f220;
	mov.f32 	%f226, 0f40490FDB;
	div.approx.ftz.f32 	%f390, %f389, %f226;
	mul.ftz.f32 	%f386, %f387, %f387;
	mul.ftz.f32 	%f385, %f388, %f388;
	mul.ftz.f32 	%f227, %f46, %f388;
	fma.rn.ftz.f32 	%f228, %f45, %f387, %f227;
	fma.rn.ftz.f32 	%f384, %f47, %f389, %f228;
	bra.uni 	$L__BB9_33;

$L__BB9_16:
	mov.u32 	%r1, 0;
	st.u32 	[%rd4], %r1;
	bra.uni 	$L__BB9_38;

$L__BB9_20:
	setp.gt.ftz.f32 	%p14, %f62, %f377;
	@%p14 bra 	$L__BB9_22;
	bra.uni 	$L__BB9_21;

$L__BB9_22:
	div.approx.ftz.f32 	%f378, %f377, %f62;
	mov.f32 	%f377, %f62;
	bra.uni 	$L__BB9_26;

$L__BB9_24:
	neg.ftz.f32 	%f67, %f62;
	div.approx.ftz.f32 	%f234, %f377, %f62;
	add.ftz.f32 	%f378, %f234, 0f40800000;
	mov.f32 	%f377, %f67;
	bra.uni 	$L__BB9_26;

$L__BB9_21:
	div.approx.ftz.f32 	%f232, %f62, %f377;
	mov.f32 	%f233, 0f40000000;
	sub.ftz.f32 	%f378, %f233, %f232;

$L__BB9_26:
	mul.ftz.f32 	%f236, %f378, 0f3F490FDB;
	cos.approx.ftz.f32 	%f237, %f236;
	mul.ftz.f32 	%f387, %f377, %f237;
	sin.approx.ftz.f32 	%f238, %f236;
	mul.ftz.f32 	%f388, %f377, %f238;

$L__BB9_27:
	mul.ftz.f32 	%f386, %f387, %f387;
	mov.f32 	%f240, 0f3F800000;
	sub.ftz.f32 	%f241, %f240, %f386;
	mul.ftz.f32 	%f385, %f388, %f388;
	sub.ftz.f32 	%f242, %f241, %f385;
	max.ftz.f32 	%f243, %f381, %f242;
	sqrt.approx.ftz.f32 	%f389, %f243;
	add.ftz.f32 	%f244, %f370, %f389;
	add.ftz.f32 	%f245, %f368, %f387;
	add.ftz.f32 	%f246, %f369, %f388;
	mul.ftz.f32 	%f247, %f246, %f246;
	fma.rn.ftz.f32 	%f248, %f245, %f245, %f247;
	fma.rn.ftz.f32 	%f249, %f244, %f244, %f248;
	rsqrt.approx.ftz.f32 	%f250, %f249;
	mul.ftz.f32 	%f79, %f245, %f250;
	mul.ftz.f32 	%f80, %f246, %f250;
	mul.ftz.f32 	%f81, %f244, %f250;
	mul.ftz.f32 	%f251, %f388, %f80;
	fma.rn.ftz.f32 	%f252, %f387, %f79, %f251;
	fma.rn.ftz.f32 	%f384, %f389, %f81, %f252;
	min.ftz.f32 	%f392, %f384, %f240;
	mul.ftz.f32 	%f253, %f369, %f80;
	fma.rn.ftz.f32 	%f254, %f368, %f79, %f253;
	fma.rn.ftz.f32 	%f383, %f370, %f81, %f254;
	mul.ftz.f32 	%f255, %f370, %f383;
	setp.le.ftz.f32 	%p16, %f255, 0f00000000;
	@%p16 bra 	$L__BB9_29;

	mul.ftz.f32 	%f256, %f2, %f2;
	mul.ftz.f32 	%f257, %f369, %f369;
	fma.rn.ftz.f32 	%f258, %f368, %f368, %f257;
	mul.ftz.f32 	%f259, %f256, %f258;
	mul.ftz.f32 	%f260, %f370, %f370;
	div.approx.ftz.f32 	%f261, %f259, %f260;
	add.ftz.f32 	%f262, %f261, 0f3F800000;
	sqrt.approx.ftz.f32 	%f263, %f262;
	add.ftz.f32 	%f264, %f263, 0f3F800000;
	mov.f32 	%f265, 0f40000000;
	div.approx.ftz.f32 	%f381, %f265, %f264;

$L__BB9_29:
	setp.le.ftz.f32 	%p17, %f81, 0f00000000;
	mov.f32 	%f393, 0f00000000;
	mov.f32 	%f382, %f393;
	@%p17 bra 	$L__BB9_31;

	mul.ftz.f32 	%f267, %f80, %f80;
	fma.rn.ftz.f32 	%f268, %f79, %f79, %f267;
	mul.ftz.f32 	%f269, %f2, %f81;
	fma.rn.ftz.f32 	%f270, %f269, %f269, %f268;
	mul.ftz.f32 	%f271, %f270, %f270;
	mul.ftz.f32 	%f272, %f271, 0f40490FDB;
	mul.ftz.f32 	%f273, %f2, %f2;
	div.approx.ftz.f32 	%f382, %f273, %f272;

$L__BB9_31:
	mov.f32 	%f275, 0f40490FDB;
	div.approx.ftz.f32 	%f390, %f389, %f275;
	abs.ftz.f32 	%f276, %f370;
	abs.ftz.f32 	%f277, %f383;
	mul.ftz.f32 	%f278, %f381, %f277;
	mul.ftz.f32 	%f279, %f278, %f382;
	div.approx.ftz.f32 	%f280, %f279, %f276;
	mul.ftz.f32 	%f281, %f392, 0f40800000;
	rcp.approx.ftz.f32 	%f282, %f281;
	mul.ftz.f32 	%f391, %f282, %f280;
	@%p17 bra 	$L__BB9_33;

	mul.ftz.f32 	%f283, %f80, %f80;
	fma.rn.ftz.f32 	%f284, %f79, %f79, %f283;
	mul.ftz.f32 	%f285, %f2, %f81;
	fma.rn.ftz.f32 	%f286, %f285, %f285, %f284;
	mul.ftz.f32 	%f287, %f286, %f286;
	mul.ftz.f32 	%f288, %f287, 0f40490FDB;
	mul.ftz.f32 	%f289, %f2, %f2;
	div.approx.ftz.f32 	%f393, %f289, %f288;

$L__BB9_33:
	mov.f32 	%f290, 0f3F800000;
	sub.ftz.f32 	%f291, %f290, %f392;
	mul.ftz.f32 	%f292, %f291, %f291;
	mul.ftz.f32 	%f293, %f292, %f292;
	mul.ftz.f32 	%f294, %f291, %f293;
	add.ftz.f32 	%f295, %f385, %f386;
	mul.ftz.f32 	%f296, %f2, %f2;
	mul.ftz.f32 	%f297, %f296, %f295;
	mul.ftz.f32 	%f298, %f389, %f389;
	div.approx.ftz.f32 	%f299, %f297, %f298;
	mul.ftz.f32 	%f300, %f369, %f369;
	fma.rn.ftz.f32 	%f301, %f368, %f368, %f300;
	mul.ftz.f32 	%f302, %f296, %f301;
	mul.ftz.f32 	%f303, %f370, %f370;
	div.approx.ftz.f32 	%f304, %f302, %f303;
	add.ftz.f32 	%f305, %f299, 0f3F800000;
	sqrt.approx.ftz.f32 	%f306, %f305;
	add.ftz.f32 	%f307, %f306, 0fBF800000;
	mov.f32 	%f308, 0f40000000;
	div.approx.ftz.f32 	%f309, %f307, %f308;
	add.ftz.f32 	%f310, %f304, 0f3F800000;
	sqrt.approx.ftz.f32 	%f311, %f310;
	add.ftz.f32 	%f312, %f311, 0fBF800000;
	div.approx.ftz.f32 	%f313, %f312, %f308;
	div.approx.ftz.f32 	%f314, %f384, %f389;
	setp.gt.ftz.f32 	%p19, %f314, 0f00000000;
	selp.f32 	%f315, 0f3F800000, 0f00000000, %p19;
	div.approx.ftz.f32 	%f316, %f383, %f370;
	setp.gt.ftz.f32 	%p20, %f316, 0f00000000;
	selp.f32 	%f317, 0f3F800000, 0f00000000, %p20;
	mul.ftz.f32 	%f318, %f315, %f317;
	add.ftz.f32 	%f319, %f309, 0f3F800000;
	add.ftz.f32 	%f320, %f319, %f313;
	div.approx.ftz.f32 	%f321, %f318, %f320;
	sub.ftz.f32 	%f322, %f290, %f294;
	mul.ftz.f32 	%f323, %f322, %f21;
	mul.ftz.f32 	%f324, %f322, %f22;
	mul.ftz.f32 	%f325, %f322, %f23;
	ld.global.f32 	%f326, [_ZNK22DiffuseAndSpecularBRDF16sampleThroughputERK10Vector3D_TIfLb0EEffPS1_Pf$574];
	ld.global.f32 	%f327, [_ZNK22DiffuseAndSpecularBRDF16sampleThroughputERK10Vector3D_TIfLb0EEffPS1_Pf$574+4];
	ld.global.f32 	%f328, [_ZNK22DiffuseAndSpecularBRDF16sampleThroughputERK10Vector3D_TIfLb0EEffPS1_Pf$574+8];
	fma.rn.ftz.f32 	%f329, %f294, %f326, %f323;
	fma.rn.ftz.f32 	%f330, %f294, %f327, %f324;
	fma.rn.ftz.f32 	%f331, %f294, %f328, %f325;
	mul.ftz.f32 	%f332, %f389, 0f40800000;
	mul.ftz.f32 	%f333, %f370, %f332;
	mul.ftz.f32 	%f334, %f393, %f321;
	div.approx.ftz.f32 	%f335, %f334, %f333;
	mul.ftz.f32 	%f336, %f329, %f335;
	mul.ftz.f32 	%f337, %f330, %f335;
	mul.ftz.f32 	%f338, %f335, %f331;
	setp.eq.ftz.f32 	%p21, %f321, 0f00000000;
	ld.global.f32 	%f339, [_ZNK22DiffuseAndSpecularBRDF16sampleThroughputERK10Vector3D_TIfLb0EEffPS1_Pf$575];
	selp.f32 	%f340, %f339, %f336, %p21;
	ld.global.f32 	%f341, [_ZNK22DiffuseAndSpecularBRDF16sampleThroughputERK10Vector3D_TIfLb0EEffPS1_Pf$575+4];
	selp.f32 	%f342, %f341, %f337, %p21;
	ld.global.f32 	%f343, [_ZNK22DiffuseAndSpecularBRDF16sampleThroughputERK10Vector3D_TIfLb0EEffPS1_Pf$575+8];
	selp.f32 	%f344, %f343, %f338, %p21;
	mul.ftz.f32 	%f345, %f392, %f13;
	fma.rn.ftz.f32 	%f346, %f392, %f345, %f14;
	sub.ftz.f32 	%f347, %f290, %f389;
	mul.ftz.f32 	%f348, %f347, %f347;
	mul.ftz.f32 	%f349, %f348, %f348;
	mul.ftz.f32 	%f350, %f347, %f349;
	fma.rn.ftz.f32 	%f351, %f12, %f346, %f15;
	sub.ftz.f32 	%f352, %f290, %f350;
	fma.rn.ftz.f32 	%f353, %f350, %f346, %f352;
	mul.ftz.f32 	%f354, %f351, %f353;
	mul.ftz.f32 	%f355, %f19, %f354;
	mov.f32 	%f356, 0f40490FDB;
	div.approx.ftz.f32 	%f357, %f355, %f356;
	fma.rn.ftz.f32 	%f103, %f357, %f16, %f340;
	fma.rn.ftz.f32 	%f104, %f357, %f17, %f342;
	fma.rn.ftz.f32 	%f105, %f357, %f18, %f344;
	@%p1 bra 	$L__BB9_35;
	bra.uni 	$L__BB9_34;

$L__BB9_35:
	mov.f32 	%f396, %f389;
	bra.uni 	$L__BB9_36;

$L__BB9_34:
	neg.ftz.f32 	%f396, %f389;
	neg.ftz.f32 	%f388, %f388;
	neg.ftz.f32 	%f387, %f387;

$L__BB9_36:
	st.f32 	[%rd3], %f387;
	st.f32 	[%rd3+4], %f388;
	st.f32 	[%rd3+8], %f396;
	mul.ftz.f32 	%f358, %f24, %f391;
	fma.rn.ftz.f32 	%f359, %f20, %f390, %f358;
	div.approx.ftz.f32 	%f360, %f359, %f25;
	st.f32 	[%rd4], %f360;
	div.approx.ftz.f32 	%f361, %f389, %f360;
	mul.ftz.f32 	%f399, %f105, %f361;
	mul.ftz.f32 	%f398, %f104, %f361;
	mul.ftz.f32 	%f397, %f103, %f361;

$L__BB9_38:
	st.param.f32 	[func_retval0+0], %f397;
	st.param.f32 	[func_retval0+4], %f398;
	st.param.f32 	[func_retval0+8], %f399;
	ret;

}
	// .globl	__direct_callable__DiffuseAndSpecularBRDF_evaluate
.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__DiffuseAndSpecularBRDF_evaluate(
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_evaluate_param_0,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_evaluate_param_1,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_evaluate_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<155>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [__direct_callable__DiffuseAndSpecularBRDF_evaluate_param_0];
	ld.param.u64 	%rd4, [__direct_callable__DiffuseAndSpecularBRDF_evaluate_param_1];
	ld.param.u64 	%rd5, [__direct_callable__DiffuseAndSpecularBRDF_evaluate_param_2];
	ld.f32 	%f1, [%rd1+24];
	mul.ftz.f32 	%f2, %f1, %f1;
	add.s64 	%rd2, %rd4, 8;
	ld.f32 	%f146, [%rd4+8];
	add.s64 	%rd3, %rd5, 8;
	ld.f32 	%f149, [%rd5+8];
	mul.ftz.f32 	%f39, %f149, %f146;
	setp.le.ftz.f32 	%p1, %f39, 0f00000000;
	mov.f32 	%f152, 0f00000000;
	mov.f32 	%f153, %f152;
	mov.f32 	%f154, %f152;
	@%p1 bra 	$L__BB10_11;

	setp.ge.ftz.f32 	%p2, %f146, 0f00000000;
	ld.f32 	%f144, [%rd2+-8];
	@%p2 bra 	$L__BB10_3;
	bra.uni 	$L__BB10_2;

$L__BB10_3:
	ld.f32 	%f145, [%rd2+-4];
	bra.uni 	$L__BB10_4;

$L__BB10_2:
	neg.ftz.f32 	%f144, %f144;
	ld.f32 	%f40, [%rd2+-4];
	neg.ftz.f32 	%f145, %f40;
	neg.ftz.f32 	%f146, %f146;

$L__BB10_4:
	ld.f32 	%f147, [%rd3+-8];
	@%p2 bra 	$L__BB10_6;
	bra.uni 	$L__BB10_5;

$L__BB10_6:
	ld.f32 	%f148, [%rd3+-4];
	bra.uni 	$L__BB10_7;

$L__BB10_5:
	neg.ftz.f32 	%f147, %f147;
	ld.f32 	%f41, [%rd3+-4];
	neg.ftz.f32 	%f148, %f41;
	neg.ftz.f32 	%f149, %f149;

$L__BB10_7:
	add.ftz.f32 	%f42, %f144, %f147;
	add.ftz.f32 	%f43, %f145, %f148;
	mul.ftz.f32 	%f44, %f43, %f43;
	fma.rn.ftz.f32 	%f45, %f42, %f42, %f44;
	add.ftz.f32 	%f46, %f146, %f149;
	fma.rn.ftz.f32 	%f47, %f46, %f46, %f45;
	rsqrt.approx.ftz.f32 	%f48, %f47;
	mul.ftz.f32 	%f21, %f42, %f48;
	mul.ftz.f32 	%f22, %f43, %f48;
	mul.ftz.f32 	%f23, %f46, %f48;
	mul.ftz.f32 	%f49, %f148, %f22;
	fma.rn.ftz.f32 	%f50, %f147, %f21, %f49;
	fma.rn.ftz.f32 	%f24, %f149, %f23, %f50;
	setp.gtu.ftz.f32 	%p4, %f23, 0f00000000;
	@%p4 bra 	$L__BB10_9;
	bra.uni 	$L__BB10_8;

$L__BB10_9:
	mul.ftz.f32 	%f52, %f22, %f22;
	fma.rn.ftz.f32 	%f53, %f21, %f21, %f52;
	mul.ftz.f32 	%f54, %f2, %f23;
	fma.rn.ftz.f32 	%f55, %f54, %f54, %f53;
	mul.ftz.f32 	%f56, %f55, %f55;
	mul.ftz.f32 	%f57, %f56, 0f40490FDB;
	mul.ftz.f32 	%f150, %f2, %f2;
	div.approx.ftz.f32 	%f151, %f150, %f57;
	bra.uni 	$L__BB10_10;

$L__BB10_8:
	mul.ftz.f32 	%f150, %f2, %f2;
	mov.f32 	%f151, 0f00000000;

$L__BB10_10:
	mov.f32 	%f58, 0f3F800000;
	sub.ftz.f32 	%f59, %f58, %f24;
	mul.ftz.f32 	%f60, %f59, %f59;
	mul.ftz.f32 	%f61, %f60, %f60;
	mul.ftz.f32 	%f62, %f59, %f61;
	mul.ftz.f32 	%f63, %f148, %f148;
	fma.rn.ftz.f32 	%f64, %f147, %f147, %f63;
	mul.ftz.f32 	%f65, %f64, %f150;
	mul.ftz.f32 	%f66, %f149, %f149;
	div.approx.ftz.f32 	%f67, %f65, %f66;
	mul.ftz.f32 	%f68, %f145, %f145;
	fma.rn.ftz.f32 	%f69, %f144, %f144, %f68;
	mul.ftz.f32 	%f70, %f69, %f150;
	mul.ftz.f32 	%f71, %f146, %f146;
	div.approx.ftz.f32 	%f72, %f70, %f71;
	add.ftz.f32 	%f73, %f67, 0f3F800000;
	sqrt.approx.ftz.f32 	%f74, %f73;
	add.ftz.f32 	%f75, %f74, 0fBF800000;
	mov.f32 	%f76, 0f40000000;
	div.approx.ftz.f32 	%f77, %f75, %f76;
	add.ftz.f32 	%f78, %f72, 0f3F800000;
	sqrt.approx.ftz.f32 	%f79, %f78;
	add.ftz.f32 	%f80, %f79, 0fBF800000;
	div.approx.ftz.f32 	%f81, %f80, %f76;
	div.approx.ftz.f32 	%f82, %f24, %f149;
	setp.gt.ftz.f32 	%p5, %f82, 0f00000000;
	selp.f32 	%f83, 0f3F800000, 0f00000000, %p5;
	mul.ftz.f32 	%f84, %f145, %f22;
	fma.rn.ftz.f32 	%f85, %f144, %f21, %f84;
	fma.rn.ftz.f32 	%f86, %f146, %f23, %f85;
	div.approx.ftz.f32 	%f87, %f86, %f146;
	setp.gt.ftz.f32 	%p6, %f87, 0f00000000;
	selp.f32 	%f88, 0f3F800000, 0f00000000, %p6;
	mul.ftz.f32 	%f89, %f83, %f88;
	add.ftz.f32 	%f90, %f77, 0f3F800000;
	add.ftz.f32 	%f91, %f90, %f81;
	div.approx.ftz.f32 	%f92, %f89, %f91;
	ld.f32 	%f93, [%rd1+12];
	sub.ftz.f32 	%f94, %f58, %f62;
	mul.ftz.f32 	%f95, %f94, %f93;
	ld.f32 	%f96, [%rd1+16];
	mul.ftz.f32 	%f97, %f94, %f96;
	ld.f32 	%f98, [%rd1+20];
	mul.ftz.f32 	%f99, %f94, %f98;
	ld.global.f32 	%f100, [_ZNK22DiffuseAndSpecularBRDF8evaluateERK10Vector3D_TIfLb0EES3_$645];
	ld.global.f32 	%f101, [_ZNK22DiffuseAndSpecularBRDF8evaluateERK10Vector3D_TIfLb0EES3_$645+4];
	ld.global.f32 	%f102, [_ZNK22DiffuseAndSpecularBRDF8evaluateERK10Vector3D_TIfLb0EES3_$645+8];
	fma.rn.ftz.f32 	%f103, %f62, %f100, %f95;
	fma.rn.ftz.f32 	%f104, %f62, %f101, %f97;
	fma.rn.ftz.f32 	%f105, %f62, %f102, %f99;
	mul.ftz.f32 	%f106, %f149, 0f40800000;
	mul.ftz.f32 	%f107, %f146, %f106;
	mul.ftz.f32 	%f108, %f151, %f92;
	div.approx.ftz.f32 	%f109, %f108, %f107;
	mul.ftz.f32 	%f110, %f103, %f109;
	mul.ftz.f32 	%f111, %f104, %f109;
	mul.ftz.f32 	%f112, %f109, %f105;
	setp.eq.ftz.f32 	%p7, %f92, 0f00000000;
	ld.global.f32 	%f113, [_ZNK22DiffuseAndSpecularBRDF8evaluateERK10Vector3D_TIfLb0EES3_$646+8];
	selp.f32 	%f114, %f113, %f112, %p7;
	ld.global.f32 	%f115, [_ZNK22DiffuseAndSpecularBRDF8evaluateERK10Vector3D_TIfLb0EES3_$646+4];
	selp.f32 	%f116, %f115, %f111, %p7;
	ld.global.f32 	%f117, [_ZNK22DiffuseAndSpecularBRDF8evaluateERK10Vector3D_TIfLb0EES3_$646];
	selp.f32 	%f118, %f117, %f110, %p7;
	add.ftz.f32 	%f119, %f1, %f1;
	mul.ftz.f32 	%f120, %f24, %f119;
	mul.ftz.f32 	%f121, %f24, %f120;
	fma.rn.ftz.f32 	%f122, %f1, 0f3F000000, %f121;
	sub.ftz.f32 	%f123, %f58, %f146;
	mul.ftz.f32 	%f124, %f123, %f123;
	mul.ftz.f32 	%f125, %f124, %f124;
	mul.ftz.f32 	%f126, %f123, %f125;
	sub.ftz.f32 	%f127, %f58, %f149;
	mul.ftz.f32 	%f128, %f127, %f127;
	mul.ftz.f32 	%f129, %f128, %f128;
	mul.ftz.f32 	%f130, %f127, %f129;
	sub.ftz.f32 	%f131, %f58, %f126;
	fma.rn.ftz.f32 	%f132, %f126, %f122, %f131;
	sub.ftz.f32 	%f133, %f58, %f130;
	fma.rn.ftz.f32 	%f134, %f130, %f122, %f133;
	mul.ftz.f32 	%f135, %f132, %f134;
	sub.ftz.f32 	%f136, %f58, %f1;
	fma.rn.ftz.f32 	%f137, %f1, 0f3F298953, %f136;
	mul.ftz.f32 	%f138, %f137, %f135;
	mov.f32 	%f139, 0f40490FDB;
	div.approx.ftz.f32 	%f140, %f138, %f139;
	ld.f32 	%f141, [%rd1];
	ld.f32 	%f142, [%rd1+4];
	ld.f32 	%f143, [%rd1+8];
	fma.rn.ftz.f32 	%f154, %f140, %f143, %f114;
	fma.rn.ftz.f32 	%f153, %f140, %f142, %f116;
	fma.rn.ftz.f32 	%f152, %f140, %f141, %f118;

$L__BB10_11:
	st.param.f32 	[func_retval0+0], %f152;
	st.param.f32 	[func_retval0+4], %f153;
	st.param.f32 	[func_retval0+8], %f154;
	ret;

}
	// .globl	__direct_callable__DiffuseAndSpecularBRDF_evaluatePDF
.visible .func  (.param .b32 func_retval0) __direct_callable__DiffuseAndSpecularBRDF_evaluatePDF(
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_evaluatePDF_param_0,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_evaluatePDF_param_1,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_evaluatePDF_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<118>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [__direct_callable__DiffuseAndSpecularBRDF_evaluatePDF_param_0];
	ld.param.u64 	%rd4, [__direct_callable__DiffuseAndSpecularBRDF_evaluatePDF_param_2];
	ld.f32 	%f1, [%rd1+24];
	mul.ftz.f32 	%f2, %f1, %f1;
	ld.param.u64 	%rd5, [__direct_callable__DiffuseAndSpecularBRDF_evaluatePDF_param_1];
	add.s64 	%rd2, %rd5, 8;
	ld.f32 	%f3, [%rd5+8];
	setp.ge.ftz.f32 	%p1, %f3, 0f00000000;
	ld.f32 	%f109, [%rd5];
	@%p1 bra 	$L__BB11_2;
	bra.uni 	$L__BB11_1;

$L__BB11_2:
	ld.f32 	%f110, [%rd2+-4];
	mov.f32 	%f111, %f3;
	bra.uni 	$L__BB11_3;

$L__BB11_1:
	neg.ftz.f32 	%f109, %f109;
	ld.f32 	%f36, [%rd2+-4];
	neg.ftz.f32 	%f110, %f36;
	neg.ftz.f32 	%f111, %f3;

$L__BB11_3:
	ld.f32 	%f112, [%rd4];
	@%p1 bra 	$L__BB11_5;
	bra.uni 	$L__BB11_4;

$L__BB11_5:
	ld.f32 	%f113, [%rd4+4];
	ld.f32 	%f114, [%rd4+8];
	bra.uni 	$L__BB11_6;

$L__BB11_4:
	neg.ftz.f32 	%f112, %f112;
	ld.f32 	%f37, [%rd4+4];
	neg.ftz.f32 	%f113, %f37;
	ld.f32 	%f38, [%rd4+8];
	neg.ftz.f32 	%f114, %f38;

$L__BB11_6:
	add.ftz.f32 	%f40, %f109, %f112;
	add.ftz.f32 	%f41, %f110, %f113;
	mul.ftz.f32 	%f42, %f41, %f41;
	fma.rn.ftz.f32 	%f43, %f40, %f40, %f42;
	add.ftz.f32 	%f44, %f111, %f114;
	fma.rn.ftz.f32 	%f45, %f44, %f44, %f43;
	rsqrt.approx.ftz.f32 	%f46, %f45;
	mov.f32 	%f47, 0f3F800000;
	mul.ftz.f32 	%f21, %f40, %f46;
	mul.ftz.f32 	%f22, %f41, %f46;
	mul.ftz.f32 	%f23, %f44, %f46;
	mul.ftz.f32 	%f48, %f113, %f22;
	fma.rn.ftz.f32 	%f49, %f112, %f21, %f48;
	fma.rn.ftz.f32 	%f24, %f114, %f23, %f49;
	add.ftz.f32 	%f50, %f1, %f1;
	mul.ftz.f32 	%f51, %f50, %f3;
	mul.ftz.f32 	%f52, %f3, %f51;
	fma.rn.ftz.f32 	%f53, %f1, 0f3F000000, %f52;
	sub.ftz.f32 	%f54, %f47, %f111;
	mul.ftz.f32 	%f55, %f54, %f54;
	mul.ftz.f32 	%f56, %f55, %f55;
	mul.ftz.f32 	%f57, %f54, %f56;
	sub.ftz.f32 	%f58, %f47, %f57;
	fma.rn.ftz.f32 	%f59, %f57, %f53, %f58;
	ld.f32 	%f60, [%rd1];
	ld.f32 	%f61, [%rd1+4];
	mul.ftz.f32 	%f62, %f61, 0f3F371437;
	fma.rn.ftz.f32 	%f63, %f60, 0f3E59C6ED, %f62;
	ld.f32 	%f64, [%rd1+8];
	fma.rn.ftz.f32 	%f65, %f64, 0f3D93D07D, %f63;
	mul.ftz.f32 	%f66, %f59, %f59;
	mul.ftz.f32 	%f67, %f65, %f66;
	sub.ftz.f32 	%f68, %f47, %f1;
	fma.rn.ftz.f32 	%f69, %f1, 0f3F298953, %f68;
	mul.ftz.f32 	%f25, %f69, %f67;
	ld.f32 	%f70, [%rd1+12];
	ld.f32 	%f71, [%rd1+16];
	mul.ftz.f32 	%f72, %f71, 0f3F371437;
	fma.rn.ftz.f32 	%f73, %f70, 0f3E59C6ED, %f72;
	ld.f32 	%f74, [%rd1+20];
	fma.rn.ftz.f32 	%f75, %f74, 0f3D93D07D, %f73;
	fma.rn.ftz.f32 	%f26, %f58, %f75, %f57;
	add.ftz.f32 	%f27, %f25, %f26;
	setp.eq.ftz.f32 	%p3, %f27, 0f00000000;
	mov.f32 	%f117, 0f00000000;
	@%p3 bra 	$L__BB11_12;

	mul.ftz.f32 	%f77, %f110, %f22;
	fma.rn.ftz.f32 	%f78, %f109, %f21, %f77;
	fma.rn.ftz.f32 	%f28, %f111, %f23, %f78;
	mul.ftz.f32 	%f79, %f111, %f28;
	setp.le.ftz.f32 	%p4, %f79, 0f00000000;
	mov.f32 	%f116, 0f00000000;
	mov.f32 	%f115, %f116;
	@%p4 bra 	$L__BB11_9;

	mul.ftz.f32 	%f80, %f2, %f2;
	mul.ftz.f32 	%f81, %f110, %f110;
	fma.rn.ftz.f32 	%f82, %f109, %f109, %f81;
	mul.ftz.f32 	%f83, %f80, %f82;
	mul.ftz.f32 	%f84, %f111, %f111;
	div.approx.ftz.f32 	%f85, %f83, %f84;
	add.ftz.f32 	%f86, %f85, 0f3F800000;
	sqrt.approx.ftz.f32 	%f87, %f86;
	add.ftz.f32 	%f88, %f87, 0f3F800000;
	mov.f32 	%f89, 0f40000000;
	div.approx.ftz.f32 	%f115, %f89, %f88;

$L__BB11_9:
	abs.ftz.f32 	%f91, %f28;
	mul.ftz.f32 	%f31, %f115, %f91;
	setp.le.ftz.f32 	%p5, %f23, 0f00000000;
	@%p5 bra 	$L__BB11_11;

	mul.ftz.f32 	%f92, %f22, %f22;
	fma.rn.ftz.f32 	%f93, %f21, %f21, %f92;
	mul.ftz.f32 	%f94, %f2, %f23;
	fma.rn.ftz.f32 	%f95, %f94, %f94, %f93;
	mul.ftz.f32 	%f96, %f95, %f95;
	mul.ftz.f32 	%f97, %f96, 0f40490FDB;
	mul.ftz.f32 	%f98, %f2, %f2;
	div.approx.ftz.f32 	%f116, %f98, %f97;

$L__BB11_11:
	abs.ftz.f32 	%f99, %f111;
	mul.ftz.f32 	%f100, %f31, %f116;
	div.approx.ftz.f32 	%f101, %f100, %f99;
	mul.ftz.f32 	%f102, %f24, 0f40800000;
	rcp.approx.ftz.f32 	%f103, %f102;
	mul.ftz.f32 	%f104, %f103, %f101;
	mul.ftz.f32 	%f105, %f26, %f104;
	mov.f32 	%f106, 0f40490FDB;
	div.approx.ftz.f32 	%f107, %f114, %f106;
	fma.rn.ftz.f32 	%f108, %f25, %f107, %f105;
	div.approx.ftz.f32 	%f117, %f108, %f27;

$L__BB11_12:
	st.param.f32 	[func_retval0+0], %f117;
	ret;

}
	// .globl	__direct_callable__DiffuseAndSpecularBRDF_evaluateDHReflectanceEstimate
.visible .func  (.param .align 4 .b8 func_retval0[12]) __direct_callable__DiffuseAndSpecularBRDF_evaluateDHReflectanceEstimate(
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_evaluateDHReflectanceEstimate_param_0,
	.param .b64 __direct_callable__DiffuseAndSpecularBRDF_evaluateDHReflectanceEstimate_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<47>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [__direct_callable__DiffuseAndSpecularBRDF_evaluateDHReflectanceEstimate_param_0];
	ld.param.u64 	%rd2, [__direct_callable__DiffuseAndSpecularBRDF_evaluateDHReflectanceEstimate_param_1];
	ld.f32 	%f1, [%rd2+8];
	setp.ltu.ftz.f32 	%p1, %f1, 0f00000000;
	neg.ftz.f32 	%f2, %f1;
	selp.f32 	%f3, %f2, %f1, %p1;
	ld.f32 	%f4, [%rd1+24];
	add.ftz.f32 	%f5, %f4, %f4;
	mul.ftz.f32 	%f6, %f3, %f3;
	mul.ftz.f32 	%f7, %f5, %f6;
	fma.rn.ftz.f32 	%f8, %f4, 0f3F000000, %f7;
	mov.f32 	%f9, 0f3F800000;
	sub.ftz.f32 	%f10, %f9, %f3;
	mul.ftz.f32 	%f11, %f10, %f10;
	mul.ftz.f32 	%f12, %f11, %f11;
	mul.ftz.f32 	%f13, %f10, %f12;
	sub.ftz.f32 	%f14, %f9, %f13;
	fma.rn.ftz.f32 	%f15, %f8, %f13, %f14;
	ld.f32 	%f16, [%rd1];
	ld.f32 	%f17, [%rd1+4];
	ld.f32 	%f18, [%rd1+8];
	mul.ftz.f32 	%f19, %f16, %f15;
	mul.ftz.f32 	%f20, %f17, %f15;
	mul.ftz.f32 	%f21, %f18, %f15;
	sub.ftz.f32 	%f22, %f9, %f4;
	fma.rn.ftz.f32 	%f23, %f4, 0f3F298953, %f22;
	mul.ftz.f32 	%f24, %f13, %f22;
	ld.global.f32 	%f25, [_ZNK22DiffuseAndSpecularBRDF29evaluateDHReflectanceEstimateERK10Vector3D_TIfLb0EE$707];
	ld.global.f32 	%f26, [_ZNK22DiffuseAndSpecularBRDF29evaluateDHReflectanceEstimateERK10Vector3D_TIfLb0EE$707+4];
	ld.global.f32 	%f27, [_ZNK22DiffuseAndSpecularBRDF29evaluateDHReflectanceEstimateERK10Vector3D_TIfLb0EE$707+8];
	sub.ftz.f32 	%f28, %f9, %f24;
	ld.f32 	%f29, [%rd1+12];
	ld.f32 	%f30, [%rd1+16];
	ld.f32 	%f31, [%rd1+20];
	mul.ftz.f32 	%f32, %f28, %f29;
	mul.ftz.f32 	%f33, %f28, %f30;
	mul.ftz.f32 	%f34, %f28, %f31;
	fma.rn.ftz.f32 	%f35, %f25, %f24, %f32;
	fma.rn.ftz.f32 	%f36, %f26, %f24, %f33;
	fma.rn.ftz.f32 	%f37, %f24, %f27, %f34;
	fma.rn.ftz.f32 	%f38, %f23, %f19, %f35;
	fma.rn.ftz.f32 	%f39, %f23, %f20, %f36;
	fma.rn.ftz.f32 	%f40, %f23, %f21, %f37;
	ld.global.f32 	%f41, [_ZNK22DiffuseAndSpecularBRDF29evaluateDHReflectanceEstimateERK10Vector3D_TIfLb0EE$708];
	ld.global.f32 	%f42, [_ZNK22DiffuseAndSpecularBRDF29evaluateDHReflectanceEstimateERK10Vector3D_TIfLb0EE$708+4];
	ld.global.f32 	%f43, [_ZNK22DiffuseAndSpecularBRDF29evaluateDHReflectanceEstimateERK10Vector3D_TIfLb0EE$708+8];
	min.ftz.f32 	%f44, %f40, %f43;
	min.ftz.f32 	%f45, %f39, %f42;
	min.ftz.f32 	%f46, %f38, %f41;
	st.param.f32 	[func_retval0+0], %f46;
	st.param.f32 	[func_retval0+4], %f45;
	st.param.f32 	[func_retval0+8], %f44;
	ret;

}
	// .globl	__direct_callable__setupLambertBRDF
.visible .func __direct_callable__setupLambertBRDF(
	.param .b64 __direct_callable__setupLambertBRDF_param_0,
	.param .align 4 .b8 __direct_callable__setupLambertBRDF_param_1[8],
	.param .b32 __direct_callable__setupLambertBRDF_param_2,
	.param .b64 __direct_callable__setupLambertBRDF_param_3,
	.param .align 4 .b8 __direct_callable__setupLambertBRDF_param_4[4]
)
{
	.reg .f32 	%f<8>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [__direct_callable__setupLambertBRDF_param_0];
	ld.param.f32 	%f1, [__direct_callable__setupLambertBRDF_param_2];
	ld.param.u64 	%rd2, [__direct_callable__setupLambertBRDF_param_3];
	ld.param.f32 	%f2, [__direct_callable__setupLambertBRDF_param_1+4];
	ld.param.f32 	%f3, [__direct_callable__setupLambertBRDF_param_1];
	ld.u64 	%rd3, [%rd1];
	tex.level.2d.v4.f32.f32 	{%f4, %f5, %f6, %f7}, [%rd3, {%f3, %f2}], %f1;
	st.f32 	[%rd2], %f4;
	st.f32 	[%rd2+4], %f5;
	st.f32 	[%rd2+8], %f6;
	ret;

}
	// .globl	__direct_callable__setupDiffuseAndSpecularBRDF
.visible .func __direct_callable__setupDiffuseAndSpecularBRDF(
	.param .b64 __direct_callable__setupDiffuseAndSpecularBRDF_param_0,
	.param .align 4 .b8 __direct_callable__setupDiffuseAndSpecularBRDF_param_1[8],
	.param .b32 __direct_callable__setupDiffuseAndSpecularBRDF_param_2,
	.param .b64 __direct_callable__setupDiffuseAndSpecularBRDF_param_3,
	.param .align 4 .b8 __direct_callable__setupDiffuseAndSpecularBRDF_param_4[4]
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<22>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<6>;


	ld.param.u32 	%r1, [__direct_callable__setupDiffuseAndSpecularBRDF_param_4];
	ld.param.u64 	%rd1, [__direct_callable__setupDiffuseAndSpecularBRDF_param_0];
	ld.param.f32 	%f1, [__direct_callable__setupDiffuseAndSpecularBRDF_param_2];
	ld.param.u64 	%rd2, [__direct_callable__setupDiffuseAndSpecularBRDF_param_3];
	ld.param.f32 	%f2, [__direct_callable__setupDiffuseAndSpecularBRDF_param_1+4];
	ld.param.f32 	%f3, [__direct_callable__setupDiffuseAndSpecularBRDF_param_1];
	ld.u64 	%rd3, [%rd1];
	tex.level.2d.v4.f32.f32 	{%f4, %f5, %f6, %f7}, [%rd3, {%f3, %f2}], %f1;
	ld.u64 	%rd4, [%rd1+8];
	tex.level.2d.v4.f32.f32 	{%f8, %f9, %f10, %f11}, [%rd4, {%f3, %f2}], %f1;
	ld.u64 	%rd5, [%rd1+16];
	tex.level.2d.v4.f32.f32 	{%f12, %f13, %f14, %f15}, [%rd5, {%f3, %f2}], %f1;
	and.b32  	%r2, %r1, 1;
	setp.eq.b32 	%p1, %r2, 1;
	mul.ftz.f32 	%f16, %f12, 0f3F000000;
	selp.f32 	%f17, %f16, %f12, %p1;
	mov.f32 	%f18, 0f3F7FBE77;
	min.ftz.f32 	%f19, %f17, %f18;
	mov.f32 	%f20, 0f3F800000;
	sub.ftz.f32 	%f21, %f20, %f19;
	st.f32 	[%rd2], %f4;
	st.f32 	[%rd2+4], %f5;
	st.f32 	[%rd2+8], %f6;
	st.f32 	[%rd2+12], %f8;
	st.f32 	[%rd2+16], %f9;
	st.f32 	[%rd2+20], %f10;
	st.f32 	[%rd2+24], %f21;
	ret;

}
	// .globl	__direct_callable__setupSimplePBR_BRDF
.visible .func __direct_callable__setupSimplePBR_BRDF(
	.param .b64 __direct_callable__setupSimplePBR_BRDF_param_0,
	.param .align 4 .b8 __direct_callable__setupSimplePBR_BRDF_param_1[8],
	.param .b32 __direct_callable__setupSimplePBR_BRDF_param_2,
	.param .b64 __direct_callable__setupSimplePBR_BRDF_param_3,
	.param .align 4 .b8 __direct_callable__setupSimplePBR_BRDF_param_4[4]
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r1, [__direct_callable__setupSimplePBR_BRDF_param_4];
	ld.param.u64 	%rd1, [__direct_callable__setupSimplePBR_BRDF_param_0];
	ld.param.f32 	%f1, [__direct_callable__setupSimplePBR_BRDF_param_2];
	ld.param.u64 	%rd2, [__direct_callable__setupSimplePBR_BRDF_param_3];
	ld.param.f32 	%f2, [__direct_callable__setupSimplePBR_BRDF_param_1+4];
	ld.param.f32 	%f3, [__direct_callable__setupSimplePBR_BRDF_param_1];
	ld.u64 	%rd3, [%rd1];
	tex.level.2d.v4.f32.f32 	{%f4, %f5, %f6, %f7}, [%rd3, {%f3, %f2}], %f1;
	ld.u64 	%rd4, [%rd1+8];
	tex.level.2d.v4.f32.f32 	{%f8, %f9, %f10, %f11}, [%rd4, {%f3, %f2}], %f1;
	mov.f32 	%f12, 0f3F800000;
	sub.ftz.f32 	%f13, %f12, %f9;
	mov.f32 	%f14, 0f3F7FBE77;
	min.ftz.f32 	%f15, %f13, %f14;
	and.b32  	%r2, %r1, 1;
	setp.eq.b32 	%p1, %r2, 1;
	mul.ftz.f32 	%f16, %f15, 0f3F000000;
	selp.f32 	%f17, %f16, %f15, %p1;
	sub.ftz.f32 	%f18, %f12, %f10;
	mul.ftz.f32 	%f19, %f4, %f18;
	mul.ftz.f32 	%f20, %f5, %f18;
	mul.ftz.f32 	%f21, %f6, %f18;
	mul.ftz.f32 	%f22, %f18, 0f3D23D70A;
	fma.rn.ftz.f32 	%f23, %f4, %f10, %f22;
	fma.rn.ftz.f32 	%f24, %f5, %f10, %f22;
	fma.rn.ftz.f32 	%f25, %f6, %f10, %f22;
	sub.ftz.f32 	%f26, %f12, %f17;
	st.f32 	[%rd2], %f19;
	st.f32 	[%rd2+4], %f20;
	st.f32 	[%rd2+8], %f21;
	st.f32 	[%rd2+12], %f23;
	st.f32 	[%rd2+16], %f24;
	st.f32 	[%rd2+20], %f25;
	st.f32 	[%rd2+24], %f26;
	ret;

}
	// .globl	__raygen__setupGBuffers
.visible .entry __raygen__setupGBuffers()
{
	.local .align 8 .b8 	__local_depot16[192];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<12>;
	.reg .f32 	%f<242>;
	.reg .b32 	%r<154>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<54>;


	mov.u64 	%SPL, __local_depot16;
	cvta.local.u64 	%SP, %SPL;
	// begin inline asm
	call (%r7), _optix_get_launch_index_x, ();
	// end inline asm
	// begin inline asm
	call (%r8), _optix_get_launch_index_y, ();
	// end inline asm
	ld.const.u64 	%rd9, [plp+8];
	cvta.to.global.u64 	%rd1, %rd9;
	ld.global.u32 	%r3, [%rd1+144];
	and.b32  	%r9, %r3, 64;
	setp.eq.s32 	%p1, %r9, 0;
	ld.const.u64 	%rd10, [plp];
	cvta.to.global.u64 	%rd2, %rd10;
	mov.f32 	%f33, 0f3F000000;
	mov.f32 	%f232, %f33;
	mov.f32 	%f233, %f33;
	@%p1 bra 	$L__BB16_2;

	ld.global.u32 	%r10, [%rd2+24];
	shr.u32 	%r11, %r8, 1;
	shr.u32 	%r12, %r7, 1;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	and.b32  	%r14, %r8, 1;
	and.b32  	%r15, %r7, 1;
	bfi.b32 	%r16, %r14, %r15, 1, 1;
	bfi.b32 	%r17, %r13, %r16, 2, 30;
	ld.global.u64 	%rd11, [%rd2+8];
	mul.wide.u32 	%rd12, %r17, 8;
	add.s64 	%rd13, %rd11, %rd12;
	ld.u64 	%rd14, [%rd13];
	mul.lo.s64 	%rd15, %rd14, 6364136223846793005;
	add.s64 	%rd16, %rd15, 1;
	mul.lo.s64 	%rd17, %rd14, 7520897724310334953;
	shr.u64 	%rd18, %rd14, 18;
	xor.b64  	%rd19, %rd18, %rd14;
	shr.u64 	%rd20, %rd19, 27;
	cvt.u32.u64 	%r18, %rd20;
	shr.u64 	%rd21, %rd14, 59;
	cvt.u32.u64 	%r19, %rd21;
	shf.r.wrap.b32 	%r20, %r18, %r18, %r19;
	shr.u32 	%r21, %r20, 9;
	or.b32  	%r22, %r21, 1065353216;
	mov.b32 	%f34, %r22;
	add.ftz.f32 	%f232, %f34, 0fBF800000;
	add.s64 	%rd22, %rd17, 6364136223846793006;
	shr.u64 	%rd23, %rd16, 18;
	xor.b64  	%rd24, %rd23, %rd16;
	shr.u64 	%rd25, %rd24, 27;
	cvt.u32.u64 	%r23, %rd25;
	shr.u64 	%rd26, %rd16, 59;
	cvt.u32.u64 	%r24, %rd26;
	shf.r.wrap.b32 	%r25, %r23, %r23, %r24;
	shr.u32 	%r26, %r25, 9;
	or.b32  	%r27, %r26, 1065353216;
	mov.b32 	%f35, %r27;
	add.ftz.f32 	%f233, %f35, 0fBF800000;
	st.u64 	[%rd13], %rd22;

$L__BB16_2:
	cvt.rn.f32.u32 	%f5, %r7;
	add.ftz.f32 	%f36, %f232, %f5;
	ld.global.v2.u32 	{%r28, %r29}, [%rd2];
	mov.u32 	%r31, 0;
	cvt.rn.f32.s32 	%f37, %r28;
	div.approx.ftz.f32 	%f38, %f36, %f37;
	cvt.rn.f32.u32 	%f6, %r8;
	add.ftz.f32 	%f39, %f233, %f6;
	cvt.rn.f32.s32 	%f40, %r29;
	div.approx.ftz.f32 	%f41, %f39, %f40;
	ld.global.f32 	%f42, [%rd1+20];
	mul.ftz.f32 	%f43, %f42, 0f3F000000;
	sin.approx.ftz.f32 	%f45, %f43;
	cos.approx.ftz.f32 	%f46, %f43;
	div.approx.ftz.f32 	%f47, %f45, %f46;
	add.ftz.f32 	%f48, %f47, %f47;
	ld.global.f32 	%f49, [%rd1+16];
	mul.ftz.f32 	%f50, %f49, %f48;
	ld.global.f32 	%f7, [%rd1+24];
	ld.global.f32 	%f8, [%rd1+28];
	ld.global.f32 	%f9, [%rd1+32];
	sub.ftz.f32 	%f51, %f33, %f38;
	mul.ftz.f32 	%f52, %f51, %f50;
	sub.ftz.f32 	%f53, %f33, %f41;
	mul.ftz.f32 	%f54, %f53, %f48;
	ld.global.f32 	%f55, [%rd1+36];
	ld.global.f32 	%f56, [%rd1+48];
	mul.ftz.f32 	%f57, %f54, %f56;
	fma.rn.ftz.f32 	%f58, %f52, %f55, %f57;
	ld.global.f32 	%f59, [%rd1+60];
	add.ftz.f32 	%f60, %f59, %f58;
	ld.global.f32 	%f61, [%rd1+40];
	ld.global.f32 	%f62, [%rd1+52];
	mul.ftz.f32 	%f63, %f54, %f62;
	fma.rn.ftz.f32 	%f64, %f52, %f61, %f63;
	ld.global.f32 	%f65, [%rd1+64];
	add.ftz.f32 	%f66, %f65, %f64;
	ld.global.f32 	%f67, [%rd1+44];
	ld.global.f32 	%f68, [%rd1+56];
	mul.ftz.f32 	%f69, %f54, %f68;
	fma.rn.ftz.f32 	%f70, %f52, %f67, %f69;
	ld.global.f32 	%f71, [%rd1+68];
	add.ftz.f32 	%f72, %f71, %f70;
	mul.ftz.f32 	%f73, %f66, %f66;
	fma.rn.ftz.f32 	%f74, %f60, %f60, %f73;
	fma.rn.ftz.f32 	%f75, %f72, %f72, %f74;
	rsqrt.approx.ftz.f32 	%f76, %f75;
	mul.ftz.f32 	%f10, %f60, %f76;
	mul.ftz.f32 	%f11, %f66, %f76;
	mul.ftz.f32 	%f12, %f72, %f76;
	add.u64 	%rd27, %SP, 0;
	add.u64 	%rd4, %SPL, 0;
	st.local.u32 	[%rd4], %r31;
	st.local.u32 	[%rd4+4], %r31;
	st.local.u32 	[%rd4+8], %r31;
	mov.u32 	%r33, 2143289344;
	st.local.u32 	[%rd4+12], %r33;
	st.local.u32 	[%rd4+16], %r33;
	st.local.u32 	[%rd4+20], %r33;
	st.local.u32 	[%rd4+24], %r33;
	st.local.u32 	[%rd4+28], %r33;
	st.local.u32 	[%rd4+32], %r33;
	st.local.u32 	[%rd4+36], %r33;
	st.local.u32 	[%rd4+40], %r33;
	st.local.u32 	[%rd4+44], %r33;
	mov.u32 	%r34, -1;
	st.local.u32 	[%rd4+48], %r34;
	st.local.u32 	[%rd4+52], %r34;
	st.local.u32 	[%rd4+56], %r34;
	mov.u16 	%rs1, 0;
	st.local.v2.u16 	[%rd4+60], {%rs1, %rs1};
	add.u64 	%rd28, %SP, 64;
	add.u64 	%rd5, %SPL, 64;
	st.local.u32 	[%rd5], %r31;
	st.local.u32 	[%rd5+4], %r31;
	st.local.u32 	[%rd5+8], %r31;
	st.local.u32 	[%rd5+12], %r31;
	st.local.u32 	[%rd5+16], %r31;
	st.local.u32 	[%rd5+20], %r31;
	st.local.u32 	[%rd5+24], %r31;
	st.local.u32 	[%rd5+28], %r31;
	st.local.u32 	[%rd5+32], %r31;
	st.local.u32 	[%rd5+36], %r31;
	st.local.u32 	[%rd5+40], %r31;
	st.local.u32 	[%rd5+44], %r31;
	st.local.u32 	[%rd5+48], %r31;
	st.local.u32 	[%rd5+52], %r31;
	st.local.u32 	[%rd5+56], %r31;
	st.local.u32 	[%rd5+60], %r31;
	st.local.v4.u8 	[%rd5+64], {%rs1, %rs1, %rs1, %rs1};
	or.b32  	%r35, %r8, %r7;
	setp.eq.s32 	%p2, %r35, 0;
	add.u64 	%rd29, %SP, 136;
	add.u64 	%rd6, %SPL, 136;
	@%p2 bra 	$L__BB16_3;
	bra.uni 	$L__BB16_4;

$L__BB16_3:
	ld.global.u64 	%rd30, [%rd1];
	st.local.u64 	[%rd6], %rd30;
	cvt.ftz.f64.f32 	%fd1, %f7;
	st.local.f64 	[%rd6+8], %fd1;
	cvt.ftz.f64.f32 	%fd2, %f8;
	st.local.f64 	[%rd6+16], %fd2;
	cvt.ftz.f64.f32 	%fd3, %f9;
	st.local.f64 	[%rd6+24], %fd3;
	cvt.ftz.f64.f32 	%fd4, %f10;
	st.local.f64 	[%rd6+32], %fd4;
	cvt.ftz.f64.f32 	%fd5, %f11;
	st.local.f64 	[%rd6+40], %fd5;
	cvt.ftz.f64.f32 	%fd6, %f12;
	st.local.f64 	[%rd6+48], %fd6;
	mov.u64 	%rd31, $str;
	cvta.global.u64 	%rd32, %rd31;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd32;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r36, [retval0+0];
	} // callseq 0
	mov.u32 	%r37, 2;
	st.local.v2.u32 	[%rd6], {%r31, %r37};
	st.local.u32 	[%rd6+8], %r31;
	mov.u64 	%rd34, $str$1;
	cvta.global.u64 	%rd35, %rd34;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd35;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r39, [retval0+0];
	} // callseq 1

$L__BB16_4:
	ld.global.u64 	%rd36, [%rd1];
	mov.b64 	{%r79, %r80}, %rd27;
	mov.b64 	{%r81, %r82}, %rd28;
	mov.f32 	%f84, 0f7F7FFFFF;
	mov.f32 	%f234, 0f00000000;
	mov.u32 	%r73, 255;
	mov.u32 	%r76, 2;
	mov.u32 	%r78, 4;
	// begin inline asm
	call(%r40,%r41,%r42,%r43,%r44,%r45,%r46,%r47,%r48,%r49,%r50,%r51,%r52,%r53,%r54,%r55,%r56,%r57,%r58,%r59,%r60,%r61,%r62,%r63,%r64,%r65,%r66,%r67,%r68,%r69,%r70,%r71),_optix_trace_typed_32,(%r31,%rd36,%f7,%f8,%f9,%f10,%f11,%f12,%f234,%f84,%f234,%r73,%r31,%r31,%r76,%r31,%r78,%r79,%r80,%r81,%r82,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31,%r31);
	// end inline asm
	ld.local.u32 	%r4, [%rd4+48];
	setp.eq.s32 	%p3, %r4, -1;
	mul.wide.u32 	%rd39, %r8, 1374389535;
	shr.u64 	%rd40, %rd39, 36;
	cvt.u32.u64 	%r111, %rd40;
	mul.lo.s32 	%r112, %r111, 50;
	sub.s32 	%r113, %r8, %r112;
	mul.wide.u32 	%rd41, %r7, 1374389535;
	shr.u64 	%rd42, %rd41, 36;
	cvt.u32.u64 	%r114, %rd42;
	mul.lo.s32 	%r115, %r114, 50;
	sub.s32 	%r116, %r7, %r115;
	or.b32  	%r117, %r113, %r116;
	setp.ne.s32 	%p4, %r117, 0;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB16_6;

	ld.local.f32 	%f86, [%rd4+12];
	cvt.ftz.f64.f32 	%fd7, %f86;
	ld.local.f32 	%f87, [%rd4+16];
	cvt.ftz.f64.f32 	%fd8, %f87;
	ld.local.f32 	%f88, [%rd4+20];
	cvt.ftz.f64.f32 	%fd9, %f88;
	ld.local.u32 	%r118, [%rd4+56];
	st.local.v2.u32 	[%rd6], {%r7, %r8};
	st.local.v2.u32 	[%rd6+8], {%r4, %r118};
	st.local.f64 	[%rd6+16], %fd7;
	st.local.f64 	[%rd6+24], %fd8;
	st.local.f64 	[%rd6+32], %fd9;
	mov.u64 	%rd43, $str$2;
	cvta.global.u64 	%rd44, %rd43;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd44;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r119, [retval0+0];
	} // callseq 2

$L__BB16_6:
	add.ftz.f32 	%f91, %f5, 0f3F000000;
	ld.global.f32 	%f92, [%rd1+108];
	ld.global.f32 	%f93, [%rd1+92];
	mul.ftz.f32 	%f94, %f93, %f92;
	ld.global.f32 	%f95, [%rd1+124];
	ld.global.f32 	%f96, [%rd1+120];
	ld.global.f32 	%f97, [%rd1+104];
	mul.ftz.f32 	%f98, %f97, %f96;
	ld.global.f32 	%f99, [%rd1+100];
	mul.ftz.f32 	%f100, %f99, %f98;
	fma.rn.ftz.f32 	%f101, %f94, %f95, %f100;
	ld.global.f32 	%f102, [%rd1+116];
	ld.global.f32 	%f103, [%rd1+96];
	mul.ftz.f32 	%f104, %f103, %f102;
	ld.global.f32 	%f105, [%rd1+112];
	fma.rn.ftz.f32 	%f106, %f105, %f104, %f101;
	mul.ftz.f32 	%f107, %f92, %f102;
	mul.ftz.f32 	%f108, %f99, %f107;
	sub.ftz.f32 	%f109, %f106, %f108;
	mul.ftz.f32 	%f110, %f103, %f97;
	mul.ftz.f32 	%f111, %f110, %f95;
	sub.ftz.f32 	%f112, %f109, %f111;
	mul.ftz.f32 	%f113, %f93, %f96;
	mul.ftz.f32 	%f114, %f105, %f113;
	sub.ftz.f32 	%f115, %f112, %f114;
	mul.ftz.f32 	%f116, %f92, %f95;
	mul.ftz.f32 	%f117, %f105, %f96;
	sub.ftz.f32 	%f118, %f116, %f117;
	mul.ftz.f32 	%f119, %f97, %f95;
	mul.ftz.f32 	%f120, %f105, %f102;
	sub.ftz.f32 	%f121, %f119, %f120;
	sub.ftz.f32 	%f122, %f98, %f107;
	mul.ftz.f32 	%f123, %f103, %f95;
	mul.ftz.f32 	%f124, %f99, %f96;
	sub.ftz.f32 	%f125, %f123, %f124;
	mul.ftz.f32 	%f126, %f93, %f95;
	mul.ftz.f32 	%f127, %f99, %f102;
	sub.ftz.f32 	%f128, %f126, %f127;
	sub.ftz.f32 	%f129, %f113, %f104;
	mul.ftz.f32 	%f130, %f103, %f105;
	mul.ftz.f32 	%f131, %f99, %f92;
	sub.ftz.f32 	%f132, %f130, %f131;
	mul.ftz.f32 	%f133, %f93, %f105;
	mul.ftz.f32 	%f134, %f99, %f97;
	sub.ftz.f32 	%f135, %f133, %f134;
	sub.ftz.f32 	%f136, %f94, %f110;
	rcp.approx.ftz.f32 	%f137, %f115;
	mov.f32 	%f138, 0f3F800000;
	mul.ftz.f32 	%f139, %f118, %f137;
	mul.ftz.f32 	%f140, %f125, %f137;
	mul.ftz.f32 	%f141, %f132, %f137;
	mul.ftz.f32 	%f142, %f121, %f137;
	mul.ftz.f32 	%f143, %f128, %f137;
	mul.ftz.f32 	%f144, %f135, %f137;
	mul.ftz.f32 	%f145, %f122, %f137;
	mul.ftz.f32 	%f146, %f129, %f137;
	mul.ftz.f32 	%f147, %f136, %f137;
	ld.global.f32 	%f148, [%rd1+80];
	ld.local.f32 	%f13, [%rd4+24];
	sub.ftz.f32 	%f149, %f13, %f148;
	ld.global.f32 	%f150, [%rd1+84];
	ld.local.f32 	%f151, [%rd4+28];
	sub.ftz.f32 	%f152, %f151, %f150;
	ld.global.f32 	%f153, [%rd1+88];
	ld.local.f32 	%f154, [%rd4+32];
	sub.ftz.f32 	%f155, %f154, %f153;
	mul.ftz.f32 	%f156, %f149, %f139;
	mul.ftz.f32 	%f157, %f152, %f142;
	sub.ftz.f32 	%f158, %f156, %f157;
	fma.rn.ftz.f32 	%f159, %f155, %f145, %f158;
	mul.ftz.f32 	%f160, %f149, %f140;
	mul.ftz.f32 	%f161, %f152, %f143;
	sub.ftz.f32 	%f162, %f161, %f160;
	mul.ftz.f32 	%f163, %f155, %f146;
	sub.ftz.f32 	%f164, %f162, %f163;
	mul.ftz.f32 	%f165, %f149, %f141;
	mul.ftz.f32 	%f166, %f152, %f144;
	sub.ftz.f32 	%f167, %f165, %f166;
	fma.rn.ftz.f32 	%f168, %f155, %f147, %f167;
	div.approx.ftz.f32 	%f169, %f159, %f168;
	div.approx.ftz.f32 	%f170, %f164, %f168;
	ld.global.f32 	%f171, [%rd1+76];
	mov.f32 	%f172, 0f40000000;
	div.approx.ftz.f32 	%f173, %f171, %f172;
	sin.approx.ftz.f32 	%f174, %f173;
	cos.approx.ftz.f32 	%f175, %f173;
	div.approx.ftz.f32 	%f176, %f174, %f175;
	add.ftz.f32 	%f177, %f176, %f176;
	ld.global.f32 	%f178, [%rd1+72];
	mul.ftz.f32 	%f179, %f178, %f177;
	fma.rn.ftz.f32 	%f180, %f179, 0f3F000000, %f169;
	div.approx.ftz.f32 	%f181, %f180, %f179;
	sub.ftz.f32 	%f182, %f138, %f181;
	fma.rn.ftz.f32 	%f183, %f177, 0f3F000000, %f170;
	div.approx.ftz.f32 	%f184, %f183, %f177;
	sub.ftz.f32 	%f185, %f138, %f184;
	ld.global.v2.u32 	{%r120, %r121}, [%rd2];
	cvt.rn.f32.s32 	%f186, %r120;
	cvt.rn.f32.s32 	%f187, %r121;
	mul.ftz.f32 	%f188, %f182, %f186;
	mul.ftz.f32 	%f189, %f185, %f187;
	add.ftz.f32 	%f190, %f6, 0f3F000000;
	sub.ftz.f32 	%f15, %f190, %f189;
	sub.ftz.f32 	%f14, %f91, %f188;
	ld.global.u8 	%rs2, [%rd1+144];
	and.b16  	%rs3, %rs2, 32;
	setp.ne.s16 	%p6, %rs3, 0;
	mov.f32 	%f235, %f234;
	@%p6 bra 	$L__BB16_8;

	abs.ftz.f32 	%f191, %f13;
	setp.le.ftz.f32 	%p7, %f191, 0f7F800000;
	selp.f32 	%f235, %f15, 0f00000000, %p7;
	selp.f32 	%f234, %f14, 0f00000000, %p7;

$L__BB16_8:
	ld.local.u32 	%r124, [%rd4+48];
	ld.local.u32 	%r125, [%rd4+52];
	ld.local.u32 	%r126, [%rd4+56];
	ld.local.u32 	%r127, [%rd4+60];
	shr.u32 	%r128, %r3, 1;
	cvt.u64.u32 	%rd46, %r128;
	and.b64  	%rd47, %rd46, 8;
	add.s64 	%rd48, %rd2, %rd47;
	add.s64 	%rd7, %rd48, 32;
	ld.global.u64 	%rd49, [%rd48+32];
	shl.b32 	%r5, %r7, 4;
	sust.b.2d.v4.b32.trap 	[%rd49, {%r5, %r8}], {%r124, %r125, %r126, %r127};
	mov.b32 	%r129, %f234;
	mov.b32 	%r130, %f235;
	ld.global.u64 	%rd50, [%rd48+48];
	shl.b32 	%r131, %r7, 3;
	sust.b.2d.v2.b32.trap 	[%rd50, {%r131, %r8}], {%r129, %r130};
	ld.global.u32 	%r132, [%rd1+136];
	setp.ne.s32 	%p8, %r7, %r132;
	@%p8 bra 	$L__BB16_11;

	ld.global.u32 	%r133, [%rd1+140];
	setp.ne.s32 	%p9, %r8, %r133;
	@%p9 bra 	$L__BB16_11;

	ld.local.u32 	%r134, [%rd4+48];
	st.local.u32 	[%rd5], %r134;
	ld.local.u32 	%r135, [%rd4+52];
	st.local.u32 	[%rd5+4], %r135;
	ld.local.u32 	%r136, [%rd4+56];
	st.local.u32 	[%rd5+8], %r136;
	ld.local.f32 	%f192, [%rd4+12];
	ld.local.f32 	%f193, [%rd4+16];
	ld.local.f32 	%f194, [%rd4+20];
	st.local.f32 	[%rd5+16], %f192;
	st.local.f32 	[%rd5+20], %f193;
	st.local.f32 	[%rd5+24], %f194;
	ld.local.f32 	%f195, [%rd4+36];
	ld.local.f32 	%f196, [%rd4+40];
	ld.local.f32 	%f197, [%rd4+44];
	st.local.f32 	[%rd5+28], %f195;
	st.local.f32 	[%rd5+32], %f196;
	st.local.f32 	[%rd5+36], %f197;
	ld.local.f32 	%f198, [%rd4];
	ld.local.f32 	%f199, [%rd4+4];
	ld.local.f32 	%f200, [%rd4+8];
	st.local.f32 	[%rd5+40], %f198;
	st.local.f32 	[%rd5+44], %f199;
	st.local.f32 	[%rd5+48], %f200;
	ld.local.u32 	%r137, [%rd5+12];
	ld.local.f32 	%f201, [%rd5+52];
	ld.local.f32 	%f202, [%rd5+56];
	ld.local.f32 	%f203, [%rd5+60];
	ld.local.v4.u8 	{%rs4, %rs5, %rs6, %rs7}, [%rd5+64];
	ld.global.u64 	%rd51, [%rd7+192];
	st.u32 	[%rd51], %r134;
	st.u32 	[%rd51+4], %r135;
	st.u32 	[%rd51+8], %r136;
	st.u32 	[%rd51+12], %r137;
	st.f32 	[%rd51+16], %f192;
	st.f32 	[%rd51+20], %f193;
	st.f32 	[%rd51+24], %f194;
	st.f32 	[%rd51+28], %f195;
	st.f32 	[%rd51+32], %f196;
	st.f32 	[%rd51+36], %f197;
	st.f32 	[%rd51+40], %f198;
	st.f32 	[%rd51+44], %f199;
	st.f32 	[%rd51+48], %f200;
	st.f32 	[%rd51+52], %f201;
	st.f32 	[%rd51+56], %f202;
	st.f32 	[%rd51+60], %f203;
	st.v4.u8 	[%rd51+64], {%rs4, %rs5, %rs6, %rs7};

$L__BB16_11:
	ld.global.u32 	%r6, [%rd1+8];
	setp.eq.s32 	%p10, %r6, 0;
	ld.global.u64 	%rd8, [%rd2+200];
	@%p10 bra 	$L__BB16_13;

	suld.b.2d.v4.b32.trap {%r138, %r139, %r140, %r141}, [%rd8, {%r5, %r8}];
	mov.b32 	%f239, %r138;
	mov.b32 	%f240, %r139;
	mov.b32 	%f241, %r140;
	ld.global.u64 	%rd52, [%rd2+208];
	suld.b.2d.v4.b32.trap {%r142, %r143, %r144, %r145}, [%rd52, {%r5, %r8}];
	mov.b32 	%f236, %r142;
	mov.b32 	%f237, %r143;
	mov.b32 	%f238, %r144;
	bra.uni 	$L__BB16_14;

$L__BB16_13:
	mov.f32 	%f236, 0f00000000;
	mov.f32 	%f237, %f236;
	mov.f32 	%f238, %f236;
	mov.f32 	%f239, %f236;
	mov.f32 	%f240, %f236;
	mov.f32 	%f241, %f236;

$L__BB16_14:
	add.s32 	%r146, %r6, 1;
	cvt.rn.f32.u32 	%f210, %r146;
	rcp.approx.ftz.f32 	%f211, %f210;
	mov.f32 	%f212, 0f3F800000;
	sub.ftz.f32 	%f213, %f212, %f211;
	mul.ftz.f32 	%f214, %f239, %f213;
	mul.ftz.f32 	%f215, %f240, %f213;
	mul.ftz.f32 	%f216, %f241, %f213;
	ld.local.f32 	%f217, [%rd4];
	ld.local.f32 	%f218, [%rd4+4];
	ld.local.f32 	%f219, [%rd4+8];
	fma.rn.ftz.f32 	%f220, %f217, %f211, %f214;
	fma.rn.ftz.f32 	%f221, %f218, %f211, %f215;
	fma.rn.ftz.f32 	%f222, %f219, %f211, %f216;
	mov.b32 	%r147, %f220;
	mov.b32 	%r148, %f221;
	mov.b32 	%r149, %f222;
	mov.u32 	%r150, 1065353216;
	sust.b.2d.v4.b32.trap 	[%rd8, {%r5, %r8}], {%r147, %r148, %r149, %r150};
	ld.local.f32 	%f223, [%rd4+36];
	mul.ftz.f32 	%f224, %f211, %f223;
	ld.local.f32 	%f225, [%rd4+40];
	mul.ftz.f32 	%f226, %f211, %f225;
	ld.local.f32 	%f227, [%rd4+44];
	mul.ftz.f32 	%f228, %f211, %f227;
	fma.rn.ftz.f32 	%f229, %f236, %f213, %f224;
	fma.rn.ftz.f32 	%f230, %f237, %f213, %f226;
	fma.rn.ftz.f32 	%f231, %f238, %f213, %f228;
	mov.b32 	%r151, %f229;
	mov.b32 	%r152, %f230;
	mov.b32 	%r153, %f231;
	ld.global.u64 	%rd53, [%rd2+208];
	sust.b.2d.v4.b32.trap 	[%rd53, {%r5, %r8}], {%r151, %r152, %r153, %r150};
	ret;

}
	// .globl	__closesthit__setupGBuffers
.visible .entry __closesthit__setupGBuffers()
{
	.local .align 16 .b8 	__local_depot17[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<59>;
	.reg .b16 	%rs<6>;
	.reg .f32 	%f<992>;
	.reg .b32 	%r<521>;
	.reg .f64 	%fd<13>;
	.reg .b64 	%rd<444>;


	mov.u64 	%SPL, __local_depot17;
	cvta.local.u64 	%SP, %SPL;
	add.u64 	%rd34, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	// begin inline asm
	call (%r23), _optix_get_launch_index_x, ();
	// end inline asm
	// begin inline asm
	call (%r24), _optix_get_launch_index_y, ();
	// end inline asm
	ld.const.u64 	%rd35, [plp+8];
	cvta.to.global.u64 	%rd1, %rd35;
	ld.global.u32 	%r3, [%rd1+144];
	mul.wide.u32 	%rd36, %r23, 1374389535;
	shr.u64 	%rd37, %rd36, 37;
	cvt.u32.u64 	%r25, %rd37;
	mul.lo.s32 	%r26, %r25, 100;
	sub.s32 	%r27, %r23, %r26;
	mul.wide.u32 	%rd38, %r24, 1374389535;
	shr.u64 	%rd39, %rd38, 37;
	cvt.u32.u64 	%r28, %rd39;
	mul.lo.s32 	%r29, %r28, 100;
	sub.s32 	%r30, %r24, %r29;
	or.b32  	%r31, %r30, %r27;
	setp.ne.s32 	%p2, %r31, 0;
	@%p2 bra 	$L__BB17_2;

	// begin inline asm
	call (%r32), _optix_read_instance_id, ();
	// end inline asm
	// begin inline asm
	call (%f348), _optix_get_ray_tmax, ();
	// end inline asm
	cvt.ftz.f64.f32 	%fd1, %f348;
	st.local.v2.u32 	[%rd2], {%r23, %r24};
	st.local.u32 	[%rd2+8], %r32;
	st.local.f64 	[%rd2+16], %fd1;
	mov.u64 	%rd40, $str$3;
	cvta.global.u64 	%rd41, %rd40;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd41;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd34;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r33, [retval0+0];
	} // callseq 3

$L__BB17_2:
	// begin inline asm
	call (%rd43), _optix_get_sbt_data_ptr_64, ();
	// end inline asm
	ld.u32 	%r45, [%rd43];
	ld.const.u64 	%rd44, [plp];
	cvta.to.global.u64 	%rd45, %rd44;
	// begin inline asm
	call (%r34), _optix_read_instance_id, ();
	// end inline asm
	mov.u32 	%r38, 1;
	cvt.u64.u32 	%rd46, %r3;
	and.b64  	%rd47, %rd46, 16;
	add.s64 	%rd48, %rd45, %rd47;
	ld.global.u64 	%rd3, [%rd48+80];
	cvt.u64.u32 	%rd4, %r45;
	ld.global.u64 	%rd49, [%rd45+112];
	mul.wide.u32 	%rd50, %r45, 64;
	add.s64 	%rd51, %rd49, %rd50;
	add.s64 	%rd5, %rd51, 56;
	ld.global.u64 	%rd6, [%rd45+64];
	ld.u32 	%r5, [%rd51+56];
	mov.u32 	%r36, 0;
	// begin inline asm
	call (%r35), _optix_get_payload, (%r36);
	// end inline asm
	// begin inline asm
	call (%r37), _optix_get_payload, (%r38);
	// end inline asm
	mov.b64 	%rd7, {%r35, %r37};
	mov.u32 	%r40, 2;
	mov.u32 	%r42, 3;
	// begin inline asm
	call (%r39), _optix_get_payload, (%r40);
	// end inline asm
	// begin inline asm
	call (%r41), _optix_get_payload, (%r42);
	// end inline asm
	mov.b64 	%rd8, {%r39, %r41};
	// begin inline asm
	call (%f349, %f350), _optix_get_triangle_barycentrics, ();
	// end inline asm
	// begin inline asm
	call (%r43), _optix_read_primitive_idx, ();
	// end inline asm
	// begin inline asm
	call (%r44), _optix_read_instance_id, ();
	// end inline asm
	st.u32 	[%rd7+48], %r44;
	st.u32 	[%rd7+52], %r45;
	st.u32 	[%rd7+56], %r43;
	setp.eq.s32 	%p3, %r24, 300;
	setp.eq.s32 	%p4, %r23, 400;
	and.pred  	%p1, %p4, %p3;
	not.pred 	%p5, %p1;
	@%p5 bra 	$L__BB17_4;

	cvt.u32.u64 	%r46, %rd4;
	st.local.v2.u32 	[%rd2], {%r43, %r46};
	cvt.ftz.f64.f32 	%fd2, %f349;
	st.local.f64 	[%rd2+8], %fd2;
	cvt.ftz.f64.f32 	%fd3, %f350;
	st.local.f64 	[%rd2+16], %fd3;
	mov.u64 	%rd52, $str$4;
	cvta.global.u64 	%rd53, %rd52;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd53;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd34;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r47, [retval0+0];
	} // callseq 4

$L__BB17_4:
	ld.u64 	%rd55, [%rd5+-40];
	mul.wide.s32 	%rd56, %r43, 12;
	add.s64 	%rd57, %rd55, %rd56;
	ld.u64 	%rd58, [%rd5+-56];
	ld.u32 	%r49, [%rd57];
	ld.u32 	%r50, [%rd57+4];
	ld.u32 	%r51, [%rd57+8];
	add.ftz.f32 	%f351, %f349, %f350;
	mov.f32 	%f352, 0f3F800000;
	sub.ftz.f32 	%f353, %f352, %f351;
	mul.ftz.f32 	%f354, %f349, 0f477FFF00;
	cvt.rzi.ftz.u32.f32 	%r52, %f354;
	min.u32 	%r53, %r52, 65535;
	mul.ftz.f32 	%f355, %f350, 0f477FFF00;
	cvt.rzi.ftz.u32.f32 	%r54, %f355;
	min.u32 	%r55, %r54, 65535;
	cvt.u16.u32 	%rs1, %r55;
	cvt.u16.u32 	%rs2, %r53;
	st.v2.u16 	[%rd7+60], {%rs2, %rs1};
	mul.wide.u32 	%rd59, %r49, 44;
	add.s64 	%rd60, %rd58, %rd59;
	ld.f32 	%f356, [%rd60];
	ld.f32 	%f357, [%rd60+4];
	ld.f32 	%f358, [%rd60+8];
	mul.wide.u32 	%rd61, %r50, 44;
	add.s64 	%rd62, %rd58, %rd61;
	ld.f32 	%f359, [%rd62];
	mul.ftz.f32 	%f360, %f349, %f359;
	ld.f32 	%f361, [%rd62+4];
	mul.ftz.f32 	%f362, %f349, %f361;
	ld.f32 	%f363, [%rd62+8];
	mul.ftz.f32 	%f364, %f349, %f363;
	fma.rn.ftz.f32 	%f365, %f353, %f356, %f360;
	fma.rn.ftz.f32 	%f366, %f353, %f357, %f362;
	fma.rn.ftz.f32 	%f367, %f353, %f358, %f364;
	mul.wide.u32 	%rd63, %r51, 44;
	add.s64 	%rd64, %rd58, %rd63;
	ld.f32 	%f368, [%rd64];
	ld.f32 	%f369, [%rd64+4];
	ld.f32 	%f370, [%rd64+8];
	fma.rn.ftz.f32 	%f922, %f350, %f368, %f365;
	fma.rn.ftz.f32 	%f923, %f350, %f369, %f366;
	fma.rn.ftz.f32 	%f924, %f350, %f370, %f367;
	ld.f32 	%f371, [%rd60+12];
	ld.f32 	%f372, [%rd60+16];
	ld.f32 	%f373, [%rd60+20];
	ld.f32 	%f374, [%rd62+12];
	mul.ftz.f32 	%f375, %f349, %f374;
	ld.f32 	%f376, [%rd62+16];
	mul.ftz.f32 	%f377, %f349, %f376;
	ld.f32 	%f378, [%rd62+20];
	mul.ftz.f32 	%f379, %f349, %f378;
	fma.rn.ftz.f32 	%f380, %f353, %f371, %f375;
	fma.rn.ftz.f32 	%f381, %f353, %f372, %f377;
	fma.rn.ftz.f32 	%f382, %f353, %f373, %f379;
	ld.f32 	%f383, [%rd64+12];
	ld.f32 	%f384, [%rd64+16];
	ld.f32 	%f385, [%rd64+20];
	fma.rn.ftz.f32 	%f980, %f350, %f383, %f380;
	fma.rn.ftz.f32 	%f981, %f350, %f384, %f381;
	fma.rn.ftz.f32 	%f8, %f350, %f385, %f382;
	ld.f32 	%f386, [%rd60+36];
	ld.f32 	%f387, [%rd60+40];
	ld.f32 	%f388, [%rd62+36];
	mul.ftz.f32 	%f389, %f349, %f388;
	ld.f32 	%f390, [%rd62+40];
	mul.ftz.f32 	%f391, %f349, %f390;
	fma.rn.ftz.f32 	%f392, %f353, %f386, %f389;
	fma.rn.ftz.f32 	%f393, %f353, %f387, %f391;
	ld.f32 	%f394, [%rd64+36];
	ld.f32 	%f395, [%rd64+40];
	fma.rn.ftz.f32 	%f9, %f350, %f394, %f392;
	fma.rn.ftz.f32 	%f10, %f350, %f395, %f393;
	// begin inline asm
	call (%r48), _optix_get_transform_list_size, ();
	// end inline asm
	setp.eq.s32 	%p6, %r48, 0;
	@%p6 bra 	$L__BB17_23;

	// begin inline asm
	call (%r56), _optix_get_transform_list_size, ();
	// end inline asm
	// begin inline asm
	call (%f396), _optix_get_ray_time, ();
	// end inline asm
	setp.lt.s32 	%p7, %r56, 1;
	@%p7 bra 	$L__BB17_22;

	add.s32 	%r517, %r56, 1;
	mov.u32 	%r518, 1;

$L__BB17_7:
	.pragma "nounroll";
	add.s32 	%r58, %r517, -2;
	// begin inline asm
	call (%rd65), _optix_get_transform_list_handle, (%r58);
	// end inline asm
	// begin inline asm
	call (%r59), _optix_get_transform_type_from_handle, (%rd65);
	// end inline asm
	or.b32  	%r60, %r59, 1;
	setp.eq.s32 	%p8, %r60, 3;
	@%p8 bra 	$L__BB17_13;
	bra.uni 	$L__BB17_8;

$L__BB17_13:
	setp.eq.s32 	%p11, %r59, 2;
	@%p11 bra 	$L__BB17_17;
	bra.uni 	$L__BB17_14;

$L__BB17_17:
	// begin inline asm
	call (%rd137), _optix_get_matrix_motion_transform_from_handle, (%rd65);
	// end inline asm
	// begin inline asm
	cvta.to.global.u64 %rd139, %rd137;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r148,%r149,%r150,%r151}, [%rd139];
	// end inline asm
	add.s64 	%rd143, %rd137, 16;
	// begin inline asm
	cvta.to.global.u64 %rd142, %rd143;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r152,%r153,%r154,%r155}, [%rd142];
	// end inline asm
	add.s64 	%rd146, %rd137, 32;
	// begin inline asm
	cvta.to.global.u64 %rd145, %rd146;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r156,%r157,%r158,%r159}, [%rd145];
	// end inline asm
	add.s64 	%rd149, %rd137, 48;
	// begin inline asm
	cvta.to.global.u64 %rd148, %rd149;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r160,%r161,%r162,%r163}, [%rd148];
	// end inline asm
	add.s64 	%rd152, %rd137, 64;
	// begin inline asm
	cvta.to.global.u64 %rd151, %rd152;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r164,%r165,%r166,%r167}, [%rd151];
	// end inline asm
	add.s64 	%rd155, %rd137, 80;
	// begin inline asm
	cvta.to.global.u64 %rd154, %rd155;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r168,%r169,%r170,%r171}, [%rd154];
	// end inline asm
	add.s64 	%rd158, %rd137, 96;
	// begin inline asm
	cvta.to.global.u64 %rd157, %rd158;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r172,%r173,%r174,%r175}, [%rd157];
	// end inline asm
	add.s64 	%rd161, %rd137, 112;
	// begin inline asm
	cvta.to.global.u64 %rd160, %rd161;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r176,%r177,%r178,%r179}, [%rd160];
	// end inline asm
	mov.b32 	%f517, %r151;
	mov.b32 	%f518, %r152;
	and.b32  	%r192, %r150, 65535;
	add.s32 	%r193, %r192, -1;
	cvt.rn.f32.s32 	%f519, %r193;
	sub.ftz.f32 	%f520, %f396, %f517;
	sub.ftz.f32 	%f521, %f518, %f517;
	div.approx.ftz.f32 	%f522, %f520, %f521;
	mul.ftz.f32 	%f523, %f522, %f519;
	min.ftz.f32 	%f524, %f519, %f523;
	mov.f32 	%f525, 0f00000000;
	max.ftz.f32 	%f526, %f525, %f524;
	setp.num.ftz.f32 	%p14, %f526, %f526;
	selp.f32 	%f527, %f526, 0f00000000, %p14;
	cvt.rmi.ftz.f32.f32 	%f528, %f527;
	add.ftz.f32 	%f529, %f519, 0fBF800000;
	min.ftz.f32 	%f530, %f528, %f529;
	sub.ftz.f32 	%f97, %f527, %f530;
	cvt.rzi.ftz.s32.f32 	%r194, %f530;
	cvt.s64.s32 	%rd15, %r194;
	mul.wide.s32 	%rd172, %r194, 48;
	add.s64 	%rd164, %rd146, %rd172;
	// begin inline asm
	cvta.to.global.u64 %rd163, %rd164;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r180,%r181,%r182,%r183}, [%rd163];
	// end inline asm
	mov.b32 	%f894, %r180;
	mov.b32 	%f895, %r181;
	mov.b32 	%f896, %r182;
	mov.b32 	%f897, %r183;
	add.s64 	%rd167, %rd164, 16;
	// begin inline asm
	cvta.to.global.u64 %rd166, %rd167;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r184,%r185,%r186,%r187}, [%rd166];
	// end inline asm
	mov.b32 	%f890, %r184;
	mov.b32 	%f891, %r185;
	mov.b32 	%f892, %r186;
	mov.b32 	%f893, %r187;
	add.s64 	%rd170, %rd164, 32;
	// begin inline asm
	cvta.to.global.u64 %rd169, %rd170;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r188,%r189,%r190,%r191}, [%rd169];
	// end inline asm
	mov.b32 	%f886, %r188;
	mov.b32 	%f887, %r189;
	mov.b32 	%f888, %r190;
	mov.b32 	%f889, %r191;
	setp.leu.ftz.f32 	%p15, %f97, 0f00000000;
	@%p15 bra 	$L__BB17_19;

	mov.f32 	%f531, 0f3F800000;
	sub.ftz.f32 	%f532, %f531, %f97;
	mul.lo.s64 	%rd182, %rd15, 48;
	add.s64 	%rd183, %rd137, %rd182;
	add.s64 	%rd174, %rd183, 80;
	// begin inline asm
	cvta.to.global.u64 %rd173, %rd174;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd173];
	// end inline asm
	mov.b32 	%f533, %r195;
	mov.b32 	%f534, %r196;
	mov.b32 	%f535, %r197;
	mov.b32 	%f536, %r198;
	mul.ftz.f32 	%f537, %f97, %f533;
	mul.ftz.f32 	%f538, %f97, %f534;
	mul.ftz.f32 	%f539, %f97, %f535;
	mul.ftz.f32 	%f540, %f97, %f536;
	fma.rn.ftz.f32 	%f894, %f532, %f894, %f537;
	fma.rn.ftz.f32 	%f895, %f532, %f895, %f538;
	fma.rn.ftz.f32 	%f896, %f532, %f896, %f539;
	fma.rn.ftz.f32 	%f897, %f532, %f897, %f540;
	add.s64 	%rd177, %rd183, 96;
	// begin inline asm
	cvta.to.global.u64 %rd176, %rd177;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd176];
	// end inline asm
	mov.b32 	%f541, %r199;
	mov.b32 	%f542, %r200;
	mov.b32 	%f543, %r201;
	mov.b32 	%f544, %r202;
	mul.ftz.f32 	%f545, %f97, %f541;
	mul.ftz.f32 	%f546, %f97, %f542;
	mul.ftz.f32 	%f547, %f97, %f543;
	mul.ftz.f32 	%f548, %f97, %f544;
	fma.rn.ftz.f32 	%f890, %f532, %f890, %f545;
	fma.rn.ftz.f32 	%f891, %f532, %f891, %f546;
	fma.rn.ftz.f32 	%f892, %f532, %f892, %f547;
	fma.rn.ftz.f32 	%f893, %f532, %f893, %f548;
	add.s64 	%rd180, %rd183, 112;
	// begin inline asm
	cvta.to.global.u64 %rd179, %rd180;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd179];
	// end inline asm
	mov.b32 	%f549, %r203;
	mov.b32 	%f550, %r204;
	mov.b32 	%f551, %r205;
	mov.b32 	%f552, %r206;
	mul.ftz.f32 	%f553, %f97, %f549;
	mul.ftz.f32 	%f554, %f97, %f550;
	mul.ftz.f32 	%f555, %f97, %f551;
	mul.ftz.f32 	%f556, %f97, %f552;
	fma.rn.ftz.f32 	%f886, %f532, %f886, %f553;
	fma.rn.ftz.f32 	%f887, %f532, %f887, %f554;
	fma.rn.ftz.f32 	%f888, %f532, %f888, %f555;
	fma.rn.ftz.f32 	%f889, %f532, %f889, %f556;
	bra.uni 	$L__BB17_19;

$L__BB17_8:
	mov.f32 	%f886, 0f00000000;
	mov.f32 	%f888, 0f3F800000;
	setp.eq.s32 	%p9, %r59, 4;
	@%p9 bra 	$L__BB17_11;

	setp.ne.s32 	%p10, %r59, 1;
	mov.f32 	%f887, %f886;
	mov.f32 	%f889, %f886;
	mov.f32 	%f890, %f886;
	mov.f32 	%f891, %f888;
	mov.f32 	%f892, %f886;
	mov.f32 	%f893, %f886;
	mov.f32 	%f894, %f888;
	mov.f32 	%f895, %f886;
	mov.f32 	%f896, %f886;
	mov.f32 	%f897, %f886;
	@%p10 bra 	$L__BB17_19;

	// begin inline asm
	call (%rd67), _optix_get_static_transform_from_handle, (%rd65);
	// end inline asm
	add.s64 	%rd441, %rd67, 16;
	bra.uni 	$L__BB17_12;

$L__BB17_14:
	// begin inline asm
	call (%rd80), _optix_get_srt_motion_transform_from_handle, (%rd65);
	// end inline asm
	// begin inline asm
	cvta.to.global.u64 %rd82, %rd80;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r73,%r74,%r75,%r76}, [%rd82];
	// end inline asm
	add.s64 	%rd86, %rd80, 16;
	// begin inline asm
	cvta.to.global.u64 %rd85, %rd86;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r77,%r78,%r79,%r80}, [%rd85];
	// end inline asm
	add.s64 	%rd89, %rd80, 32;
	// begin inline asm
	cvta.to.global.u64 %rd88, %rd89;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r81,%r82,%r83,%r84}, [%rd88];
	// end inline asm
	add.s64 	%rd92, %rd80, 48;
	// begin inline asm
	cvta.to.global.u64 %rd91, %rd92;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r85,%r86,%r87,%r88}, [%rd91];
	// end inline asm
	add.s64 	%rd95, %rd80, 64;
	// begin inline asm
	cvta.to.global.u64 %rd94, %rd95;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r89,%r90,%r91,%r92}, [%rd94];
	// end inline asm
	add.s64 	%rd98, %rd80, 80;
	// begin inline asm
	cvta.to.global.u64 %rd97, %rd98;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r93,%r94,%r95,%r96}, [%rd97];
	// end inline asm
	add.s64 	%rd101, %rd80, 96;
	// begin inline asm
	cvta.to.global.u64 %rd100, %rd101;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r97,%r98,%r99,%r100}, [%rd100];
	// end inline asm
	add.s64 	%rd104, %rd80, 112;
	// begin inline asm
	cvta.to.global.u64 %rd103, %rd104;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r101,%r102,%r103,%r104}, [%rd103];
	// end inline asm
	add.s64 	%rd107, %rd80, 128;
	// begin inline asm
	cvta.to.global.u64 %rd106, %rd107;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r105,%r106,%r107,%r108}, [%rd106];
	// end inline asm
	add.s64 	%rd110, %rd80, 144;
	// begin inline asm
	cvta.to.global.u64 %rd109, %rd110;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r109,%r110,%r111,%r112}, [%rd109];
	// end inline asm
	mov.b32 	%f411, %r76;
	mov.b32 	%f412, %r77;
	and.b32  	%r129, %r75, 65535;
	add.s32 	%r130, %r129, -1;
	cvt.rn.f32.s32 	%f413, %r130;
	sub.ftz.f32 	%f414, %f396, %f411;
	sub.ftz.f32 	%f415, %f412, %f411;
	div.approx.ftz.f32 	%f416, %f414, %f415;
	mul.ftz.f32 	%f417, %f416, %f413;
	min.ftz.f32 	%f418, %f413, %f417;
	mov.f32 	%f419, 0f00000000;
	max.ftz.f32 	%f420, %f419, %f418;
	setp.num.ftz.f32 	%p12, %f420, %f420;
	selp.f32 	%f421, %f420, 0f00000000, %p12;
	cvt.rmi.ftz.f32.f32 	%f422, %f421;
	add.ftz.f32 	%f423, %f413, 0fBF800000;
	min.ftz.f32 	%f424, %f422, %f423;
	sub.ftz.f32 	%f36, %f421, %f424;
	cvt.rzi.ftz.s32.f32 	%r131, %f424;
	mul.wide.s32 	%rd124, %r131, 64;
	add.s64 	%rd113, %rd89, %rd124;
	// begin inline asm
	cvta.to.global.u64 %rd112, %rd113;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r113,%r114,%r115,%r116}, [%rd112];
	// end inline asm
	mov.b32 	%f870, %r113;
	mov.b32 	%f871, %r114;
	mov.b32 	%f872, %r115;
	mov.b32 	%f873, %r116;
	add.s64 	%rd116, %rd113, 16;
	// begin inline asm
	cvta.to.global.u64 %rd115, %rd116;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r117,%r118,%r119,%r120}, [%rd115];
	// end inline asm
	mov.b32 	%f874, %r117;
	mov.b32 	%f875, %r118;
	mov.b32 	%f876, %r119;
	mov.b32 	%f877, %r120;
	add.s64 	%rd119, %rd113, 32;
	// begin inline asm
	cvta.to.global.u64 %rd118, %rd119;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r121,%r122,%r123,%r124}, [%rd118];
	// end inline asm
	mov.b32 	%f878, %r121;
	mov.b32 	%f879, %r122;
	mov.b32 	%f880, %r123;
	mov.b32 	%f881, %r124;
	add.s64 	%rd122, %rd113, 48;
	// begin inline asm
	cvta.to.global.u64 %rd121, %rd122;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r125,%r126,%r127,%r128}, [%rd121];
	// end inline asm
	mov.b32 	%f882, %r125;
	mov.b32 	%f883, %r126;
	mov.b32 	%f884, %r127;
	mov.b32 	%f885, %r128;
	setp.leu.ftz.f32 	%p13, %f36, 0f00000000;
	@%p13 bra 	$L__BB17_16;

	mov.f32 	%f425, 0f3F800000;
	sub.ftz.f32 	%f426, %f425, %f36;
	add.s64 	%rd126, %rd113, 64;
	// begin inline asm
	cvta.to.global.u64 %rd125, %rd126;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r132,%r133,%r134,%r135}, [%rd125];
	// end inline asm
	mov.b32 	%f427, %r132;
	mov.b32 	%f428, %r133;
	mov.b32 	%f429, %r134;
	mov.b32 	%f430, %r135;
	mul.ftz.f32 	%f431, %f36, %f427;
	mul.ftz.f32 	%f432, %f36, %f428;
	mul.ftz.f32 	%f433, %f36, %f429;
	mul.ftz.f32 	%f434, %f36, %f430;
	fma.rn.ftz.f32 	%f870, %f426, %f870, %f431;
	fma.rn.ftz.f32 	%f871, %f426, %f871, %f432;
	fma.rn.ftz.f32 	%f872, %f426, %f872, %f433;
	fma.rn.ftz.f32 	%f873, %f426, %f873, %f434;
	add.s64 	%rd129, %rd113, 80;
	// begin inline asm
	cvta.to.global.u64 %rd128, %rd129;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r136,%r137,%r138,%r139}, [%rd128];
	// end inline asm
	mov.b32 	%f435, %r136;
	mov.b32 	%f436, %r137;
	mov.b32 	%f437, %r138;
	mov.b32 	%f438, %r139;
	mul.ftz.f32 	%f439, %f36, %f435;
	mul.ftz.f32 	%f440, %f36, %f436;
	mul.ftz.f32 	%f441, %f36, %f437;
	mul.ftz.f32 	%f442, %f36, %f438;
	fma.rn.ftz.f32 	%f874, %f426, %f874, %f439;
	fma.rn.ftz.f32 	%f875, %f426, %f875, %f440;
	fma.rn.ftz.f32 	%f876, %f426, %f876, %f441;
	fma.rn.ftz.f32 	%f877, %f426, %f877, %f442;
	add.s64 	%rd132, %rd113, 96;
	// begin inline asm
	cvta.to.global.u64 %rd131, %rd132;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r140,%r141,%r142,%r143}, [%rd131];
	// end inline asm
	mov.b32 	%f443, %r140;
	mov.b32 	%f444, %r141;
	mov.b32 	%f445, %r142;
	mov.b32 	%f446, %r143;
	mul.ftz.f32 	%f447, %f36, %f443;
	mul.ftz.f32 	%f448, %f36, %f444;
	mul.ftz.f32 	%f449, %f36, %f445;
	mul.ftz.f32 	%f450, %f36, %f446;
	fma.rn.ftz.f32 	%f878, %f426, %f878, %f447;
	fma.rn.ftz.f32 	%f451, %f426, %f879, %f448;
	fma.rn.ftz.f32 	%f452, %f426, %f880, %f449;
	fma.rn.ftz.f32 	%f453, %f426, %f881, %f450;
	add.s64 	%rd135, %rd113, 112;
	// begin inline asm
	cvta.to.global.u64 %rd134, %rd135;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r144,%r145,%r146,%r147}, [%rd134];
	// end inline asm
	mov.b32 	%f454, %r144;
	mov.b32 	%f455, %r145;
	mov.b32 	%f456, %r146;
	mov.b32 	%f457, %r147;
	mul.ftz.f32 	%f458, %f36, %f454;
	mul.ftz.f32 	%f459, %f36, %f455;
	mul.ftz.f32 	%f460, %f36, %f456;
	mul.ftz.f32 	%f461, %f36, %f457;
	fma.rn.ftz.f32 	%f462, %f426, %f882, %f458;
	fma.rn.ftz.f32 	%f883, %f426, %f883, %f459;
	fma.rn.ftz.f32 	%f884, %f426, %f884, %f460;
	fma.rn.ftz.f32 	%f885, %f426, %f885, %f461;
	mul.ftz.f32 	%f463, %f452, %f452;
	fma.rn.ftz.f32 	%f464, %f451, %f451, %f463;
	fma.rn.ftz.f32 	%f465, %f453, %f453, %f464;
	fma.rn.ftz.f32 	%f466, %f462, %f462, %f465;
	rsqrt.approx.ftz.f32 	%f467, %f466;
	mul.ftz.f32 	%f879, %f451, %f467;
	mul.ftz.f32 	%f880, %f452, %f467;
	mul.ftz.f32 	%f881, %f453, %f467;
	mul.ftz.f32 	%f882, %f467, %f462;

$L__BB17_16:
	mul.ftz.f32 	%f468, %f880, %f880;
	mul.ftz.f32 	%f469, %f879, %f879;
	sub.ftz.f32 	%f470, %f469, %f468;
	mul.ftz.f32 	%f471, %f881, %f881;
	sub.ftz.f32 	%f472, %f470, %f471;
	fma.rn.ftz.f32 	%f473, %f882, %f882, %f472;
	mul.ftz.f32 	%f474, %f881, %f882;
	mul.ftz.f32 	%f475, %f879, %f880;
	sub.ftz.f32 	%f476, %f475, %f474;
	add.ftz.f32 	%f477, %f476, %f476;
	mul.ftz.f32 	%f478, %f880, %f882;
	mul.ftz.f32 	%f479, %f879, %f881;
	add.ftz.f32 	%f480, %f479, %f478;
	add.ftz.f32 	%f481, %f480, %f480;
	add.ftz.f32 	%f482, %f475, %f474;
	add.ftz.f32 	%f483, %f482, %f482;
	sub.ftz.f32 	%f484, %f468, %f469;
	sub.ftz.f32 	%f485, %f484, %f471;
	fma.rn.ftz.f32 	%f486, %f882, %f882, %f485;
	mul.ftz.f32 	%f487, %f879, %f882;
	mul.ftz.f32 	%f488, %f880, %f881;
	sub.ftz.f32 	%f489, %f488, %f487;
	add.ftz.f32 	%f490, %f489, %f489;
	sub.ftz.f32 	%f491, %f479, %f478;
	add.ftz.f32 	%f492, %f491, %f491;
	add.ftz.f32 	%f493, %f488, %f487;
	add.ftz.f32 	%f494, %f493, %f493;
	neg.ftz.f32 	%f495, %f469;
	sub.ftz.f32 	%f496, %f495, %f468;
	add.ftz.f32 	%f497, %f496, %f471;
	fma.rn.ftz.f32 	%f498, %f882, %f882, %f497;
	mul.ftz.f32 	%f499, %f876, %f477;
	fma.rn.ftz.f32 	%f500, %f873, %f473, %f499;
	fma.rn.ftz.f32 	%f501, %f878, %f481, %f500;
	add.ftz.f32 	%f897, %f883, %f501;
	mul.ftz.f32 	%f502, %f873, %f483;
	fma.rn.ftz.f32 	%f503, %f876, %f486, %f502;
	fma.rn.ftz.f32 	%f504, %f878, %f490, %f503;
	add.ftz.f32 	%f893, %f884, %f504;
	mul.ftz.f32 	%f505, %f876, %f494;
	fma.rn.ftz.f32 	%f506, %f873, %f492, %f505;
	fma.rn.ftz.f32 	%f507, %f878, %f498, %f506;
	add.ftz.f32 	%f889, %f885, %f507;
	mul.ftz.f32 	%f508, %f875, %f477;
	fma.rn.ftz.f32 	%f509, %f872, %f473, %f508;
	fma.rn.ftz.f32 	%f896, %f877, %f481, %f509;
	mul.ftz.f32 	%f510, %f872, %f483;
	fma.rn.ftz.f32 	%f511, %f875, %f486, %f510;
	fma.rn.ftz.f32 	%f892, %f877, %f490, %f511;
	mul.ftz.f32 	%f512, %f875, %f494;
	fma.rn.ftz.f32 	%f513, %f872, %f492, %f512;
	fma.rn.ftz.f32 	%f888, %f877, %f498, %f513;
	mul.ftz.f32 	%f514, %f874, %f477;
	fma.rn.ftz.f32 	%f895, %f871, %f473, %f514;
	mul.ftz.f32 	%f515, %f871, %f483;
	fma.rn.ftz.f32 	%f891, %f874, %f486, %f515;
	mul.ftz.f32 	%f516, %f874, %f494;
	fma.rn.ftz.f32 	%f887, %f871, %f492, %f516;
	mul.ftz.f32 	%f894, %f870, %f473;
	mul.ftz.f32 	%f890, %f870, %f483;
	mul.ftz.f32 	%f886, %f870, %f492;
	bra.uni 	$L__BB17_19;

$L__BB17_11:
	// begin inline asm
	call (%rd441), _optix_get_instance_transform_from_handle, (%rd65);
	// end inline asm

$L__BB17_12:
	// begin inline asm
	cvta.to.global.u64 %rd71, %rd441;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r61,%r62,%r63,%r64}, [%rd71];
	// end inline asm
	mov.b32 	%f894, %r61;
	mov.b32 	%f895, %r62;
	mov.b32 	%f896, %r63;
	mov.b32 	%f897, %r64;
	add.s64 	%rd75, %rd441, 16;
	// begin inline asm
	cvta.to.global.u64 %rd74, %rd75;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r65,%r66,%r67,%r68}, [%rd74];
	// end inline asm
	mov.b32 	%f890, %r65;
	mov.b32 	%f891, %r66;
	mov.b32 	%f892, %r67;
	mov.b32 	%f893, %r68;
	add.s64 	%rd78, %rd441, 32;
	// begin inline asm
	cvta.to.global.u64 %rd77, %rd78;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r69,%r70,%r71,%r72}, [%rd77];
	// end inline asm
	mov.b32 	%f886, %r69;
	mov.b32 	%f887, %r70;
	mov.b32 	%f888, %r71;
	mov.b32 	%f889, %r72;

$L__BB17_19:
	setp.eq.s32 	%p16, %r518, 1;
	@%p16 bra 	$L__BB17_21;

	mul.ftz.f32 	%f557, %f865, %f895;
	fma.rn.ftz.f32 	%f558, %f861, %f894, %f557;
	fma.rn.ftz.f32 	%f134, %f869, %f896, %f558;
	mul.ftz.f32 	%f559, %f864, %f895;
	fma.rn.ftz.f32 	%f560, %f860, %f894, %f559;
	fma.rn.ftz.f32 	%f135, %f868, %f896, %f560;
	mul.ftz.f32 	%f561, %f863, %f895;
	fma.rn.ftz.f32 	%f562, %f859, %f894, %f561;
	fma.rn.ftz.f32 	%f136, %f867, %f896, %f562;
	mul.ftz.f32 	%f563, %f862, %f895;
	fma.rn.ftz.f32 	%f564, %f858, %f894, %f563;
	fma.rn.ftz.f32 	%f565, %f866, %f896, %f564;
	add.ftz.f32 	%f897, %f897, %f565;
	mul.ftz.f32 	%f566, %f865, %f891;
	fma.rn.ftz.f32 	%f567, %f861, %f890, %f566;
	fma.rn.ftz.f32 	%f138, %f869, %f892, %f567;
	mul.ftz.f32 	%f568, %f864, %f891;
	fma.rn.ftz.f32 	%f569, %f860, %f890, %f568;
	fma.rn.ftz.f32 	%f139, %f868, %f892, %f569;
	mul.ftz.f32 	%f570, %f863, %f891;
	fma.rn.ftz.f32 	%f571, %f859, %f890, %f570;
	fma.rn.ftz.f32 	%f140, %f867, %f892, %f571;
	mul.ftz.f32 	%f572, %f862, %f891;
	fma.rn.ftz.f32 	%f573, %f858, %f890, %f572;
	fma.rn.ftz.f32 	%f574, %f866, %f892, %f573;
	add.ftz.f32 	%f893, %f893, %f574;
	mul.ftz.f32 	%f575, %f865, %f887;
	fma.rn.ftz.f32 	%f576, %f861, %f886, %f575;
	fma.rn.ftz.f32 	%f142, %f869, %f888, %f576;
	mul.ftz.f32 	%f577, %f864, %f887;
	fma.rn.ftz.f32 	%f578, %f860, %f886, %f577;
	fma.rn.ftz.f32 	%f143, %f868, %f888, %f578;
	mul.ftz.f32 	%f579, %f863, %f887;
	fma.rn.ftz.f32 	%f580, %f859, %f886, %f579;
	fma.rn.ftz.f32 	%f144, %f867, %f888, %f580;
	mul.ftz.f32 	%f581, %f862, %f887;
	fma.rn.ftz.f32 	%f582, %f858, %f886, %f581;
	fma.rn.ftz.f32 	%f583, %f866, %f888, %f582;
	add.ftz.f32 	%f889, %f889, %f583;
	mov.f32 	%f886, %f142;
	mov.f32 	%f887, %f143;
	mov.f32 	%f888, %f144;
	mov.f32 	%f890, %f138;
	mov.f32 	%f891, %f139;
	mov.f32 	%f892, %f140;
	mov.f32 	%f894, %f134;
	mov.f32 	%f895, %f135;
	mov.f32 	%f896, %f136;

$L__BB17_21:
	add.s32 	%r518, %r518, -1;
	add.s32 	%r517, %r517, -1;
	setp.gt.s32 	%p17, %r517, 1;
	mov.f32 	%f858, %f897;
	mov.f32 	%f859, %f896;
	mov.f32 	%f860, %f895;
	mov.f32 	%f861, %f894;
	mov.f32 	%f862, %f893;
	mov.f32 	%f863, %f892;
	mov.f32 	%f864, %f891;
	mov.f32 	%f865, %f890;
	mov.f32 	%f866, %f889;
	mov.f32 	%f867, %f888;
	mov.f32 	%f868, %f887;
	mov.f32 	%f869, %f886;
	@%p17 bra 	$L__BB17_7;

$L__BB17_22:
	mul.ftz.f32 	%f584, %f923, %f895;
	fma.rn.ftz.f32 	%f585, %f922, %f894, %f584;
	fma.rn.ftz.f32 	%f586, %f924, %f896, %f585;
	mul.ftz.f32 	%f587, %f923, %f891;
	fma.rn.ftz.f32 	%f588, %f922, %f890, %f587;
	fma.rn.ftz.f32 	%f589, %f924, %f892, %f588;
	mul.ftz.f32 	%f590, %f923, %f887;
	fma.rn.ftz.f32 	%f591, %f922, %f886, %f590;
	fma.rn.ftz.f32 	%f592, %f924, %f888, %f591;
	add.ftz.f32 	%f924, %f889, %f592;
	add.ftz.f32 	%f923, %f893, %f589;
	add.ftz.f32 	%f922, %f897, %f586;

$L__BB17_23:
	mul.wide.u32 	%rd184, %r34, 216;
	add.s64 	%rd185, %rd3, %rd184;
	ld.f32 	%f593, [%rd185+64];
	ld.f32 	%f594, [%rd185+80];
	mul.ftz.f32 	%f595, %f923, %f594;
	fma.rn.ftz.f32 	%f596, %f593, %f922, %f595;
	ld.f32 	%f597, [%rd185+96];
	fma.rn.ftz.f32 	%f598, %f924, %f597, %f596;
	ld.f32 	%f599, [%rd185+112];
	add.ftz.f32 	%f182, %f599, %f598;
	ld.f32 	%f600, [%rd185+68];
	ld.f32 	%f601, [%rd185+84];
	mul.ftz.f32 	%f602, %f923, %f601;
	fma.rn.ftz.f32 	%f603, %f922, %f600, %f602;
	ld.f32 	%f604, [%rd185+100];
	fma.rn.ftz.f32 	%f605, %f924, %f604, %f603;
	ld.f32 	%f606, [%rd185+116];
	add.ftz.f32 	%f183, %f606, %f605;
	ld.f32 	%f607, [%rd185+72];
	ld.f32 	%f608, [%rd185+88];
	mul.ftz.f32 	%f609, %f923, %f608;
	fma.rn.ftz.f32 	%f610, %f922, %f607, %f609;
	ld.f32 	%f611, [%rd185+104];
	fma.rn.ftz.f32 	%f612, %f924, %f611, %f610;
	ld.f32 	%f613, [%rd185+120];
	add.ftz.f32 	%f184, %f613, %f612;
	// begin inline asm
	call (%r207), _optix_get_transform_list_size, ();
	// end inline asm
	setp.eq.s32 	%p18, %r207, 0;
	@%p18 bra 	$L__BB17_43;

	// begin inline asm
	call (%r208), _optix_get_transform_list_size, ();
	// end inline asm
	// begin inline asm
	call (%f614), _optix_get_ray_time, ();
	// end inline asm
	setp.eq.s32 	%p19, %r208, 0;
	@%p19 bra 	$L__BB17_42;

	mov.u32 	%r519, 0;

$L__BB17_26:
	.pragma "nounroll";
	// begin inline asm
	call (%rd186), _optix_get_transform_list_handle, (%r519);
	// end inline asm
	// begin inline asm
	call (%r211), _optix_get_transform_type_from_handle, (%rd186);
	// end inline asm
	or.b32  	%r212, %r211, 1;
	setp.eq.s32 	%p20, %r212, 3;
	@%p20 bra 	$L__BB17_32;
	bra.uni 	$L__BB17_27;

$L__BB17_32:
	setp.eq.s32 	%p23, %r211, 2;
	@%p23 bra 	$L__BB17_36;
	bra.uni 	$L__BB17_33;

$L__BB17_36:
	// begin inline asm
	call (%rd258), _optix_get_matrix_motion_transform_from_handle, (%rd186);
	// end inline asm
	// begin inline asm
	cvta.to.global.u64 %rd260, %rd258;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r300,%r301,%r302,%r303}, [%rd260];
	// end inline asm
	add.s64 	%rd264, %rd258, 16;
	// begin inline asm
	cvta.to.global.u64 %rd263, %rd264;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r304,%r305,%r306,%r307}, [%rd263];
	// end inline asm
	add.s64 	%rd267, %rd258, 32;
	// begin inline asm
	cvta.to.global.u64 %rd266, %rd267;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r308,%r309,%r310,%r311}, [%rd266];
	// end inline asm
	add.s64 	%rd270, %rd258, 48;
	// begin inline asm
	cvta.to.global.u64 %rd269, %rd270;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r312,%r313,%r314,%r315}, [%rd269];
	// end inline asm
	add.s64 	%rd273, %rd258, 64;
	// begin inline asm
	cvta.to.global.u64 %rd272, %rd273;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r316,%r317,%r318,%r319}, [%rd272];
	// end inline asm
	add.s64 	%rd276, %rd258, 80;
	// begin inline asm
	cvta.to.global.u64 %rd275, %rd276;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r320,%r321,%r322,%r323}, [%rd275];
	// end inline asm
	add.s64 	%rd279, %rd258, 96;
	// begin inline asm
	cvta.to.global.u64 %rd278, %rd279;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r324,%r325,%r326,%r327}, [%rd278];
	// end inline asm
	add.s64 	%rd282, %rd258, 112;
	// begin inline asm
	cvta.to.global.u64 %rd281, %rd282;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r328,%r329,%r330,%r331}, [%rd281];
	// end inline asm
	mov.b32 	%f711, %r303;
	mov.b32 	%f712, %r304;
	and.b32  	%r344, %r302, 65535;
	add.s32 	%r345, %r344, -1;
	cvt.rn.f32.s32 	%f713, %r345;
	sub.ftz.f32 	%f714, %f614, %f711;
	sub.ftz.f32 	%f715, %f712, %f711;
	div.approx.ftz.f32 	%f716, %f714, %f715;
	mul.ftz.f32 	%f717, %f716, %f713;
	min.ftz.f32 	%f718, %f713, %f717;
	mov.f32 	%f719, 0f00000000;
	max.ftz.f32 	%f720, %f719, %f718;
	setp.num.ftz.f32 	%p26, %f720, %f720;
	selp.f32 	%f721, %f720, 0f00000000, %p26;
	cvt.rmi.ftz.f32.f32 	%f722, %f721;
	add.ftz.f32 	%f723, %f713, 0fBF800000;
	min.ftz.f32 	%f724, %f722, %f723;
	sub.ftz.f32 	%f244, %f721, %f724;
	cvt.rzi.ftz.s32.f32 	%r346, %f724;
	cvt.s64.s32 	%rd22, %r346;
	mul.wide.s32 	%rd293, %r346, 48;
	add.s64 	%rd285, %rd267, %rd293;
	// begin inline asm
	cvta.to.global.u64 %rd284, %rd285;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r332,%r333,%r334,%r335}, [%rd284];
	// end inline asm
	mov.b32 	%f950, %r332;
	mov.b32 	%f951, %r333;
	mov.b32 	%f952, %r334;
	add.s64 	%rd288, %rd285, 16;
	// begin inline asm
	cvta.to.global.u64 %rd287, %rd288;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r336,%r337,%r338,%r339}, [%rd287];
	// end inline asm
	mov.b32 	%f947, %r336;
	mov.b32 	%f948, %r337;
	mov.b32 	%f949, %r338;
	add.s64 	%rd291, %rd285, 32;
	// begin inline asm
	cvta.to.global.u64 %rd290, %rd291;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r340,%r341,%r342,%r343}, [%rd290];
	// end inline asm
	mov.b32 	%f944, %r340;
	mov.b32 	%f945, %r341;
	mov.b32 	%f946, %r342;
	setp.leu.ftz.f32 	%p27, %f244, 0f00000000;
	@%p27 bra 	$L__BB17_38;

	mov.f32 	%f725, 0f3F800000;
	sub.ftz.f32 	%f726, %f725, %f244;
	mul.lo.s64 	%rd303, %rd22, 48;
	add.s64 	%rd304, %rd258, %rd303;
	add.s64 	%rd295, %rd304, 80;
	// begin inline asm
	cvta.to.global.u64 %rd294, %rd295;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r347,%r348,%r349,%r350}, [%rd294];
	// end inline asm
	mov.b32 	%f727, %r347;
	mov.b32 	%f728, %r348;
	mov.b32 	%f729, %r349;
	mul.ftz.f32 	%f730, %f244, %f727;
	mul.ftz.f32 	%f731, %f244, %f728;
	mul.ftz.f32 	%f732, %f244, %f729;
	fma.rn.ftz.f32 	%f950, %f726, %f950, %f730;
	fma.rn.ftz.f32 	%f951, %f726, %f951, %f731;
	fma.rn.ftz.f32 	%f952, %f726, %f952, %f732;
	add.s64 	%rd298, %rd304, 96;
	// begin inline asm
	cvta.to.global.u64 %rd297, %rd298;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r351,%r352,%r353,%r354}, [%rd297];
	// end inline asm
	mov.b32 	%f733, %r351;
	mov.b32 	%f734, %r352;
	mov.b32 	%f735, %r353;
	mul.ftz.f32 	%f736, %f244, %f733;
	mul.ftz.f32 	%f737, %f244, %f734;
	mul.ftz.f32 	%f738, %f244, %f735;
	fma.rn.ftz.f32 	%f947, %f726, %f947, %f736;
	fma.rn.ftz.f32 	%f948, %f726, %f948, %f737;
	fma.rn.ftz.f32 	%f949, %f726, %f949, %f738;
	add.s64 	%rd301, %rd304, 112;
	// begin inline asm
	cvta.to.global.u64 %rd300, %rd301;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd300];
	// end inline asm
	mov.b32 	%f739, %r355;
	mov.b32 	%f740, %r356;
	mov.b32 	%f741, %r357;
	mul.ftz.f32 	%f742, %f244, %f739;
	mul.ftz.f32 	%f743, %f244, %f740;
	mul.ftz.f32 	%f744, %f244, %f741;
	fma.rn.ftz.f32 	%f944, %f726, %f944, %f742;
	fma.rn.ftz.f32 	%f945, %f726, %f945, %f743;
	fma.rn.ftz.f32 	%f946, %f726, %f946, %f744;
	bra.uni 	$L__BB17_38;

$L__BB17_27:
	mov.f32 	%f953, 0f00000000;
	mov.f32 	%f955, 0f3F800000;
	setp.eq.s32 	%p21, %r211, 4;
	@%p21 bra 	$L__BB17_30;

	setp.ne.s32 	%p22, %r211, 1;
	mov.f32 	%f954, %f953;
	mov.f32 	%f956, %f953;
	mov.f32 	%f957, %f955;
	mov.f32 	%f958, %f953;
	mov.f32 	%f959, %f955;
	mov.f32 	%f960, %f953;
	mov.f32 	%f961, %f953;
	@%p22 bra 	$L__BB17_39;

	// begin inline asm
	call (%rd188), _optix_get_static_transform_from_handle, (%rd186);
	// end inline asm
	add.s64 	%rd442, %rd188, 64;
	bra.uni 	$L__BB17_31;

$L__BB17_33:
	// begin inline asm
	call (%rd201), _optix_get_srt_motion_transform_from_handle, (%rd186);
	// end inline asm
	// begin inline asm
	cvta.to.global.u64 %rd203, %rd201;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r225,%r226,%r227,%r228}, [%rd203];
	// end inline asm
	add.s64 	%rd207, %rd201, 16;
	// begin inline asm
	cvta.to.global.u64 %rd206, %rd207;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r229,%r230,%r231,%r232}, [%rd206];
	// end inline asm
	add.s64 	%rd210, %rd201, 32;
	// begin inline asm
	cvta.to.global.u64 %rd209, %rd210;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r233,%r234,%r235,%r236}, [%rd209];
	// end inline asm
	add.s64 	%rd213, %rd201, 48;
	// begin inline asm
	cvta.to.global.u64 %rd212, %rd213;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r237,%r238,%r239,%r240}, [%rd212];
	// end inline asm
	add.s64 	%rd216, %rd201, 64;
	// begin inline asm
	cvta.to.global.u64 %rd215, %rd216;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r241,%r242,%r243,%r244}, [%rd215];
	// end inline asm
	add.s64 	%rd219, %rd201, 80;
	// begin inline asm
	cvta.to.global.u64 %rd218, %rd219;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r245,%r246,%r247,%r248}, [%rd218];
	// end inline asm
	add.s64 	%rd222, %rd201, 96;
	// begin inline asm
	cvta.to.global.u64 %rd221, %rd222;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r249,%r250,%r251,%r252}, [%rd221];
	// end inline asm
	add.s64 	%rd225, %rd201, 112;
	// begin inline asm
	cvta.to.global.u64 %rd224, %rd225;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r253,%r254,%r255,%r256}, [%rd224];
	// end inline asm
	add.s64 	%rd228, %rd201, 128;
	// begin inline asm
	cvta.to.global.u64 %rd227, %rd228;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r257,%r258,%r259,%r260}, [%rd227];
	// end inline asm
	add.s64 	%rd231, %rd201, 144;
	// begin inline asm
	cvta.to.global.u64 %rd230, %rd231;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r261,%r262,%r263,%r264}, [%rd230];
	// end inline asm
	mov.b32 	%f626, %r228;
	mov.b32 	%f627, %r229;
	and.b32  	%r281, %r227, 65535;
	add.s32 	%r282, %r281, -1;
	cvt.rn.f32.s32 	%f628, %r282;
	sub.ftz.f32 	%f629, %f614, %f626;
	sub.ftz.f32 	%f630, %f627, %f626;
	div.approx.ftz.f32 	%f631, %f629, %f630;
	mul.ftz.f32 	%f632, %f631, %f628;
	min.ftz.f32 	%f633, %f628, %f632;
	mov.f32 	%f634, 0f00000000;
	max.ftz.f32 	%f635, %f634, %f633;
	setp.num.ftz.f32 	%p24, %f635, %f635;
	selp.f32 	%f636, %f635, 0f00000000, %p24;
	cvt.rmi.ftz.f32.f32 	%f637, %f636;
	add.ftz.f32 	%f638, %f628, 0fBF800000;
	min.ftz.f32 	%f639, %f637, %f638;
	sub.ftz.f32 	%f204, %f636, %f639;
	cvt.rzi.ftz.s32.f32 	%r283, %f639;
	mul.wide.s32 	%rd245, %r283, 64;
	add.s64 	%rd234, %rd210, %rd245;
	// begin inline asm
	cvta.to.global.u64 %rd233, %rd234;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r265,%r266,%r267,%r268}, [%rd233];
	// end inline asm
	mov.b32 	%f934, %r265;
	mov.b32 	%f935, %r266;
	mov.b32 	%f936, %r267;
	add.s64 	%rd237, %rd234, 16;
	// begin inline asm
	cvta.to.global.u64 %rd236, %rd237;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r269,%r270,%r271,%r272}, [%rd236];
	// end inline asm
	mov.b32 	%f937, %r269;
	mov.b32 	%f938, %r270;
	mov.b32 	%f939, %r272;
	add.s64 	%rd240, %rd234, 32;
	// begin inline asm
	cvta.to.global.u64 %rd239, %rd240;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r273,%r274,%r275,%r276}, [%rd239];
	// end inline asm
	mov.b32 	%f940, %r274;
	mov.b32 	%f941, %r275;
	mov.b32 	%f942, %r276;
	add.s64 	%rd243, %rd234, 48;
	// begin inline asm
	cvta.to.global.u64 %rd242, %rd243;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r277,%r278,%r279,%r280}, [%rd242];
	// end inline asm
	mov.b32 	%f943, %r277;
	setp.leu.ftz.f32 	%p25, %f204, 0f00000000;
	@%p25 bra 	$L__BB17_35;

	mov.f32 	%f640, 0f3F800000;
	sub.ftz.f32 	%f641, %f640, %f204;
	add.s64 	%rd247, %rd234, 64;
	// begin inline asm
	cvta.to.global.u64 %rd246, %rd247;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r284,%r285,%r286,%r287}, [%rd246];
	// end inline asm
	mov.b32 	%f642, %r284;
	mov.b32 	%f643, %r285;
	mov.b32 	%f644, %r286;
	mul.ftz.f32 	%f645, %f204, %f642;
	mul.ftz.f32 	%f646, %f204, %f643;
	mul.ftz.f32 	%f647, %f204, %f644;
	fma.rn.ftz.f32 	%f934, %f641, %f934, %f645;
	fma.rn.ftz.f32 	%f935, %f641, %f935, %f646;
	fma.rn.ftz.f32 	%f936, %f641, %f936, %f647;
	add.s64 	%rd250, %rd234, 80;
	// begin inline asm
	cvta.to.global.u64 %rd249, %rd250;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r288,%r289,%r290,%r291}, [%rd249];
	// end inline asm
	mov.b32 	%f648, %r288;
	mov.b32 	%f649, %r289;
	mov.b32 	%f650, %r291;
	mul.ftz.f32 	%f651, %f204, %f648;
	mul.ftz.f32 	%f652, %f204, %f649;
	mul.ftz.f32 	%f653, %f204, %f650;
	fma.rn.ftz.f32 	%f937, %f641, %f937, %f651;
	fma.rn.ftz.f32 	%f938, %f641, %f938, %f652;
	fma.rn.ftz.f32 	%f939, %f641, %f939, %f653;
	add.s64 	%rd253, %rd234, 96;
	// begin inline asm
	cvta.to.global.u64 %rd252, %rd253;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r292,%r293,%r294,%r295}, [%rd252];
	// end inline asm
	mov.b32 	%f654, %r293;
	mov.b32 	%f655, %r294;
	mov.b32 	%f656, %r295;
	mul.ftz.f32 	%f657, %f204, %f654;
	mul.ftz.f32 	%f658, %f204, %f655;
	mul.ftz.f32 	%f659, %f204, %f656;
	fma.rn.ftz.f32 	%f660, %f641, %f940, %f657;
	fma.rn.ftz.f32 	%f661, %f641, %f941, %f658;
	fma.rn.ftz.f32 	%f662, %f641, %f942, %f659;
	add.s64 	%rd256, %rd234, 112;
	// begin inline asm
	cvta.to.global.u64 %rd255, %rd256;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r296,%r297,%r298,%r299}, [%rd255];
	// end inline asm
	mov.b32 	%f663, %r296;
	mul.ftz.f32 	%f664, %f204, %f663;
	fma.rn.ftz.f32 	%f665, %f641, %f943, %f664;
	mul.ftz.f32 	%f666, %f661, %f661;
	fma.rn.ftz.f32 	%f667, %f660, %f660, %f666;
	fma.rn.ftz.f32 	%f668, %f662, %f662, %f667;
	fma.rn.ftz.f32 	%f669, %f665, %f665, %f668;
	rsqrt.approx.ftz.f32 	%f670, %f669;
	mul.ftz.f32 	%f940, %f660, %f670;
	mul.ftz.f32 	%f941, %f661, %f670;
	mul.ftz.f32 	%f942, %f662, %f670;
	mul.ftz.f32 	%f943, %f670, %f665;

$L__BB17_35:
	mul.ftz.f32 	%f671, %f941, %f941;
	mul.ftz.f32 	%f672, %f940, %f940;
	sub.ftz.f32 	%f673, %f672, %f671;
	mul.ftz.f32 	%f674, %f942, %f942;
	sub.ftz.f32 	%f675, %f673, %f674;
	fma.rn.ftz.f32 	%f676, %f943, %f943, %f675;
	mul.ftz.f32 	%f677, %f942, %f943;
	mul.ftz.f32 	%f678, %f940, %f941;
	sub.ftz.f32 	%f679, %f678, %f677;
	add.ftz.f32 	%f680, %f679, %f679;
	mul.ftz.f32 	%f681, %f941, %f943;
	mul.ftz.f32 	%f682, %f940, %f942;
	add.ftz.f32 	%f683, %f682, %f681;
	add.ftz.f32 	%f684, %f683, %f683;
	add.ftz.f32 	%f685, %f678, %f677;
	add.ftz.f32 	%f686, %f685, %f685;
	sub.ftz.f32 	%f687, %f671, %f672;
	sub.ftz.f32 	%f688, %f687, %f674;
	fma.rn.ftz.f32 	%f689, %f943, %f943, %f688;
	mul.ftz.f32 	%f690, %f940, %f943;
	mul.ftz.f32 	%f691, %f941, %f942;
	sub.ftz.f32 	%f692, %f691, %f690;
	add.ftz.f32 	%f693, %f692, %f692;
	sub.ftz.f32 	%f694, %f682, %f681;
	add.ftz.f32 	%f695, %f694, %f694;
	add.ftz.f32 	%f696, %f691, %f690;
	add.ftz.f32 	%f697, %f696, %f696;
	neg.ftz.f32 	%f698, %f672;
	sub.ftz.f32 	%f699, %f698, %f671;
	add.ftz.f32 	%f700, %f699, %f674;
	fma.rn.ftz.f32 	%f701, %f943, %f943, %f700;
	mul.ftz.f32 	%f702, %f938, %f680;
	fma.rn.ftz.f32 	%f703, %f936, %f676, %f702;
	fma.rn.ftz.f32 	%f952, %f939, %f684, %f703;
	mul.ftz.f32 	%f704, %f936, %f686;
	fma.rn.ftz.f32 	%f705, %f938, %f689, %f704;
	fma.rn.ftz.f32 	%f949, %f939, %f693, %f705;
	mul.ftz.f32 	%f706, %f938, %f697;
	fma.rn.ftz.f32 	%f707, %f936, %f695, %f706;
	fma.rn.ftz.f32 	%f946, %f939, %f701, %f707;
	mul.ftz.f32 	%f708, %f937, %f680;
	fma.rn.ftz.f32 	%f951, %f935, %f676, %f708;
	mul.ftz.f32 	%f709, %f935, %f686;
	fma.rn.ftz.f32 	%f948, %f937, %f689, %f709;
	mul.ftz.f32 	%f710, %f937, %f697;
	fma.rn.ftz.f32 	%f945, %f935, %f695, %f710;
	mul.ftz.f32 	%f950, %f934, %f676;
	mul.ftz.f32 	%f947, %f934, %f686;
	mul.ftz.f32 	%f944, %f934, %f695;

$L__BB17_38:
	mul.ftz.f32 	%f745, %f945, %f949;
	mul.ftz.f32 	%f746, %f946, %f948;
	sub.ftz.f32 	%f747, %f746, %f745;
	mul.ftz.f32 	%f748, %f950, %f747;
	mul.ftz.f32 	%f749, %f944, %f949;
	mul.ftz.f32 	%f750, %f946, %f947;
	sub.ftz.f32 	%f751, %f750, %f749;
	mul.ftz.f32 	%f752, %f751, %f951;
	sub.ftz.f32 	%f753, %f748, %f752;
	mul.ftz.f32 	%f754, %f944, %f948;
	mul.ftz.f32 	%f755, %f945, %f947;
	sub.ftz.f32 	%f756, %f755, %f754;
	fma.rn.ftz.f32 	%f757, %f756, %f952, %f753;
	rcp.approx.ftz.f32 	%f758, %f757;
	mul.ftz.f32 	%f959, %f747, %f758;
	mul.ftz.f32 	%f759, %f946, %f951;
	mul.ftz.f32 	%f760, %f945, %f952;
	sub.ftz.f32 	%f761, %f760, %f759;
	mul.ftz.f32 	%f960, %f761, %f758;
	mul.ftz.f32 	%f762, %f948, %f952;
	mul.ftz.f32 	%f763, %f949, %f951;
	sub.ftz.f32 	%f764, %f763, %f762;
	mul.ftz.f32 	%f961, %f764, %f758;
	sub.ftz.f32 	%f765, %f749, %f750;
	mul.ftz.f32 	%f956, %f765, %f758;
	mul.ftz.f32 	%f766, %f944, %f952;
	mul.ftz.f32 	%f767, %f946, %f950;
	sub.ftz.f32 	%f768, %f767, %f766;
	mul.ftz.f32 	%f957, %f768, %f758;
	mul.ftz.f32 	%f769, %f949, %f950;
	mul.ftz.f32 	%f770, %f947, %f952;
	sub.ftz.f32 	%f771, %f770, %f769;
	mul.ftz.f32 	%f958, %f771, %f758;
	mul.ftz.f32 	%f953, %f756, %f758;
	mul.ftz.f32 	%f772, %f945, %f950;
	mul.ftz.f32 	%f773, %f944, %f951;
	sub.ftz.f32 	%f774, %f773, %f772;
	mul.ftz.f32 	%f954, %f774, %f758;
	mul.ftz.f32 	%f775, %f947, %f951;
	mul.ftz.f32 	%f776, %f948, %f950;
	sub.ftz.f32 	%f777, %f776, %f775;
	mul.ftz.f32 	%f955, %f777, %f758;
	bra.uni 	$L__BB17_39;

$L__BB17_30:
	// begin inline asm
	call (%rd442), _optix_get_instance_inverse_transform_from_handle, (%rd186);
	// end inline asm

$L__BB17_31:
	// begin inline asm
	cvta.to.global.u64 %rd192, %rd442;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r213,%r214,%r215,%r216}, [%rd192];
	// end inline asm
	mov.b32 	%f959, %r213;
	mov.b32 	%f960, %r214;
	mov.b32 	%f961, %r215;
	add.s64 	%rd196, %rd442, 16;
	// begin inline asm
	cvta.to.global.u64 %rd195, %rd196;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r217,%r218,%r219,%r220}, [%rd195];
	// end inline asm
	mov.b32 	%f956, %r217;
	mov.b32 	%f957, %r218;
	mov.b32 	%f958, %r219;
	add.s64 	%rd199, %rd442, 32;
	// begin inline asm
	cvta.to.global.u64 %rd198, %rd199;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r221,%r222,%r223,%r224}, [%rd198];
	// end inline asm
	mov.b32 	%f953, %r221;
	mov.b32 	%f954, %r222;
	mov.b32 	%f955, %r223;

$L__BB17_39:
	setp.eq.s32 	%p28, %r519, 0;
	@%p28 bra 	$L__BB17_41;

	mul.ftz.f32 	%f778, %f930, %f960;
	fma.rn.ftz.f32 	%f779, %f927, %f959, %f778;
	fma.rn.ftz.f32 	%f290, %f933, %f961, %f779;
	mul.ftz.f32 	%f780, %f929, %f960;
	fma.rn.ftz.f32 	%f781, %f926, %f959, %f780;
	fma.rn.ftz.f32 	%f291, %f932, %f961, %f781;
	mul.ftz.f32 	%f782, %f928, %f960;
	fma.rn.ftz.f32 	%f783, %f925, %f959, %f782;
	fma.rn.ftz.f32 	%f961, %f931, %f961, %f783;
	mul.ftz.f32 	%f784, %f930, %f957;
	fma.rn.ftz.f32 	%f785, %f927, %f956, %f784;
	fma.rn.ftz.f32 	%f293, %f933, %f958, %f785;
	mul.ftz.f32 	%f786, %f929, %f957;
	fma.rn.ftz.f32 	%f787, %f926, %f956, %f786;
	fma.rn.ftz.f32 	%f294, %f932, %f958, %f787;
	mul.ftz.f32 	%f788, %f928, %f957;
	fma.rn.ftz.f32 	%f789, %f925, %f956, %f788;
	fma.rn.ftz.f32 	%f958, %f931, %f958, %f789;
	mul.ftz.f32 	%f790, %f930, %f954;
	fma.rn.ftz.f32 	%f791, %f927, %f953, %f790;
	fma.rn.ftz.f32 	%f296, %f933, %f955, %f791;
	mul.ftz.f32 	%f792, %f929, %f954;
	fma.rn.ftz.f32 	%f793, %f926, %f953, %f792;
	fma.rn.ftz.f32 	%f297, %f932, %f955, %f793;
	mul.ftz.f32 	%f794, %f928, %f954;
	fma.rn.ftz.f32 	%f795, %f925, %f953, %f794;
	fma.rn.ftz.f32 	%f955, %f931, %f955, %f795;
	mov.f32 	%f953, %f296;
	mov.f32 	%f954, %f297;
	mov.f32 	%f956, %f293;
	mov.f32 	%f957, %f294;
	mov.f32 	%f959, %f290;
	mov.f32 	%f960, %f291;

$L__BB17_41:
	add.s32 	%r519, %r519, 1;
	setp.lt.u32 	%p29, %r519, %r208;
	mov.f32 	%f925, %f961;
	mov.f32 	%f926, %f960;
	mov.f32 	%f927, %f959;
	mov.f32 	%f928, %f958;
	mov.f32 	%f929, %f957;
	mov.f32 	%f930, %f956;
	mov.f32 	%f931, %f955;
	mov.f32 	%f932, %f954;
	mov.f32 	%f933, %f953;
	@%p29 bra 	$L__BB17_26;

$L__BB17_42:
	mul.ftz.f32 	%f796, %f980, %f959;
	fma.rn.ftz.f32 	%f797, %f981, %f956, %f796;
	mul.ftz.f32 	%f798, %f980, %f960;
	fma.rn.ftz.f32 	%f799, %f981, %f957, %f798;
	mul.ftz.f32 	%f800, %f980, %f961;
	fma.rn.ftz.f32 	%f801, %f981, %f958, %f800;
	fma.rn.ftz.f32 	%f982, %f8, %f955, %f801;
	fma.rn.ftz.f32 	%f981, %f8, %f954, %f799;
	fma.rn.ftz.f32 	%f980, %f8, %f953, %f797;
	bra.uni 	$L__BB17_44;

$L__BB17_43:
	mov.f32 	%f982, %f8;

$L__BB17_44:
	mul.ftz.f32 	%f802, %f981, %f981;
	fma.rn.ftz.f32 	%f803, %f980, %f980, %f802;
	fma.rn.ftz.f32 	%f804, %f982, %f982, %f803;
	rsqrt.approx.ftz.f32 	%f805, %f804;
	mul.ftz.f32 	%f985, %f980, %f805;
	mul.ftz.f32 	%f984, %f981, %f805;
	mul.ftz.f32 	%f983, %f982, %f805;
	// begin inline asm
	call (%r359), _optix_get_transform_list_size, ();
	// end inline asm
	setp.eq.s32 	%p30, %r359, 0;
	@%p30 bra 	$L__BB17_59;

	// begin inline asm
	call (%r360), _optix_get_transform_list_size, ();
	// end inline asm
	// begin inline asm
	call (%f806), _optix_get_ray_time, ();
	// end inline asm
	setp.lt.s32 	%p31, %r360, 1;
	@%p31 bra 	$L__BB17_59;

	add.s32 	%r520, %r360, 1;

$L__BB17_47:
	.pragma "nounroll";
	add.s32 	%r361, %r520, -2;
	// begin inline asm
	call (%rd305), _optix_get_transform_list_handle, (%r361);
	// end inline asm
	// begin inline asm
	call (%r362), _optix_get_transform_type_from_handle, (%rd305);
	// end inline asm
	or.b32  	%r363, %r362, 1;
	setp.eq.s32 	%p32, %r363, 3;
	@%p32 bra 	$L__BB17_53;
	bra.uni 	$L__BB17_48;

$L__BB17_53:
	setp.eq.s32 	%p35, %r362, 2;
	@%p35 bra 	$L__BB17_56;
	bra.uni 	$L__BB17_54;

$L__BB17_56:
	// begin inline asm
	call (%rd377), _optix_get_matrix_motion_transform_from_handle, (%rd305);
	// end inline asm
	// begin inline asm
	cvta.to.global.u64 %rd379, %rd377;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r451,%r452,%r453,%r454}, [%rd379];
	// end inline asm
	add.s64 	%rd383, %rd377, 16;
	// begin inline asm
	cvta.to.global.u64 %rd382, %rd383;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r455,%r456,%r457,%r458}, [%rd382];
	// end inline asm
	add.s64 	%rd386, %rd377, 32;
	// begin inline asm
	cvta.to.global.u64 %rd385, %rd386;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r459,%r460,%r461,%r462}, [%rd385];
	// end inline asm
	add.s64 	%rd389, %rd377, 48;
	// begin inline asm
	cvta.to.global.u64 %rd388, %rd389;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r463,%r464,%r465,%r466}, [%rd388];
	// end inline asm
	add.s64 	%rd392, %rd377, 64;
	// begin inline asm
	cvta.to.global.u64 %rd391, %rd392;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r467,%r468,%r469,%r470}, [%rd391];
	// end inline asm
	add.s64 	%rd395, %rd377, 80;
	// begin inline asm
	cvta.to.global.u64 %rd394, %rd395;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r471,%r472,%r473,%r474}, [%rd394];
	// end inline asm
	add.s64 	%rd398, %rd377, 96;
	// begin inline asm
	cvta.to.global.u64 %rd397, %rd398;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r475,%r476,%r477,%r478}, [%rd397];
	// end inline asm
	add.s64 	%rd401, %rd377, 112;
	// begin inline asm
	cvta.to.global.u64 %rd400, %rd401;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r479,%r480,%r481,%r482}, [%rd400];
	// end inline asm
	mov.b32 	%f822, %r454;
	mov.b32 	%f823, %r455;
	and.b32  	%r495, %r453, 65535;
	add.s32 	%r496, %r495, -1;
	cvt.rn.f32.s32 	%f824, %r496;
	sub.ftz.f32 	%f825, %f806, %f822;
	sub.ftz.f32 	%f826, %f823, %f822;
	div.approx.ftz.f32 	%f827, %f825, %f826;
	mul.ftz.f32 	%f828, %f827, %f824;
	min.ftz.f32 	%f829, %f824, %f828;
	mov.f32 	%f830, 0f00000000;
	max.ftz.f32 	%f831, %f830, %f829;
	setp.num.ftz.f32 	%p38, %f831, %f831;
	selp.f32 	%f832, %f831, 0f00000000, %p38;
	cvt.rmi.ftz.f32.f32 	%f833, %f832;
	add.ftz.f32 	%f834, %f824, 0fBF800000;
	min.ftz.f32 	%f835, %f833, %f834;
	sub.ftz.f32 	%f836, %f832, %f835;
	cvt.rzi.ftz.s32.f32 	%r497, %f835;
	cvt.s64.s32 	%rd29, %r497;
	mul.wide.s32 	%rd412, %r497, 48;
	add.s64 	%rd404, %rd386, %rd412;
	// begin inline asm
	cvta.to.global.u64 %rd403, %rd404;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r483,%r484,%r485,%r486}, [%rd403];
	// end inline asm
	add.s64 	%rd407, %rd404, 16;
	// begin inline asm
	cvta.to.global.u64 %rd406, %rd407;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r487,%r488,%r489,%r490}, [%rd406];
	// end inline asm
	add.s64 	%rd410, %rd404, 32;
	// begin inline asm
	cvta.to.global.u64 %rd409, %rd410;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r491,%r492,%r493,%r494}, [%rd409];
	// end inline asm
	setp.leu.ftz.f32 	%p39, %f836, 0f00000000;
	@%p39 bra 	$L__BB17_58;

	mul.lo.s64 	%rd422, %rd29, 48;
	add.s64 	%rd423, %rd377, %rd422;
	add.s64 	%rd414, %rd423, 80;
	// begin inline asm
	cvta.to.global.u64 %rd413, %rd414;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r498,%r499,%r500,%r501}, [%rd413];
	// end inline asm
	add.s64 	%rd417, %rd423, 96;
	// begin inline asm
	cvta.to.global.u64 %rd416, %rd417;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r502,%r503,%r504,%r505}, [%rd416];
	// end inline asm
	add.s64 	%rd420, %rd423, 112;
	// begin inline asm
	cvta.to.global.u64 %rd419, %rd420;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r506,%r507,%r508,%r509}, [%rd419];
	// end inline asm
	bra.uni 	$L__BB17_58;

$L__BB17_48:
	setp.eq.s32 	%p33, %r362, 4;
	@%p33 bra 	$L__BB17_51;

	setp.ne.s32 	%p34, %r362, 1;
	@%p34 bra 	$L__BB17_58;

	// begin inline asm
	call (%rd307), _optix_get_static_transform_from_handle, (%rd305);
	// end inline asm
	add.s64 	%rd443, %rd307, 16;
	bra.uni 	$L__BB17_52;

$L__BB17_54:
	// begin inline asm
	call (%rd320), _optix_get_srt_motion_transform_from_handle, (%rd305);
	// end inline asm
	// begin inline asm
	cvta.to.global.u64 %rd322, %rd320;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r376,%r377,%r378,%r379}, [%rd322];
	// end inline asm
	add.s64 	%rd326, %rd320, 16;
	// begin inline asm
	cvta.to.global.u64 %rd325, %rd326;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r380,%r381,%r382,%r383}, [%rd325];
	// end inline asm
	add.s64 	%rd329, %rd320, 32;
	// begin inline asm
	cvta.to.global.u64 %rd328, %rd329;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r384,%r385,%r386,%r387}, [%rd328];
	// end inline asm
	add.s64 	%rd332, %rd320, 48;
	// begin inline asm
	cvta.to.global.u64 %rd331, %rd332;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r388,%r389,%r390,%r391}, [%rd331];
	// end inline asm
	add.s64 	%rd335, %rd320, 64;
	// begin inline asm
	cvta.to.global.u64 %rd334, %rd335;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r392,%r393,%r394,%r395}, [%rd334];
	// end inline asm
	add.s64 	%rd338, %rd320, 80;
	// begin inline asm
	cvta.to.global.u64 %rd337, %rd338;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r396,%r397,%r398,%r399}, [%rd337];
	// end inline asm
	add.s64 	%rd341, %rd320, 96;
	// begin inline asm
	cvta.to.global.u64 %rd340, %rd341;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r400,%r401,%r402,%r403}, [%rd340];
	// end inline asm
	add.s64 	%rd344, %rd320, 112;
	// begin inline asm
	cvta.to.global.u64 %rd343, %rd344;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r404,%r405,%r406,%r407}, [%rd343];
	// end inline asm
	add.s64 	%rd347, %rd320, 128;
	// begin inline asm
	cvta.to.global.u64 %rd346, %rd347;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r408,%r409,%r410,%r411}, [%rd346];
	// end inline asm
	add.s64 	%rd350, %rd320, 144;
	// begin inline asm
	cvta.to.global.u64 %rd349, %rd350;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r412,%r413,%r414,%r415}, [%rd349];
	// end inline asm
	mov.b32 	%f807, %r379;
	mov.b32 	%f808, %r380;
	and.b32  	%r432, %r378, 65535;
	add.s32 	%r433, %r432, -1;
	cvt.rn.f32.s32 	%f809, %r433;
	sub.ftz.f32 	%f810, %f806, %f807;
	sub.ftz.f32 	%f811, %f808, %f807;
	div.approx.ftz.f32 	%f812, %f810, %f811;
	mul.ftz.f32 	%f813, %f812, %f809;
	min.ftz.f32 	%f814, %f809, %f813;
	mov.f32 	%f815, 0f00000000;
	max.ftz.f32 	%f816, %f815, %f814;
	setp.num.ftz.f32 	%p36, %f816, %f816;
	selp.f32 	%f817, %f816, 0f00000000, %p36;
	cvt.rmi.ftz.f32.f32 	%f818, %f817;
	add.ftz.f32 	%f819, %f809, 0fBF800000;
	min.ftz.f32 	%f820, %f818, %f819;
	sub.ftz.f32 	%f821, %f817, %f820;
	cvt.rzi.ftz.s32.f32 	%r434, %f820;
	mul.wide.s32 	%rd364, %r434, 64;
	add.s64 	%rd353, %rd329, %rd364;
	// begin inline asm
	cvta.to.global.u64 %rd352, %rd353;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r416,%r417,%r418,%r419}, [%rd352];
	// end inline asm
	add.s64 	%rd356, %rd353, 16;
	// begin inline asm
	cvta.to.global.u64 %rd355, %rd356;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r420,%r421,%r422,%r423}, [%rd355];
	// end inline asm
	add.s64 	%rd359, %rd353, 32;
	// begin inline asm
	cvta.to.global.u64 %rd358, %rd359;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r424,%r425,%r426,%r427}, [%rd358];
	// end inline asm
	add.s64 	%rd362, %rd353, 48;
	// begin inline asm
	cvta.to.global.u64 %rd361, %rd362;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r428,%r429,%r430,%r431}, [%rd361];
	// end inline asm
	setp.leu.ftz.f32 	%p37, %f821, 0f00000000;
	@%p37 bra 	$L__BB17_58;

	add.s64 	%rd366, %rd353, 64;
	// begin inline asm
	cvta.to.global.u64 %rd365, %rd366;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r435,%r436,%r437,%r438}, [%rd365];
	// end inline asm
	add.s64 	%rd369, %rd353, 80;
	// begin inline asm
	cvta.to.global.u64 %rd368, %rd369;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r439,%r440,%r441,%r442}, [%rd368];
	// end inline asm
	add.s64 	%rd372, %rd353, 96;
	// begin inline asm
	cvta.to.global.u64 %rd371, %rd372;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r443,%r444,%r445,%r446}, [%rd371];
	// end inline asm
	add.s64 	%rd375, %rd353, 112;
	// begin inline asm
	cvta.to.global.u64 %rd374, %rd375;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r447,%r448,%r449,%r450}, [%rd374];
	// end inline asm
	bra.uni 	$L__BB17_58;

$L__BB17_51:
	// begin inline asm
	call (%rd443), _optix_get_instance_transform_from_handle, (%rd305);
	// end inline asm

$L__BB17_52:
	// begin inline asm
	cvta.to.global.u64 %rd311, %rd443;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r364,%r365,%r366,%r367}, [%rd311];
	// end inline asm
	add.s64 	%rd315, %rd443, 16;
	// begin inline asm
	cvta.to.global.u64 %rd314, %rd315;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r368,%r369,%r370,%r371}, [%rd314];
	// end inline asm
	add.s64 	%rd318, %rd443, 32;
	// begin inline asm
	cvta.to.global.u64 %rd317, %rd318;
	// end inline asm
	// begin inline asm
	ld.global.v4.u32 {%r372,%r373,%r374,%r375}, [%rd317];
	// end inline asm

$L__BB17_58:
	add.s32 	%r520, %r520, -1;
	setp.gt.s32 	%p40, %r520, 1;
	@%p40 bra 	$L__BB17_47;

$L__BB17_59:
	abs.ftz.f32 	%f837, %f985;
	setp.geu.ftz.f32 	%p41, %f837, 0f7F800000;
	@%p41 bra 	$L__BB17_62;

	abs.ftz.f32 	%f838, %f984;
	setp.geu.ftz.f32 	%p42, %f838, 0f7F800000;
	@%p42 bra 	$L__BB17_62;

	abs.ftz.f32 	%f839, %f983;
	setp.lt.ftz.f32 	%p43, %f839, 0f7F800000;
	@%p43 bra 	$L__BB17_63;

$L__BB17_62:
	mov.f32 	%f984, 0f00000000;
	mov.f32 	%f983, 0f3F800000;
	mov.f32 	%f985, %f984;

$L__BB17_63:
	setp.eq.s32 	%p58, %r24, 300;
	setp.eq.s32 	%p57, %r23, 400;
	and.pred  	%p56, %p57, %p58;
	not.pred 	%p55, %p56;
	@%p55 bra 	$L__BB17_65;

	add.u64 	%rd436, %SP, 0;
	add.u64 	%rd435, %SP, 0;
	add.u64 	%rd434, %SPL, 0;
	cvt.ftz.f64.f32 	%fd4, %f923;
	cvt.ftz.f64.f32 	%fd5, %f922;
	st.local.v2.f64 	[%rd434], {%fd5, %fd4};
	cvt.ftz.f64.f32 	%fd6, %f985;
	cvt.ftz.f64.f32 	%fd7, %f924;
	st.local.v2.f64 	[%rd434+16], {%fd7, %fd6};
	cvt.ftz.f64.f32 	%fd8, %f983;
	cvt.ftz.f64.f32 	%fd9, %f984;
	st.local.v2.f64 	[%rd434+32], {%fd9, %fd8};
	mov.u64 	%rd424, $str$5;
	cvta.global.u64 	%rd425, %rd424;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd425;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd435;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r510, [retval0+0];
	} // callseq 5

$L__BB17_65:
	st.f32 	[%rd7+12], %f922;
	st.f32 	[%rd7+16], %f923;
	st.f32 	[%rd7+20], %f924;
	st.f32 	[%rd7+24], %f182;
	st.f32 	[%rd7+28], %f183;
	st.f32 	[%rd7+32], %f184;
	st.f32 	[%rd7+36], %f985;
	st.f32 	[%rd7+40], %f984;
	st.f32 	[%rd7+44], %f983;
	mul.wide.u32 	%rd427, %r5, 272;
	add.s64 	%rd30, %rd6, %rd427;
	ld.u64 	%rd31, [%rd30];
	setp.eq.s64 	%p45, %rd31, 0;
	mov.f32 	%f986, 0f3F4CCCCD;
	mov.f32 	%f987, %f986;
	mov.f32 	%f988, %f986;
	@%p45 bra 	$L__BB17_67;

	mov.f32 	%f846, 0f00000000;
	tex.level.2d.v4.f32.f32 	{%f988, %f987, %f986, %f847}, [%rd31, {%f9, %f10}], %f846;

$L__BB17_67:
	st.f32 	[%rd7], %f988;
	st.f32 	[%rd7+4], %f987;
	st.f32 	[%rd7+8], %f986;
	ld.global.u8 	%rs3, [_ZZ27__closesthit__setupGBuffersE8firstHit];
	and.b16  	%rs4, %rs3, 1;
	setp.eq.b16 	%p46, %rs4, 1;
	setp.gt.u32 	%p47, %r23, 9;
	or.pred  	%p48, %p47, %p46;
	setp.gt.u32 	%p49, %r24, 9;
	or.pred  	%p50, %p49, %p48;
	@%p50 bra 	$L__BB17_69;

	add.u64 	%rd433, %SP, 0;
	add.u64 	%rd432, %SP, 0;
	add.u64 	%rd431, %SPL, 0;
	cvt.ftz.f64.f32 	%fd10, %f987;
	cvt.ftz.f64.f32 	%fd11, %f988;
	st.local.v2.f64 	[%rd431], {%fd11, %fd10};
	cvt.ftz.f64.f32 	%fd12, %f986;
	st.local.f64 	[%rd431+16], %fd12;
	st.local.v2.u32 	[%rd431+24], {%r23, %r24};
	mov.u64 	%rd428, $str$6;
	cvta.global.u64 	%rd429, %rd428;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd429;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd432;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r511, [retval0+0];
	} // callseq 6
	mov.u16 	%rs5, 1;
	st.global.u8 	[_ZZ27__closesthit__setupGBuffersE8firstHit], %rs5;

$L__BB17_69:
	ld.const.u64 	%rd438, [plp+8];
	cvta.to.global.u64 	%rd437, %rd438;
	ld.global.u32 	%r512, [%rd437+136];
	setp.ne.s32 	%p51, %r23, %r512;
	@%p51 bra 	$L__BB17_75;

	ld.const.u64 	%rd440, [plp+8];
	cvta.to.global.u64 	%rd439, %rd440;
	ld.global.u32 	%r513, [%rd439+140];
	setp.ne.s32 	%p52, %r24, %r513;
	@%p52 bra 	$L__BB17_75;

	ld.u32 	%r514, [%rd8+64];
	or.b32  	%r515, %r514, 1;
	st.u32 	[%rd8+64], %r515;
	ld.u32 	%r516, [%rd5];
	st.u32 	[%rd8+12], %r516;
	ld.u64 	%rd32, [%rd30+160];
	setp.eq.s64 	%p53, %rd32, 0;
	mov.f32 	%f989, 0f00000000;
	mov.f32 	%f990, %f989;
	mov.f32 	%f991, %f989;
	@%p53 bra 	$L__BB17_74;

	mov.f32 	%f851, 0f00000000;
	tex.level.2d.v4.f32.f32 	{%f991, %f990, %f989, %f852}, [%rd32, {%f9, %f10}], %f851;
	ld.u64 	%rd33, [%rd30+168];
	setp.eq.s64 	%p54, %rd33, 0;
	@%p54 bra 	$L__BB17_74;

	tex.level.2d.v4.f32.f32 	{%f854, %f855, %f856, %f857}, [%rd33, {%f9, %f10}], %f851;
	mul.ftz.f32 	%f991, %f991, %f854;
	mul.ftz.f32 	%f990, %f990, %f854;
	mul.ftz.f32 	%f989, %f989, %f854;

$L__BB17_74:
	st.f32 	[%rd8+52], %f991;
	st.f32 	[%rd8+56], %f990;
	st.f32 	[%rd8+60], %f989;

$L__BB17_75:
	ret;

}
	// .globl	__miss__setupGBuffers
.visible .entry __miss__setupGBuffers()
{
	.local .align 8 .b8 	__local_depot18[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<36>;
	.reg .b16 	%rs<5>;
	.reg .f32 	%f<163>;
	.reg .b32 	%r<72>;
	.reg .b64 	%rd<13>;


	mov.u64 	%SPL, __local_depot18;
	cvta.local.u64 	%SP, %SPL;
	// begin inline asm
	call (%r14), _optix_get_launch_index_x, ();
	// end inline asm
	// begin inline asm
	call (%r15), _optix_get_launch_index_y, ();
	// end inline asm
	// begin inline asm
	call (%f38), _optix_get_world_ray_direction_x, ();
	// end inline asm
	// begin inline asm
	call (%f39), _optix_get_world_ray_direction_y, ();
	// end inline asm
	// begin inline asm
	call (%f40), _optix_get_world_ray_direction_z, ();
	// end inline asm
	neg.ftz.f32 	%f4, %f38;
	abs.ftz.f32 	%f5, %f40;
	abs.ftz.f32 	%f6, %f4;
	setp.eq.ftz.f32 	%p1, %f5, 0f00000000;
	setp.eq.ftz.f32 	%p2, %f6, 0f00000000;
	and.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB18_4;
	bra.uni 	$L__BB18_1;

$L__BB18_4:
	mov.b32 	%r26, %f40;
	shr.s32 	%r27, %r26, 31;
	and.b32  	%r28, %r27, 1078530011;
	mov.b32 	%r29, %f4;
	and.b32  	%r30, %r29, -2147483648;
	or.b32  	%r31, %r28, %r30;
	mov.b32 	%f156, %r31;
	bra.uni 	$L__BB18_5;

$L__BB18_1:
	setp.eq.ftz.f32 	%p4, %f5, 0f7F800000;
	setp.eq.ftz.f32 	%p5, %f6, 0f7F800000;
	and.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB18_3;
	bra.uni 	$L__BB18_2;

$L__BB18_3:
	mov.b32 	%r21, %f40;
	setp.lt.s32 	%p10, %r21, 0;
	selp.b32 	%r22, 1075235812, 1061752795, %p10;
	mov.b32 	%r23, %f4;
	and.b32  	%r24, %r23, -2147483648;
	or.b32  	%r25, %r22, %r24;
	mov.b32 	%f156, %r25;
	bra.uni 	$L__BB18_5;

$L__BB18_2:
	max.ftz.f32 	%f41, %f6, %f5;
	min.ftz.f32 	%f42, %f6, %f5;
	div.full.ftz.f32 	%f43, %f42, %f41;
	mul.rn.ftz.f32 	%f44, %f43, %f43;
	mov.f32 	%f45, 0fC0B59883;
	mov.f32 	%f46, 0fBF52C7EA;
	fma.rn.ftz.f32 	%f47, %f44, %f46, %f45;
	mov.f32 	%f48, 0fC0D21907;
	fma.rn.ftz.f32 	%f49, %f47, %f44, %f48;
	mul.ftz.f32 	%f50, %f44, %f49;
	mul.ftz.f32 	%f51, %f43, %f50;
	add.ftz.f32 	%f52, %f44, 0f41355DC0;
	mov.f32 	%f53, 0f41E6BD60;
	fma.rn.ftz.f32 	%f54, %f52, %f44, %f53;
	mov.f32 	%f55, 0f419D92C8;
	fma.rn.ftz.f32 	%f56, %f54, %f44, %f55;
	rcp.approx.ftz.f32 	%f57, %f56;
	fma.rn.ftz.f32 	%f58, %f51, %f57, %f43;
	mov.f32 	%f59, 0f3FC90FDB;
	sub.ftz.f32 	%f60, %f59, %f58;
	setp.gt.ftz.f32 	%p7, %f6, %f5;
	selp.f32 	%f61, %f60, %f58, %p7;
	mov.b32 	%r16, %f40;
	setp.lt.s32 	%p8, %r16, 0;
	mov.f32 	%f62, 0f40490FDB;
	sub.ftz.f32 	%f63, %f62, %f61;
	selp.f32 	%f64, %f63, %f61, %p8;
	mov.b32 	%r17, %f64;
	mov.b32 	%r18, %f4;
	and.b32  	%r19, %r18, -2147483648;
	or.b32  	%r20, %r19, %r17;
	mov.b32 	%f65, %r20;
	add.ftz.f32 	%f66, %f5, %f6;
	setp.le.ftz.f32 	%p9, %f66, 0f7F800000;
	selp.f32 	%f156, %f65, %f66, %p9;

$L__BB18_5:
	add.ftz.f32 	%f11, %f156, 0f40C90FDB;
	mov.f32 	%f67, 0f40C90FDB;
	abs.ftz.f32 	%f159, %f11;
	abs.ftz.f32 	%f13, %f67;
	setp.lt.ftz.f32 	%p11, %f159, %f13;
	@%p11 bra 	$L__BB18_17;

	mul.ftz.f32 	%f14, %f13, 0f4B000000;
	setp.gtu.ftz.f32 	%p12, %f159, %f14;
	@%p12 bra 	$L__BB18_13;
	bra.uni 	$L__BB18_7;

$L__BB18_13:
	mov.b32 	%r4, %f159;
	mov.b32 	%r5, %f14;
	and.b32  	%r33, %r4, 8388607;
	or.b32  	%r70, %r33, 1065353216;
	mov.b32 	%f158, %r70;
	add.s32 	%r34, %r5, -192937984;
	and.b32  	%r35, %r34, -8388608;
	sub.s32 	%r36, %r4, %r35;
	and.b32  	%r71, %r36, -8388608;
	setp.eq.s32 	%p18, %r71, 0;
	@%p18 bra 	$L__BB18_16;

	and.b32  	%r37, %r5, 8388607;
	or.b32  	%r38, %r37, 1065353216;
	mov.b32 	%f76, %r38;
	rcp.approx.ftz.f32 	%f24, %f76;
	neg.ftz.f32 	%f25, %f76;

$L__BB18_15:
	min.u32 	%r39, %r71, 192937984;
	add.s32 	%r40, %r39, %r70;
	mov.b32 	%f77, %r40;
	mov.f32 	%f78, 0f80000000;
	fma.rn.ftz.f32 	%f79, %f77, %f24, %f78;
	fma.rn.ftz.f32 	%f80, %f25, %f79, %f77;
	fma.rn.ftz.f32 	%f81, %f80, %f24, %f79;
	fma.rn.ftz.f32 	%f82, %f25, %f81, %f77;
	fma.rz.ftz.f32 	%f83, %f82, %f24, %f81;
	cvt.rzi.f32.f32 	%f84, %f83;
	fma.rn.ftz.f32 	%f158, %f25, %f84, %f77;
	sub.s32 	%r71, %r71, %r39;
	mov.b32 	%r70, %f158;
	setp.ne.s32 	%p19, %r71, 0;
	setp.ne.s32 	%p20, %r70, 0;
	and.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB18_15;

$L__BB18_16:
	setp.gt.u32 	%p22, %r4, 2139095039;
	setp.leu.ftz.f32 	%p23, %f13, 0f00000000;
	or.pred  	%p24, %p23, %p22;
	and.b32  	%r41, %r5, -8388608;
	mov.b32 	%f85, %r41;
	selp.f32 	%f86, 0f7FFFFFFF, %f85, %p24;
	mul.ftz.f32 	%f87, %f158, 0f34000000;
	mul.ftz.f32 	%f159, %f86, %f87;
	bra.uni 	$L__BB18_17;

$L__BB18_7:
	div.approx.ftz.f32 	%f68, %f159, %f13;
	cvt.rzi.f32.f32 	%f157, %f68;
	neg.ftz.f32 	%f16, %f13;
	fma.rn.f32 	%f17, %f16, %f157, %f159;
	mov.b32 	%r3, %f17;
	mov.b32 	%r32, %f13;
	setp.lt.u32 	%p13, %r3, %r32;
	@%p13 bra 	$L__BB18_12;

	setp.gt.u32 	%p14, %r3, -2147483648;
	@%p14 bra 	$L__BB18_11;
	bra.uni 	$L__BB18_9;

$L__BB18_11:
	add.ftz.f32 	%f74, %f157, 0fBF800000;
	add.ftz.f32 	%f75, %f74, 0fBF800000;
	setp.lt.ftz.f32 	%p17, %f17, %f16;
	selp.f32 	%f157, %f75, %f74, %p17;
	bra.uni 	$L__BB18_12;

$L__BB18_9:
	add.ftz.f32 	%f157, %f157, 0f3F800000;
	add.ftz.f32 	%f69, %f13, %f13;
	setp.ltu.ftz.f32 	%p15, %f17, %f69;
	@%p15 bra 	$L__BB18_12;

	add.ftz.f32 	%f70, %f157, 0f3F800000;
	mov.f32 	%f71, 0fC0400000;
	fma.rn.ftz.f32 	%f72, %f71, %f13, %f17;
	setp.ge.ftz.f32 	%p16, %f72, 0f00000000;
	add.ftz.f32 	%f73, %f70, 0f3F800000;
	selp.f32 	%f157, %f73, %f70, %p16;

$L__BB18_12:
	fma.rn.ftz.f32 	%f159, %f16, %f157, %f159;

$L__BB18_17:
	mov.f32 	%f88, 0fBF800000;
	max.ftz.f32 	%f89, %f39, %f88;
	mov.f32 	%f90, 0f3F800000;
	min.ftz.f32 	%f91, %f89, %f90;
	abs.ftz.f32 	%f92, %f91;
	neg.ftz.f32 	%f93, %f92;
	mov.f32 	%f94, 0f3F000000;
	fma.rn.ftz.f32 	%f95, %f94, %f93, %f94;
	rsqrt.approx.ftz.f32 	%f96, %f95;
	mul.ftz.f32 	%f97, %f95, %f96;
	setp.gt.ftz.f32 	%p25, %f92, 0f3F0F5C29;
	mul.ftz.f32 	%f98, %f96, 0f3F000000;
	neg.ftz.f32 	%f99, %f97;
	fma.rn.ftz.f32 	%f100, %f99, %f98, %f94;
	fma.rn.ftz.f32 	%f101, %f97, %f100, %f97;
	setp.eq.ftz.f32 	%p26, %f92, 0f3F800000;
	selp.f32 	%f102, 0f00000000, %f101, %p26;
	selp.f32 	%f103, %f102, %f92, %p25;
	mov.b32 	%r50, %f103;
	mov.b32 	%r51, %f91;
	and.b32  	%r52, %r51, -2147483648;
	or.b32  	%r53, %r52, %r50;
	mov.b32 	%f104, %r53;
	mul.ftz.f32 	%f105, %f104, %f104;
	mov.f32 	%f106, 0f3C8B1ABB;
	mov.f32 	%f107, 0f3D10ECEF;
	fma.rn.ftz.f32 	%f108, %f107, %f105, %f106;
	mov.f32 	%f109, 0f3CFC028C;
	fma.rn.ftz.f32 	%f110, %f108, %f105, %f109;
	mov.f32 	%f111, 0f3D372139;
	fma.rn.ftz.f32 	%f112, %f110, %f105, %f111;
	mov.f32 	%f113, 0f3D9993DB;
	fma.rn.ftz.f32 	%f114, %f112, %f105, %f113;
	mov.f32 	%f115, 0f3E2AAAC6;
	fma.rn.ftz.f32 	%f116, %f114, %f105, %f115;
	mul.ftz.f32 	%f117, %f116, %f105;
	fma.rn.ftz.f32 	%f118, %f117, %f104, %f104;
	neg.ftz.f32 	%f119, %f118;
	selp.f32 	%f120, %f118, %f119, %p25;
	mov.f32 	%f121, 0f3FD774EB;
	mov.f32 	%f122, 0f3F6EE581;
	fma.rn.ftz.f32 	%f123, %f122, %f121, %f120;
	setp.gt.ftz.f32 	%p27, %f91, 0f3F0F5C29;
	selp.f32 	%f124, %f118, %f123, %p27;
	abs.ftz.f32 	%f125, %f159;
	setp.gtu.ftz.f32 	%p28, %f125, 0f7F800000;
	mov.b32 	%r54, %f11;
	and.b32  	%r55, %r54, -2147483648;
	mov.b32 	%r56, %f159;
	or.b32  	%r57, %r55, %r56;
	mov.b32 	%f126, %r57;
	selp.f32 	%f127, %f159, %f126, %p28;
	ld.const.u64 	%rd4, [plp+8];
	cvta.to.global.u64 	%rd5, %rd4;
	add.s64 	%rd1, %rd5, 132;
	ld.global.f32 	%f128, [%rd5+132];
	add.ftz.f32 	%f129, %f128, %f127;
	div.approx.ftz.f32 	%f131, %f129, %f67;
	cvt.rmi.ftz.f32.f32 	%f132, %f131;
	sub.ftz.f32 	%f30, %f131, %f132;
	add.ftz.f32 	%f133, %f124, %f124;
	selp.f32 	%f134, %f133, %f124, %p25;
	mov.f32 	%f135, 0f40490FDB;
	div.approx.ftz.f32 	%f31, %f134, %f135;
	mov.u32 	%r43, 0;
	// begin inline asm
	call (%r42), _optix_get_payload, (%r43);
	// end inline asm
	mov.u32 	%r45, 1;
	// begin inline asm
	call (%r44), _optix_get_payload, (%r45);
	// end inline asm
	mov.u32 	%r47, 2;
	mov.u32 	%r49, 3;
	// begin inline asm
	call (%r46), _optix_get_payload, (%r47);
	// end inline asm
	// begin inline asm
	call (%r48), _optix_get_payload, (%r49);
	// end inline asm
	mov.b64 	%rd2, {%r46, %r48};
	setp.ne.s32 	%p29, %r15, 300;
	setp.ne.s32 	%p30, %r14, 400;
	or.pred  	%p31, %p30, %p29;
	@%p31 bra 	$L__BB18_19;

	add.u64 	%rd6, %SP, 0;
	add.u64 	%rd7, %SPL, 0;
	mov.u32 	%r58, 300;
	mov.u32 	%r59, 400;
	st.local.v2.u32 	[%rd7], {%r59, %r58};
	mov.u64 	%rd8, $str$7;
	cvta.global.u64 	%rd9, %rd8;
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd9;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd6;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r60, [retval0+0];
	} // callseq 7

$L__BB18_19:
	mov.b64 	%rd10, {%r42, %r44};
	st.f32 	[%rd10+12], %f38;
	st.f32 	[%rd10+16], %f39;
	st.f32 	[%rd10+20], %f40;
	st.f32 	[%rd10+24], %f38;
	st.f32 	[%rd10+28], %f39;
	st.f32 	[%rd10+32], %f40;
	st.f32 	[%rd10+36], %f4;
	neg.ftz.f32 	%f136, %f39;
	st.f32 	[%rd10+40], %f136;
	neg.ftz.f32 	%f137, %f40;
	st.f32 	[%rd10+44], %f137;
	mul.ftz.f32 	%f138, %f30, 0f477FFF00;
	cvt.rzi.ftz.u32.f32 	%r61, %f138;
	min.u32 	%r62, %r61, 65535;
	mul.ftz.f32 	%f139, %f31, 0f477FFF00;
	cvt.rzi.ftz.u32.f32 	%r63, %f139;
	min.u32 	%r64, %r63, 65535;
	cvt.u16.u32 	%rs1, %r64;
	cvt.u16.u32 	%rs2, %r62;
	st.v2.u16 	[%rd10+60], {%rs2, %rs1};
	ld.global.u32 	%r65, [%rd1+4];
	setp.ne.s32 	%p32, %r14, %r65;
	@%p32 bra 	$L__BB18_25;

	ld.global.u32 	%r66, [%rd1+8];
	setp.ne.s32 	%p33, %r15, %r66;
	mov.f32 	%f160, 0f00000000;
	mov.f32 	%f161, 0f00000000;
	mov.f32 	%f162, 0f00000000;
	@%p33 bra 	$L__BB18_25;

	ld.u32 	%r67, [%rd2+64];
	or.b32  	%r68, %r67, 1;
	st.u32 	[%rd2+64], %r68;
	mov.u32 	%r69, -1;
	st.u32 	[%rd2+12], %r69;
	ld.const.u64 	%rd11, [plp];
	cvta.to.global.u64 	%rd12, %rd11;
	ld.global.u64 	%rd3, [%rd12+184];
	setp.eq.s64 	%p34, %rd3, 0;
	@%p34 bra 	$L__BB18_24;

	ld.global.u8 	%rs3, [%rd1+12];
	and.b16  	%rs4, %rs3, 128;
	setp.eq.s16 	%p35, %rs4, 0;
	@%p35 bra 	$L__BB18_24;

	mov.f32 	%f146, 0f00000000;
	tex.level.2d.v4.f32.f32 	{%f147, %f148, %f149, %f150}, [%rd3, {%f30, %f31}], %f146;
	ld.global.f32 	%f151, [%rd1+-4];
	mul.ftz.f32 	%f152, %f151, 0f40490FDB;
	mul.ftz.f32 	%f160, %f152, %f147;
	mul.ftz.f32 	%f161, %f152, %f148;
	mul.ftz.f32 	%f162, %f152, %f149;

$L__BB18_24:
	st.f32 	[%rd2+52], %f160;
	st.f32 	[%rd2+56], %f161;
	st.f32 	[%rd2+60], %f162;

$L__BB18_25:
	ret;

}

